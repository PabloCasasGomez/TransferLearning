{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicamos transfer learning con un modelo que tienen todos los datos menos el de la especie a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "import os\n",
    "import json\n",
    "\n",
    "import random as python_random\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import MeanAbsolutePercentageError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from keras import layers, models\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D\n",
    "\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "from funcionesComunes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESANDO ARCHIVO: Pinus wallichiana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/.venv/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 34 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,644</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │         \u001b[38;5;34m4,644\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m37\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">362,717</span> (1.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m362,717\u001b[0m (1.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120,649</span> (471.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120,649\u001b[0m (471.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">241,300</span> (942.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m241,300\u001b[0m (942.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - loss: 0.0165 - root_mean_squared_error: 0.1285 - val_loss: 0.0202 - val_root_mean_squared_error: 0.1423\n",
      "Epoch 2/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0158 - root_mean_squared_error: 0.1255 - val_loss: 0.0214 - val_root_mean_squared_error: 0.1462\n",
      "Epoch 3/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0155 - root_mean_squared_error: 0.1244 - val_loss: 0.0337 - val_root_mean_squared_error: 0.1835\n",
      "Epoch 4/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1239 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1220\n",
      "Epoch 5/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1235 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 6/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1230 - val_loss: 0.0181 - val_root_mean_squared_error: 0.1346\n",
      "Epoch 7/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1224 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1673\n",
      "Epoch 8/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0149 - root_mean_squared_error: 0.1219 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1275\n",
      "Epoch 9/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1216 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1265\n",
      "Epoch 10/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1212 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1213\n",
      "Epoch 11/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0146 - root_mean_squared_error: 0.1210 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1227\n",
      "Epoch 12/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0146 - root_mean_squared_error: 0.1208 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1209\n",
      "Epoch 13/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1202 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1239\n",
      "Epoch 14/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1202 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1222\n",
      "Epoch 15/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1196 - val_loss: 0.0219 - val_root_mean_squared_error: 0.1480\n",
      "Epoch 16/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1195 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1337\n",
      "Epoch 17/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0142 - root_mean_squared_error: 0.1192 - val_loss: 0.0191 - val_root_mean_squared_error: 0.1381\n",
      "Epoch 18/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0142 - root_mean_squared_error: 0.1191 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1262\n",
      "Epoch 19/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0140 - root_mean_squared_error: 0.1184 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1264\n",
      "Epoch 20/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0140 - root_mean_squared_error: 0.1184 - val_loss: 0.0183 - val_root_mean_squared_error: 0.1353\n",
      "Epoch 21/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0139 - root_mean_squared_error: 0.1179 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1296\n",
      "Epoch 22/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0140 - root_mean_squared_error: 0.1182 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1318\n",
      "Epoch 23/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0138 - root_mean_squared_error: 0.1177 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1270\n",
      "Epoch 24/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0138 - root_mean_squared_error: 0.1173 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1238\n",
      "Epoch 25/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0137 - root_mean_squared_error: 0.1172 - val_loss: 0.0245 - val_root_mean_squared_error: 0.1564\n",
      "Epoch 26/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1216\n",
      "Epoch 27/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0136 - root_mean_squared_error: 0.1168 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1251\n",
      "Epoch 28/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1161 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1243\n",
      "Epoch 29/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1160 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1231\n",
      "Epoch 30/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1163 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1240\n",
      "Epoch 31/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0133 - root_mean_squared_error: 0.1154 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1224\n",
      "Epoch 32/200\n",
      "\u001b[1m6835/6835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0133 - root_mean_squared_error: 0.1154 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1246\n",
      "PROCESANDO ARCHIVO: Pinus gerardiana\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │         \u001b[38;5;34m6,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m49\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">367,397</span> (1.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m367,397\u001b[0m (1.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,209</span> (477.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m122,209\u001b[0m (477.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">244,420</span> (954.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m244,420\u001b[0m (954.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0287 - root_mean_squared_error: 0.1678 - val_loss: 0.0180 - val_root_mean_squared_error: 0.1343\n",
      "Epoch 2/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0188 - root_mean_squared_error: 0.1372 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1252\n",
      "Epoch 3/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0180 - root_mean_squared_error: 0.1342 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1256\n",
      "Epoch 4/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0176 - root_mean_squared_error: 0.1326 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1239\n",
      "Epoch 5/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0172 - root_mean_squared_error: 0.1311 - val_loss: 0.0196 - val_root_mean_squared_error: 0.1401\n",
      "Epoch 6/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0172 - root_mean_squared_error: 0.1311 - val_loss: 0.0206 - val_root_mean_squared_error: 0.1437\n",
      "Epoch 7/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0169 - root_mean_squared_error: 0.1302 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1294\n",
      "Epoch 8/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0168 - root_mean_squared_error: 0.1295 - val_loss: 0.0178 - val_root_mean_squared_error: 0.1335\n",
      "Epoch 9/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0166 - root_mean_squared_error: 0.1288 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1240\n",
      "Epoch 10/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0166 - root_mean_squared_error: 0.1288 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1240\n",
      "Epoch 11/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0165 - root_mean_squared_error: 0.1284 - val_loss: 0.0199 - val_root_mean_squared_error: 0.1409\n",
      "Epoch 12/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0162 - root_mean_squared_error: 0.1275 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1275\n",
      "Epoch 13/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0164 - root_mean_squared_error: 0.1279 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1210\n",
      "Epoch 14/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0162 - root_mean_squared_error: 0.1273 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1238\n",
      "Epoch 15/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0162 - root_mean_squared_error: 0.1271 - val_loss: 0.0203 - val_root_mean_squared_error: 0.1423\n",
      "Epoch 16/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0161 - root_mean_squared_error: 0.1269 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1294\n",
      "Epoch 17/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0161 - root_mean_squared_error: 0.1267 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1274\n",
      "Epoch 18/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0160 - root_mean_squared_error: 0.1264 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1216\n",
      "Epoch 19/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0157 - root_mean_squared_error: 0.1255 - val_loss: 0.0176 - val_root_mean_squared_error: 0.1325\n",
      "Epoch 20/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0158 - root_mean_squared_error: 0.1258 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1226\n",
      "Epoch 21/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0157 - root_mean_squared_error: 0.1254 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1271\n",
      "Epoch 22/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3ms/step - loss: 0.0157 - root_mean_squared_error: 0.1253 - val_loss: 0.0204 - val_root_mean_squared_error: 0.1430\n",
      "Epoch 23/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3ms/step - loss: 0.0156 - root_mean_squared_error: 0.1251 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1245\n",
      "Epoch 24/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0155 - root_mean_squared_error: 0.1247 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1212\n",
      "Epoch 25/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0155 - root_mean_squared_error: 0.1247 - val_loss: 0.0188 - val_root_mean_squared_error: 0.1372\n",
      "Epoch 26/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0156 - root_mean_squared_error: 0.1248 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1270\n",
      "Epoch 27/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0155 - root_mean_squared_error: 0.1246 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1238\n",
      "Epoch 28/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0154 - root_mean_squared_error: 0.1241 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1227\n",
      "Epoch 29/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1238 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1263\n",
      "Epoch 30/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1236 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1209\n",
      "Epoch 31/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1236 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 32/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0152 - root_mean_squared_error: 0.1234 - val_loss: 0.0173 - val_root_mean_squared_error: 0.1315\n",
      "Epoch 33/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1238 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1237\n",
      "Epoch 34/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0152 - root_mean_squared_error: 0.1232 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1318\n",
      "Epoch 35/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1255\n",
      "Epoch 36/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0152 - root_mean_squared_error: 0.1233 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1210\n",
      "Epoch 37/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1230 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1249\n",
      "Epoch 38/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1228 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1284\n",
      "Epoch 39/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1223 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1245\n",
      "Epoch 40/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0149 - root_mean_squared_error: 0.1221 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1216\n",
      "Epoch 41/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1225 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1212\n",
      "Epoch 42/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1223 - val_loss: 0.0300 - val_root_mean_squared_error: 0.1733\n",
      "Epoch 43/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0149 - root_mean_squared_error: 0.1219 - val_loss: 0.0204 - val_root_mean_squared_error: 0.1427\n",
      "Epoch 44/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0149 - root_mean_squared_error: 0.1220 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1233\n",
      "Epoch 45/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.0149 - root_mean_squared_error: 0.1220 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1283\n",
      "Epoch 46/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0149 - root_mean_squared_error: 0.1220 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1252\n",
      "Epoch 47/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1215 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1220\n",
      "Epoch 48/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1217 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1252\n",
      "Epoch 49/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1214 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1216\n",
      "Epoch 50/200\n",
      "\u001b[1m10236/10236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1215 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1319\n",
      "PROCESANDO ARCHIVO: Betula utilis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/.venv/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 66 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,492</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m27,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │         \u001b[38;5;34m3,492\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m37\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">549,629</span> (2.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m549,629\u001b[0m (2.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">182,761</span> (713.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m182,761\u001b[0m (713.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">365,524</span> (1.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m365,524\u001b[0m (1.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5ms/step - loss: 0.0140 - root_mean_squared_error: 0.1184 - val_loss: 0.0224 - val_root_mean_squared_error: 0.1497\n",
      "Epoch 2/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0140 - root_mean_squared_error: 0.1183 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1262\n",
      "Epoch 3/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0139 - root_mean_squared_error: 0.1178 - val_loss: 0.0304 - val_root_mean_squared_error: 0.1745\n",
      "Epoch 4/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0138 - root_mean_squared_error: 0.1176 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 5/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0138 - root_mean_squared_error: 0.1176 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1223\n",
      "Epoch 6/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0137 - root_mean_squared_error: 0.1172 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1281\n",
      "Epoch 7/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0138 - root_mean_squared_error: 0.1173 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1225\n",
      "Epoch 8/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0137 - root_mean_squared_error: 0.1170 - val_loss: 0.0299 - val_root_mean_squared_error: 0.1729\n",
      "Epoch 9/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0136 - root_mean_squared_error: 0.1164 - val_loss: 0.0186 - val_root_mean_squared_error: 0.1365\n",
      "Epoch 10/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0135 - root_mean_squared_error: 0.1162 - val_loss: 0.0201 - val_root_mean_squared_error: 0.1417\n",
      "Epoch 11/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0135 - root_mean_squared_error: 0.1161 - val_loss: 0.0265 - val_root_mean_squared_error: 0.1629\n",
      "Epoch 12/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0217 - val_root_mean_squared_error: 0.1471\n",
      "Epoch 13/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0133 - root_mean_squared_error: 0.1154 - val_loss: 0.0177 - val_root_mean_squared_error: 0.1331\n",
      "Epoch 14/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0132 - root_mean_squared_error: 0.1150 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1220\n",
      "Epoch 15/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0132 - root_mean_squared_error: 0.1150 - val_loss: 0.0224 - val_root_mean_squared_error: 0.1496\n",
      "Epoch 16/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0131 - root_mean_squared_error: 0.1146 - val_loss: 0.0196 - val_root_mean_squared_error: 0.1399\n",
      "Epoch 17/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0131 - root_mean_squared_error: 0.1144 - val_loss: 0.0299 - val_root_mean_squared_error: 0.1728\n",
      "Epoch 18/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0130 - root_mean_squared_error: 0.1142 - val_loss: 0.0181 - val_root_mean_squared_error: 0.1345\n",
      "Epoch 19/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0130 - root_mean_squared_error: 0.1140 - val_loss: 0.0182 - val_root_mean_squared_error: 0.1348\n",
      "Epoch 20/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0130 - root_mean_squared_error: 0.1140 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1380\n",
      "Epoch 21/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0129 - root_mean_squared_error: 0.1137 - val_loss: 0.0244 - val_root_mean_squared_error: 0.1561\n",
      "Epoch 22/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0128 - root_mean_squared_error: 0.1132 - val_loss: 0.0236 - val_root_mean_squared_error: 0.1535\n",
      "Epoch 23/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0129 - root_mean_squared_error: 0.1134 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1208\n",
      "Epoch 24/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0128 - root_mean_squared_error: 0.1129 - val_loss: 0.0181 - val_root_mean_squared_error: 0.1346\n",
      "Epoch 25/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0127 - root_mean_squared_error: 0.1127 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1293\n",
      "Epoch 26/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0127 - root_mean_squared_error: 0.1126 - val_loss: 0.0273 - val_root_mean_squared_error: 0.1651\n",
      "Epoch 27/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0127 - root_mean_squared_error: 0.1128 - val_loss: 0.0205 - val_root_mean_squared_error: 0.1431\n",
      "Epoch 28/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0126 - root_mean_squared_error: 0.1123 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1291\n",
      "Epoch 29/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0125 - root_mean_squared_error: 0.1119 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1195\n",
      "Epoch 30/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0124 - root_mean_squared_error: 0.1115 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 31/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0125 - root_mean_squared_error: 0.1116 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1272\n",
      "Epoch 32/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0124 - root_mean_squared_error: 0.1113 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1212\n",
      "Epoch 33/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0124 - root_mean_squared_error: 0.1112 - val_loss: 0.0142 - val_root_mean_squared_error: 0.1193\n",
      "Epoch 34/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0123 - root_mean_squared_error: 0.1111 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1267\n",
      "Epoch 35/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0123 - root_mean_squared_error: 0.1110 - val_loss: 0.0189 - val_root_mean_squared_error: 0.1376\n",
      "Epoch 36/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0122 - root_mean_squared_error: 0.1105 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1211\n",
      "Epoch 37/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0122 - root_mean_squared_error: 0.1104 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1194\n",
      "Epoch 38/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0121 - root_mean_squared_error: 0.1101 - val_loss: 0.0192 - val_root_mean_squared_error: 0.1387\n",
      "Epoch 39/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0122 - root_mean_squared_error: 0.1103 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1197\n",
      "Epoch 40/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0120 - root_mean_squared_error: 0.1098 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 41/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0120 - root_mean_squared_error: 0.1096 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1320\n",
      "Epoch 42/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0120 - root_mean_squared_error: 0.1095 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1226\n",
      "Epoch 43/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0120 - root_mean_squared_error: 0.1094 - val_loss: 0.0201 - val_root_mean_squared_error: 0.1418\n",
      "Epoch 44/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0119 - root_mean_squared_error: 0.1090 - val_loss: 0.0200 - val_root_mean_squared_error: 0.1413\n",
      "Epoch 45/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0119 - root_mean_squared_error: 0.1089 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1204\n",
      "Epoch 46/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0118 - root_mean_squared_error: 0.1088 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1216\n",
      "Epoch 47/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0118 - root_mean_squared_error: 0.1085 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1241\n",
      "Epoch 48/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0118 - root_mean_squared_error: 0.1084 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1280\n",
      "Epoch 49/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0117 - root_mean_squared_error: 0.1083 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1194\n",
      "Epoch 50/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0117 - root_mean_squared_error: 0.1084 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1228\n",
      "Epoch 51/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0117 - root_mean_squared_error: 0.1083 - val_loss: 0.0200 - val_root_mean_squared_error: 0.1416\n",
      "Epoch 52/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0117 - root_mean_squared_error: 0.1082 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1230\n",
      "Epoch 53/200\n",
      "\u001b[1m5239/5239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0117 - root_mean_squared_error: 0.1081 - val_loss: 0.0144 - val_root_mean_squared_error: 0.1199\n",
      "PROCESANDO ARCHIVO: Picea smithiana\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m5\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">350,237</span> (1.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m350,237\u001b[0m (1.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,489</span> (455.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,489\u001b[0m (455.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">232,980</span> (910.08 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m232,980\u001b[0m (910.08 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step - loss: 0.0201 - root_mean_squared_error: 0.1414 - val_loss: 0.0432 - val_root_mean_squared_error: 0.2078\n",
      "Epoch 2/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0167 - root_mean_squared_error: 0.1292 - val_loss: 0.0198 - val_root_mean_squared_error: 0.1407\n",
      "Epoch 3/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0162 - root_mean_squared_error: 0.1272 - val_loss: 0.0318 - val_root_mean_squared_error: 0.1783\n",
      "Epoch 4/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0158 - root_mean_squared_error: 0.1257 - val_loss: 0.0239 - val_root_mean_squared_error: 0.1547\n",
      "Epoch 5/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0156 - root_mean_squared_error: 0.1247 - val_loss: 0.0360 - val_root_mean_squared_error: 0.1897\n",
      "Epoch 6/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0154 - root_mean_squared_error: 0.1242 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1271\n",
      "Epoch 7/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1236 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1234\n",
      "Epoch 8/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1227 - val_loss: 0.0286 - val_root_mean_squared_error: 0.1691\n",
      "Epoch 9/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1223 - val_loss: 0.0313 - val_root_mean_squared_error: 0.1769\n",
      "Epoch 10/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1218 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1230\n",
      "Epoch 11/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1215 - val_loss: 0.0263 - val_root_mean_squared_error: 0.1621\n",
      "Epoch 12/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0146 - root_mean_squared_error: 0.1209 - val_loss: 0.0307 - val_root_mean_squared_error: 0.1752\n",
      "Epoch 13/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1203 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1222\n",
      "Epoch 14/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1202 - val_loss: 0.0238 - val_root_mean_squared_error: 0.1542\n",
      "Epoch 15/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1197 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1258\n",
      "Epoch 16/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1195 - val_loss: 0.0192 - val_root_mean_squared_error: 0.1385\n",
      "Epoch 17/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
      "Epoch 18/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0141 - root_mean_squared_error: 0.1188 - val_loss: 0.0296 - val_root_mean_squared_error: 0.1720\n",
      "Epoch 19/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0140 - root_mean_squared_error: 0.1185 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1196\n",
      "Epoch 20/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0139 - root_mean_squared_error: 0.1181 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1218\n",
      "Epoch 21/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0140 - root_mean_squared_error: 0.1183 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1209\n",
      "Epoch 22/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0139 - root_mean_squared_error: 0.1178 - val_loss: 0.0260 - val_root_mean_squared_error: 0.1613\n",
      "Epoch 23/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0138 - root_mean_squared_error: 0.1176 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1203\n",
      "Epoch 24/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0138 - root_mean_squared_error: 0.1173 - val_loss: 0.0326 - val_root_mean_squared_error: 0.1805\n",
      "Epoch 25/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0138 - root_mean_squared_error: 0.1174 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1196\n",
      "Epoch 26/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1203\n",
      "Epoch 27/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1214\n",
      "Epoch 28/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1164 - val_loss: 0.0177 - val_root_mean_squared_error: 0.1331\n",
      "Epoch 29/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1164 - val_loss: 0.0424 - val_root_mean_squared_error: 0.2060\n",
      "Epoch 30/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0134 - root_mean_squared_error: 0.1157 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1297\n",
      "Epoch 31/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1245\n",
      "Epoch 32/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0133 - root_mean_squared_error: 0.1155 - val_loss: 0.0186 - val_root_mean_squared_error: 0.1365\n",
      "Epoch 33/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0133 - root_mean_squared_error: 0.1153 - val_loss: 0.0195 - val_root_mean_squared_error: 0.1397\n",
      "Epoch 34/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0133 - root_mean_squared_error: 0.1154 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1242\n",
      "Epoch 35/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0133 - root_mean_squared_error: 0.1152 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1307\n",
      "Epoch 36/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0132 - root_mean_squared_error: 0.1150 - val_loss: 0.0210 - val_root_mean_squared_error: 0.1448\n",
      "Epoch 37/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0132 - root_mean_squared_error: 0.1147 - val_loss: 0.0227 - val_root_mean_squared_error: 0.1505\n",
      "Epoch 38/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0131 - root_mean_squared_error: 0.1145 - val_loss: 0.0183 - val_root_mean_squared_error: 0.1354\n",
      "Epoch 39/200\n",
      "\u001b[1m4658/4658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 0.0131 - root_mean_squared_error: 0.1146 - val_loss: 0.0198 - val_root_mean_squared_error: 0.1406\n",
      "PROCESANDO ARCHIVO: Abies spectabilis\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,860</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │         \u001b[38;5;34m2,860\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m45\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">507,661</span> (1.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m507,661\u001b[0m (1.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">168,793</span> (659.35 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m168,793\u001b[0m (659.35 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> (5.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,280\u001b[0m (5.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">337,588</span> (1.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m337,588\u001b[0m (1.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0168 - root_mean_squared_error: 0.1298 - val_loss: 0.0539 - val_root_mean_squared_error: 0.2321\n",
      "Epoch 2/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0175 - root_mean_squared_error: 0.1324 - val_loss: 0.0262 - val_root_mean_squared_error: 0.1620\n",
      "Epoch 3/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0168 - root_mean_squared_error: 0.1295 - val_loss: 0.0290 - val_root_mean_squared_error: 0.1704\n",
      "Epoch 4/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0163 - root_mean_squared_error: 0.1278 - val_loss: 0.0445 - val_root_mean_squared_error: 0.2109\n",
      "Epoch 5/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0169 - root_mean_squared_error: 0.1298 - val_loss: 0.0497 - val_root_mean_squared_error: 0.2229\n",
      "Epoch 6/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0168 - root_mean_squared_error: 0.1295 - val_loss: 0.0333 - val_root_mean_squared_error: 0.1824\n",
      "Epoch 7/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0160 - root_mean_squared_error: 0.1264 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1296\n",
      "Epoch 8/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0156 - root_mean_squared_error: 0.1251 - val_loss: 0.0854 - val_root_mean_squared_error: 0.2923\n",
      "Epoch 9/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0184 - root_mean_squared_error: 0.1355 - val_loss: 0.0225 - val_root_mean_squared_error: 0.1501\n",
      "Epoch 10/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0158 - root_mean_squared_error: 0.1256 - val_loss: 0.0254 - val_root_mean_squared_error: 0.1593\n",
      "Epoch 11/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0155 - root_mean_squared_error: 0.1246 - val_loss: 0.0626 - val_root_mean_squared_error: 0.2502\n",
      "Epoch 12/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0153 - root_mean_squared_error: 0.1238 - val_loss: 0.2364 - val_root_mean_squared_error: 0.4862\n",
      "Epoch 13/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0164 - root_mean_squared_error: 0.1279 - val_loss: 0.0306 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 14/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0157 - root_mean_squared_error: 0.1252 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1311\n",
      "Epoch 15/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0155 - root_mean_squared_error: 0.1245 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1302\n",
      "Epoch 16/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0155 - root_mean_squared_error: 0.1244 - val_loss: 0.0306 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 17/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0158 - root_mean_squared_error: 0.1256 - val_loss: 0.0686 - val_root_mean_squared_error: 0.2620\n",
      "Epoch 18/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0155 - root_mean_squared_error: 0.1246 - val_loss: 0.0666 - val_root_mean_squared_error: 0.2581\n",
      "Epoch 19/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0160 - root_mean_squared_error: 0.1266 - val_loss: 0.0379 - val_root_mean_squared_error: 0.1946\n",
      "Epoch 20/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0156 - root_mean_squared_error: 0.1248 - val_loss: 0.0284 - val_root_mean_squared_error: 0.1684\n",
      "Epoch 21/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0157 - root_mean_squared_error: 0.1254 - val_loss: 0.0242 - val_root_mean_squared_error: 0.1555\n",
      "Epoch 22/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0153 - root_mean_squared_error: 0.1237 - val_loss: 0.0271 - val_root_mean_squared_error: 0.1646\n",
      "Epoch 23/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0162 - root_mean_squared_error: 0.1273 - val_loss: 0.0367 - val_root_mean_squared_error: 0.1916\n",
      "Epoch 24/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0149 - root_mean_squared_error: 0.1220 - val_loss: 0.0366 - val_root_mean_squared_error: 0.1914\n",
      "Epoch 25/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0161 - root_mean_squared_error: 0.1264 - val_loss: 0.1062 - val_root_mean_squared_error: 0.3259\n",
      "Epoch 26/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0238 - root_mean_squared_error: 0.1540 - val_loss: 0.0803 - val_root_mean_squared_error: 0.2834\n",
      "Epoch 27/200\n",
      "\u001b[1m4679/4679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0192 - root_mean_squared_error: 0.1385 - val_loss: 0.0694 - val_root_mean_squared_error: 0.2634\n",
      "PROCESANDO ARCHIVO: Cedrus deodara\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │        \u001b[38;5;34m12,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m2,580\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">580,893</span> (2.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m580,893\u001b[0m (2.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193,161</span> (754.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m193,161\u001b[0m (754.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> (5.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,408\u001b[0m (5.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">386,324</span> (1.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m386,324\u001b[0m (1.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 5ms/step - loss: 0.0161 - root_mean_squared_error: 0.1267 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1250\n",
      "Epoch 2/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0161 - root_mean_squared_error: 0.1270 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1277\n",
      "Epoch 3/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0157 - root_mean_squared_error: 0.1255 - val_loss: 0.0236 - val_root_mean_squared_error: 0.1535\n",
      "Epoch 4/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0157 - root_mean_squared_error: 0.1255 - val_loss: 0.0232 - val_root_mean_squared_error: 0.1525\n",
      "Epoch 5/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0163 - root_mean_squared_error: 0.1276 - val_loss: 0.0188 - val_root_mean_squared_error: 0.1369\n",
      "Epoch 6/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0157 - root_mean_squared_error: 0.1254 - val_loss: 0.0241 - val_root_mean_squared_error: 0.1553\n",
      "Epoch 7/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0156 - root_mean_squared_error: 0.1249 - val_loss: 0.0361 - val_root_mean_squared_error: 0.1899\n",
      "Epoch 8/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0156 - root_mean_squared_error: 0.1249 - val_loss: 0.0204 - val_root_mean_squared_error: 0.1429\n",
      "Epoch 9/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0153 - root_mean_squared_error: 0.1238 - val_loss: 0.0244 - val_root_mean_squared_error: 0.1563\n",
      "Epoch 10/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0181 - root_mean_squared_error: 0.1342 - val_loss: 0.0253 - val_root_mean_squared_error: 0.1592\n",
      "Epoch 11/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0214 - root_mean_squared_error: 0.1463 - val_loss: 0.0211 - val_root_mean_squared_error: 0.1453\n",
      "Epoch 12/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0165 - root_mean_squared_error: 0.1283 - val_loss: 0.0178 - val_root_mean_squared_error: 0.1335\n",
      "Epoch 13/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1246 - val_loss: 0.0203 - val_root_mean_squared_error: 0.1426\n",
      "Epoch 14/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1247 - val_loss: 0.0219 - val_root_mean_squared_error: 0.1481\n",
      "Epoch 15/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0153 - root_mean_squared_error: 0.1238 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
      "Epoch 16/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0154 - root_mean_squared_error: 0.1240 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 17/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0151 - root_mean_squared_error: 0.1230 - val_loss: 0.0211 - val_root_mean_squared_error: 0.1452\n",
      "Epoch 18/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0157 - root_mean_squared_error: 0.1251 - val_loss: 0.0195 - val_root_mean_squared_error: 0.1397\n",
      "Epoch 19/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1245 - val_loss: 0.0324 - val_root_mean_squared_error: 0.1799\n",
      "Epoch 20/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1246 - val_loss: 0.0358 - val_root_mean_squared_error: 0.1891\n",
      "Epoch 21/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0151 - root_mean_squared_error: 0.1228 - val_loss: 0.0215 - val_root_mean_squared_error: 0.1467\n",
      "Epoch 22/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0152 - root_mean_squared_error: 0.1231 - val_loss: 0.0256 - val_root_mean_squared_error: 0.1599\n",
      "Epoch 23/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0149 - root_mean_squared_error: 0.1219 - val_loss: 0.0265 - val_root_mean_squared_error: 0.1627\n",
      "Epoch 24/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1245 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1338\n",
      "Epoch 25/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0149 - root_mean_squared_error: 0.1220 - val_loss: 0.0242 - val_root_mean_squared_error: 0.1555\n",
      "Epoch 26/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0147 - root_mean_squared_error: 0.1213 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1260\n",
      "Epoch 27/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0148 - root_mean_squared_error: 0.1215 - val_loss: 0.0197 - val_root_mean_squared_error: 0.1403\n",
      "Epoch 28/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0150 - root_mean_squared_error: 0.1224 - val_loss: 0.0331 - val_root_mean_squared_error: 0.1819\n",
      "Epoch 29/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0146 - root_mean_squared_error: 0.1210 - val_loss: 0.0230 - val_root_mean_squared_error: 0.1515\n",
      "Epoch 30/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0154 - root_mean_squared_error: 0.1242 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1292\n",
      "Epoch 31/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0154 - root_mean_squared_error: 0.1239 - val_loss: 0.0333 - val_root_mean_squared_error: 0.1824\n",
      "Epoch 32/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0148 - root_mean_squared_error: 0.1216 - val_loss: 0.0316 - val_root_mean_squared_error: 0.1777\n",
      "Epoch 33/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0152 - root_mean_squared_error: 0.1233 - val_loss: 0.0378 - val_root_mean_squared_error: 0.1944\n",
      "Epoch 34/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0147 - root_mean_squared_error: 0.1214 - val_loss: 0.0182 - val_root_mean_squared_error: 0.1351\n",
      "Epoch 35/200\n",
      "\u001b[1m4591/4591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0148 - root_mean_squared_error: 0.1217 - val_loss: 0.0223 - val_root_mean_squared_error: 0.1494\n",
      "PROCESANDO ARCHIVO: Tsuga dumosa\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,740</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)             │         \u001b[38;5;34m7,740\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m61\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">967,533</span> (3.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m967,533\u001b[0m (3.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">321,913</span> (1.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m321,913\u001b[0m (1.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">643,828</span> (2.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m643,828\u001b[0m (2.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6ms/step - loss: 0.0129 - root_mean_squared_error: 0.1136 - val_loss: 0.0275 - val_root_mean_squared_error: 0.1657\n",
      "Epoch 2/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0128 - root_mean_squared_error: 0.1130 - val_loss: 0.0268 - val_root_mean_squared_error: 0.1636\n",
      "Epoch 3/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0127 - root_mean_squared_error: 0.1126 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1221\n",
      "Epoch 4/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0126 - root_mean_squared_error: 0.1124 - val_loss: 0.0325 - val_root_mean_squared_error: 0.1802\n",
      "Epoch 5/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0126 - root_mean_squared_error: 0.1121 - val_loss: 0.0386 - val_root_mean_squared_error: 0.1966\n",
      "Epoch 6/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0127 - root_mean_squared_error: 0.1126 - val_loss: 0.0305 - val_root_mean_squared_error: 0.1747\n",
      "Epoch 7/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0124 - root_mean_squared_error: 0.1115 - val_loss: 0.0234 - val_root_mean_squared_error: 0.1530\n",
      "Epoch 8/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0124 - root_mean_squared_error: 0.1115 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1287\n",
      "Epoch 9/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0124 - root_mean_squared_error: 0.1113 - val_loss: 0.0177 - val_root_mean_squared_error: 0.1332\n",
      "Epoch 10/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0124 - root_mean_squared_error: 0.1112 - val_loss: 0.0335 - val_root_mean_squared_error: 0.1829\n",
      "Epoch 11/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0122 - root_mean_squared_error: 0.1106 - val_loss: 0.0373 - val_root_mean_squared_error: 0.1931\n",
      "Epoch 12/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0122 - root_mean_squared_error: 0.1103 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1272\n",
      "Epoch 13/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0122 - root_mean_squared_error: 0.1104 - val_loss: 0.0269 - val_root_mean_squared_error: 0.1640\n",
      "Epoch 14/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0120 - root_mean_squared_error: 0.1097 - val_loss: 0.0476 - val_root_mean_squared_error: 0.2182\n",
      "Epoch 15/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0120 - root_mean_squared_error: 0.1098 - val_loss: 0.0226 - val_root_mean_squared_error: 0.1504\n",
      "Epoch 16/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0120 - root_mean_squared_error: 0.1094 - val_loss: 0.0410 - val_root_mean_squared_error: 0.2024\n",
      "Epoch 17/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0120 - root_mean_squared_error: 0.1095 - val_loss: 0.0222 - val_root_mean_squared_error: 0.1491\n",
      "Epoch 18/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0118 - root_mean_squared_error: 0.1088 - val_loss: 0.0265 - val_root_mean_squared_error: 0.1629\n",
      "Epoch 19/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0118 - root_mean_squared_error: 0.1089 - val_loss: 0.0398 - val_root_mean_squared_error: 0.1995\n",
      "Epoch 20/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0119 - root_mean_squared_error: 0.1090 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1320\n",
      "Epoch 21/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0118 - root_mean_squared_error: 0.1086 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1256\n",
      "Epoch 22/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0116 - root_mean_squared_error: 0.1079 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1283\n",
      "Epoch 23/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0116 - root_mean_squared_error: 0.1076 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1204\n",
      "Epoch 24/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0115 - root_mean_squared_error: 0.1075 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1673\n",
      "Epoch 25/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0116 - root_mean_squared_error: 0.1078 - val_loss: 0.0237 - val_root_mean_squared_error: 0.1539\n",
      "Epoch 26/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0115 - root_mean_squared_error: 0.1070 - val_loss: 0.0451 - val_root_mean_squared_error: 0.2124\n",
      "Epoch 27/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0114 - root_mean_squared_error: 0.1069 - val_loss: 0.0217 - val_root_mean_squared_error: 0.1473\n",
      "Epoch 28/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0114 - root_mean_squared_error: 0.1067 - val_loss: 0.0456 - val_root_mean_squared_error: 0.2135\n",
      "Epoch 29/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0114 - root_mean_squared_error: 0.1069 - val_loss: 0.0322 - val_root_mean_squared_error: 0.1794\n",
      "Epoch 30/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0113 - root_mean_squared_error: 0.1064 - val_loss: 0.0315 - val_root_mean_squared_error: 0.1775\n",
      "Epoch 31/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0113 - root_mean_squared_error: 0.1062 - val_loss: 0.0283 - val_root_mean_squared_error: 0.1682\n",
      "Epoch 32/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0113 - root_mean_squared_error: 0.1062 - val_loss: 0.0289 - val_root_mean_squared_error: 0.1700\n",
      "Epoch 33/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0112 - root_mean_squared_error: 0.1060 - val_loss: 0.0553 - val_root_mean_squared_error: 0.2351\n",
      "Epoch 34/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0112 - root_mean_squared_error: 0.1058 - val_loss: 0.0283 - val_root_mean_squared_error: 0.1683\n",
      "Epoch 35/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0113 - root_mean_squared_error: 0.1061 - val_loss: 0.0271 - val_root_mean_squared_error: 0.1646\n",
      "Epoch 36/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0111 - root_mean_squared_error: 0.1054 - val_loss: 0.0194 - val_root_mean_squared_error: 0.1391\n",
      "Epoch 37/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0110 - root_mean_squared_error: 0.1051 - val_loss: 0.0271 - val_root_mean_squared_error: 0.1646\n",
      "Epoch 38/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0110 - root_mean_squared_error: 0.1049 - val_loss: 0.0375 - val_root_mean_squared_error: 0.1937\n",
      "Epoch 39/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0110 - root_mean_squared_error: 0.1051 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1306\n",
      "Epoch 40/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0109 - root_mean_squared_error: 0.1044 - val_loss: 0.0391 - val_root_mean_squared_error: 0.1976\n",
      "Epoch 41/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0110 - root_mean_squared_error: 0.1048 - val_loss: 0.0304 - val_root_mean_squared_error: 0.1744\n",
      "Epoch 42/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0108 - root_mean_squared_error: 0.1041 - val_loss: 0.0349 - val_root_mean_squared_error: 0.1868\n",
      "Epoch 43/200\n",
      "\u001b[1m4916/4916\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0108 - root_mean_squared_error: 0.1040 - val_loss: 0.0247 - val_root_mean_squared_error: 0.1572\n",
      "PROCESANDO ARCHIVO: Juniperus recurva\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">924</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)             │           \u001b[38;5;34m924\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m29\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">380,941</span> (1.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m380,941\u001b[0m (1.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,617</span> (494.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m126,617\u001b[0m (494.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> (4.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,088\u001b[0m (4.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">253,236</span> (989.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m253,236\u001b[0m (989.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0135 - root_mean_squared_error: 0.1164 - val_loss: 0.0300 - val_root_mean_squared_error: 0.1732\n",
      "Epoch 2/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1157 - val_loss: 0.0227 - val_root_mean_squared_error: 0.1506\n",
      "Epoch 3/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1157 - val_loss: 0.0239 - val_root_mean_squared_error: 0.1546\n",
      "Epoch 4/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0133 - root_mean_squared_error: 0.1152 - val_loss: 0.0206 - val_root_mean_squared_error: 0.1436\n",
      "Epoch 5/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0133 - root_mean_squared_error: 0.1153 - val_loss: 0.0222 - val_root_mean_squared_error: 0.1490\n",
      "Epoch 6/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0132 - root_mean_squared_error: 0.1148 - val_loss: 0.0299 - val_root_mean_squared_error: 0.1729\n",
      "Epoch 7/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0131 - root_mean_squared_error: 0.1145 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1311\n",
      "Epoch 8/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0131 - root_mean_squared_error: 0.1144 - val_loss: 0.0203 - val_root_mean_squared_error: 0.1424\n",
      "Epoch 9/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0130 - root_mean_squared_error: 0.1141 - val_loss: 0.0222 - val_root_mean_squared_error: 0.1490\n",
      "Epoch 10/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0130 - root_mean_squared_error: 0.1140 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1673\n",
      "Epoch 11/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0129 - root_mean_squared_error: 0.1137 - val_loss: 0.0250 - val_root_mean_squared_error: 0.1582\n",
      "Epoch 12/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0128 - root_mean_squared_error: 0.1132 - val_loss: 0.0380 - val_root_mean_squared_error: 0.1950\n",
      "Epoch 13/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0129 - root_mean_squared_error: 0.1135 - val_loss: 0.0272 - val_root_mean_squared_error: 0.1650\n",
      "Epoch 14/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0128 - root_mean_squared_error: 0.1131 - val_loss: 0.0244 - val_root_mean_squared_error: 0.1562\n",
      "Epoch 15/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0127 - root_mean_squared_error: 0.1128 - val_loss: 0.0230 - val_root_mean_squared_error: 0.1516\n",
      "Epoch 16/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0127 - root_mean_squared_error: 0.1128 - val_loss: 0.0563 - val_root_mean_squared_error: 0.2373\n",
      "Epoch 17/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0126 - root_mean_squared_error: 0.1125 - val_loss: 0.0910 - val_root_mean_squared_error: 0.3016\n",
      "Epoch 18/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0126 - root_mean_squared_error: 0.1124 - val_loss: 0.0674 - val_root_mean_squared_error: 0.2597\n",
      "Epoch 19/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0126 - root_mean_squared_error: 0.1122 - val_loss: 0.7248 - val_root_mean_squared_error: 0.8514\n",
      "Epoch 20/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0126 - root_mean_squared_error: 0.1120 - val_loss: 0.0224 - val_root_mean_squared_error: 0.1496\n",
      "Epoch 21/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0125 - root_mean_squared_error: 0.1117 - val_loss: 0.0248 - val_root_mean_squared_error: 0.1576\n",
      "Epoch 22/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0124 - root_mean_squared_error: 0.1113 - val_loss: 0.0667 - val_root_mean_squared_error: 0.2582\n",
      "Epoch 23/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0125 - root_mean_squared_error: 0.1116 - val_loss: 0.0361 - val_root_mean_squared_error: 0.1900\n",
      "Epoch 24/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0123 - root_mean_squared_error: 0.1110 - val_loss: 0.0296 - val_root_mean_squared_error: 0.1722\n",
      "Epoch 25/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0122 - root_mean_squared_error: 0.1106 - val_loss: 0.0275 - val_root_mean_squared_error: 0.1658\n",
      "Epoch 26/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0122 - root_mean_squared_error: 0.1105 - val_loss: 0.0334 - val_root_mean_squared_error: 0.1829\n",
      "Epoch 27/200\n",
      "\u001b[1m5202/5202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - loss: 0.0121 - root_mean_squared_error: 0.1102 - val_loss: 0.0237 - val_root_mean_squared_error: 0.1540\n",
      "PROCESANDO ARCHIVO: Juniperus spp. L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/.venv/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 50 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m6,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │         \u001b[38;5;34m1,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m41\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,829</span> (374.34 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m95,829\u001b[0m (374.34 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,793</span> (124.19 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,793\u001b[0m (124.19 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,588</span> (248.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m63,588\u001b[0m (248.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0154 - root_mean_squared_error: 0.1240 - val_loss: 0.0198 - val_root_mean_squared_error: 0.1407\n",
      "Epoch 2/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1238 - val_loss: 0.0217 - val_root_mean_squared_error: 0.1474\n",
      "Epoch 3/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1237 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1225\n",
      "Epoch 4/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1236 - val_loss: 0.0177 - val_root_mean_squared_error: 0.1330\n",
      "Epoch 5/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0153 - root_mean_squared_error: 0.1236 - val_loss: 0.0297 - val_root_mean_squared_error: 0.1725\n",
      "Epoch 6/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0152 - root_mean_squared_error: 0.1234 - val_loss: 0.0181 - val_root_mean_squared_error: 0.1345\n",
      "Epoch 7/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0152 - root_mean_squared_error: 0.1233 - val_loss: 0.0252 - val_root_mean_squared_error: 0.1588\n",
      "Epoch 8/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0152 - root_mean_squared_error: 0.1231 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1223\n",
      "Epoch 9/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1231 - val_loss: 0.0288 - val_root_mean_squared_error: 0.1696\n",
      "Epoch 10/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1230 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1243\n",
      "Epoch 11/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0195 - val_root_mean_squared_error: 0.1396\n",
      "Epoch 12/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1228 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1313\n",
      "Epoch 13/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0151 - root_mean_squared_error: 0.1228 - val_loss: 0.0228 - val_root_mean_squared_error: 0.1510\n",
      "Epoch 14/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1226 - val_loss: 0.0208 - val_root_mean_squared_error: 0.1443\n",
      "Epoch 15/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1226 - val_loss: 0.0186 - val_root_mean_squared_error: 0.1362\n",
      "Epoch 16/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1225 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1243\n",
      "Epoch 17/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1224 - val_loss: 0.0184 - val_root_mean_squared_error: 0.1358\n",
      "Epoch 18/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1223 - val_loss: 0.0281 - val_root_mean_squared_error: 0.1675\n",
      "Epoch 19/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0149 - root_mean_squared_error: 0.1223 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1221\n",
      "Epoch 20/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0150 - root_mean_squared_error: 0.1223 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1320\n",
      "Epoch 21/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0149 - root_mean_squared_error: 0.1221 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1263\n",
      "Epoch 22/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0149 - root_mean_squared_error: 0.1221 - val_loss: 0.0285 - val_root_mean_squared_error: 0.1689\n",
      "Epoch 23/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1218 - val_loss: 0.0378 - val_root_mean_squared_error: 0.1945\n",
      "Epoch 24/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1218 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1307\n",
      "Epoch 25/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1218 - val_loss: 0.0319 - val_root_mean_squared_error: 0.1787\n",
      "Epoch 26/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1215 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1227\n",
      "Epoch 27/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0148 - root_mean_squared_error: 0.1217 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1304\n",
      "Epoch 28/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1214 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1219\n",
      "Epoch 29/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1213 - val_loss: 0.0218 - val_root_mean_squared_error: 0.1477\n",
      "Epoch 30/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1214 - val_loss: 0.0370 - val_root_mean_squared_error: 0.1925\n",
      "Epoch 31/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1212 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1203\n",
      "Epoch 32/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1214 - val_loss: 0.0194 - val_root_mean_squared_error: 0.1393\n",
      "Epoch 33/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1213 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1252\n",
      "Epoch 34/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1212 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 35/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0147 - root_mean_squared_error: 0.1211 - val_loss: 0.0186 - val_root_mean_squared_error: 0.1364\n",
      "Epoch 36/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0146 - root_mean_squared_error: 0.1208 - val_loss: 0.0316 - val_root_mean_squared_error: 0.1777\n",
      "Epoch 37/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0146 - root_mean_squared_error: 0.1210 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1219\n",
      "Epoch 38/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0146 - root_mean_squared_error: 0.1208 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1307\n",
      "Epoch 39/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0146 - root_mean_squared_error: 0.1208 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1294\n",
      "Epoch 40/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0146 - root_mean_squared_error: 0.1208 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1279\n",
      "Epoch 41/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1206 - val_loss: 0.0176 - val_root_mean_squared_error: 0.1325\n",
      "Epoch 42/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1206 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1212\n",
      "Epoch 43/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1205 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1288\n",
      "Epoch 44/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1204 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1212\n",
      "Epoch 45/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1204 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1209\n",
      "Epoch 46/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1205 - val_loss: 0.0178 - val_root_mean_squared_error: 0.1334\n",
      "Epoch 47/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1202 - val_loss: 0.0144 - val_root_mean_squared_error: 0.1199\n",
      "Epoch 48/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1203 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1207\n",
      "Epoch 49/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1203 - val_loss: 0.0206 - val_root_mean_squared_error: 0.1436\n",
      "Epoch 50/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1205 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1244\n",
      "Epoch 51/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1204 - val_loss: 0.0189 - val_root_mean_squared_error: 0.1375\n",
      "Epoch 52/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1200 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1312\n",
      "Epoch 53/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1201 - val_loss: 0.0178 - val_root_mean_squared_error: 0.1334\n",
      "Epoch 54/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1203 - val_loss: 0.0194 - val_root_mean_squared_error: 0.1394\n",
      "Epoch 55/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1200 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1212\n",
      "Epoch 56/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1198 - val_loss: 0.0200 - val_root_mean_squared_error: 0.1414\n",
      "Epoch 57/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1199 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1256\n",
      "Epoch 58/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1199 - val_loss: 0.0235 - val_root_mean_squared_error: 0.1531\n",
      "Epoch 59/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1197 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1241\n",
      "Epoch 60/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1197 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1298\n",
      "Epoch 61/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1197 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1242\n",
      "Epoch 62/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1196 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1237\n",
      "Epoch 63/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194 - val_loss: 0.0238 - val_root_mean_squared_error: 0.1542\n",
      "Epoch 64/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1196 - val_loss: 0.0274 - val_root_mean_squared_error: 0.1657\n",
      "Epoch 65/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1225\n",
      "Epoch 66/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1195 - val_loss: 0.0189 - val_root_mean_squared_error: 0.1374\n",
      "Epoch 67/200\n",
      "\u001b[1m6891/6891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194 - val_loss: 0.0228 - val_root_mean_squared_error: 0.1508\n",
      "PROCESANDO ARCHIVO: Juniperus turkestanica Komar\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_11 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m27,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">548,453</span> (2.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m548,453\u001b[0m (2.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">182,369</span> (712.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m182,369\u001b[0m (712.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">364,740</span> (1.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m364,740\u001b[0m (1.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1244 - val_loss: 0.0264 - val_root_mean_squared_error: 0.1624\n",
      "Epoch 2/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0159 - root_mean_squared_error: 0.1261 - val_loss: 0.0181 - val_root_mean_squared_error: 0.1345\n",
      "Epoch 3/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1244 - val_loss: 0.0244 - val_root_mean_squared_error: 0.1561\n",
      "Epoch 4/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0156 - root_mean_squared_error: 0.1250 - val_loss: 0.0243 - val_root_mean_squared_error: 0.1560\n",
      "Epoch 5/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1246 - val_loss: 0.0198 - val_root_mean_squared_error: 0.1408\n",
      "Epoch 6/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0286 - val_root_mean_squared_error: 0.1693\n",
      "Epoch 7/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0151 - root_mean_squared_error: 0.1230 - val_loss: 0.0241 - val_root_mean_squared_error: 0.1552\n",
      "Epoch 8/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0150 - root_mean_squared_error: 0.1226 - val_loss: 0.0178 - val_root_mean_squared_error: 0.1335\n",
      "Epoch 9/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0148 - root_mean_squared_error: 0.1218 - val_loss: 0.0188 - val_root_mean_squared_error: 0.1373\n",
      "Epoch 10/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0148 - root_mean_squared_error: 0.1218 - val_loss: 0.0260 - val_root_mean_squared_error: 0.1612\n",
      "Epoch 11/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0148 - root_mean_squared_error: 0.1217 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1273\n",
      "Epoch 12/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0147 - root_mean_squared_error: 0.1213 - val_loss: 0.0201 - val_root_mean_squared_error: 0.1416\n",
      "Epoch 13/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0146 - root_mean_squared_error: 0.1210 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1281\n",
      "Epoch 14/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0146 - root_mean_squared_error: 0.1210 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1260\n",
      "Epoch 15/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0146 - root_mean_squared_error: 0.1208 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1311\n",
      "Epoch 16/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0154 - root_mean_squared_error: 0.1239 - val_loss: 0.0282 - val_root_mean_squared_error: 0.1680\n",
      "Epoch 17/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0144 - root_mean_squared_error: 0.1201 - val_loss: 0.0210 - val_root_mean_squared_error: 0.1448\n",
      "Epoch 18/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0145 - root_mean_squared_error: 0.1203 - val_loss: 0.0225 - val_root_mean_squared_error: 0.1502\n",
      "Epoch 19/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0144 - root_mean_squared_error: 0.1199 - val_loss: 0.0200 - val_root_mean_squared_error: 0.1414\n",
      "Epoch 20/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0144 - root_mean_squared_error: 0.1199 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1261\n",
      "Epoch 21/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0143 - root_mean_squared_error: 0.1197 - val_loss: 0.0254 - val_root_mean_squared_error: 0.1593\n",
      "Epoch 22/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0144 - root_mean_squared_error: 0.1201 - val_loss: 0.0360 - val_root_mean_squared_error: 0.1897\n",
      "Epoch 23/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0144 - root_mean_squared_error: 0.1198 - val_loss: 0.0263 - val_root_mean_squared_error: 0.1622\n",
      "Epoch 24/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0145 - root_mean_squared_error: 0.1205 - val_loss: 0.0260 - val_root_mean_squared_error: 0.1612\n",
      "Epoch 25/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0144 - root_mean_squared_error: 0.1200 - val_loss: 0.0204 - val_root_mean_squared_error: 0.1428\n",
      "Epoch 26/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0141 - root_mean_squared_error: 0.1189 - val_loss: 0.0232 - val_root_mean_squared_error: 0.1523\n",
      "Epoch 27/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0143 - root_mean_squared_error: 0.1197 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1231\n",
      "Epoch 28/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0142 - root_mean_squared_error: 0.1191 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1225\n",
      "Epoch 29/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0143 - root_mean_squared_error: 0.1195 - val_loss: 0.0688 - val_root_mean_squared_error: 0.2623\n",
      "Epoch 30/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0142 - root_mean_squared_error: 0.1194 - val_loss: 0.0212 - val_root_mean_squared_error: 0.1457\n",
      "Epoch 31/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0140 - root_mean_squared_error: 0.1184 - val_loss: 0.0304 - val_root_mean_squared_error: 0.1742\n",
      "Epoch 32/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0140 - root_mean_squared_error: 0.1184 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1288\n",
      "Epoch 33/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0138 - root_mean_squared_error: 0.1174 - val_loss: 0.0189 - val_root_mean_squared_error: 0.1375\n",
      "Epoch 34/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0142 - root_mean_squared_error: 0.1190 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1258\n",
      "Epoch 35/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0139 - root_mean_squared_error: 0.1181 - val_loss: 0.0210 - val_root_mean_squared_error: 0.1449\n",
      "Epoch 36/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0140 - root_mean_squared_error: 0.1182 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1306\n",
      "Epoch 37/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0137 - root_mean_squared_error: 0.1170 - val_loss: 0.0193 - val_root_mean_squared_error: 0.1389\n",
      "Epoch 38/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0137 - root_mean_squared_error: 0.1170 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1221\n",
      "Epoch 39/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1261\n",
      "Epoch 40/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0136 - root_mean_squared_error: 0.1167 - val_loss: 0.0309 - val_root_mean_squared_error: 0.1758\n",
      "Epoch 41/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0138 - root_mean_squared_error: 0.1174 - val_loss: 0.0254 - val_root_mean_squared_error: 0.1594\n",
      "Epoch 42/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0139 - root_mean_squared_error: 0.1177 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1212\n",
      "Epoch 43/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0140 - root_mean_squared_error: 0.1184 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1302\n",
      "Epoch 44/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0207 - val_root_mean_squared_error: 0.1439\n",
      "Epoch 45/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0136 - root_mean_squared_error: 0.1167 - val_loss: 0.0207 - val_root_mean_squared_error: 0.1439\n",
      "Epoch 46/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0136 - root_mean_squared_error: 0.1165 - val_loss: 0.0199 - val_root_mean_squared_error: 0.1411\n",
      "Epoch 47/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0136 - root_mean_squared_error: 0.1165 - val_loss: 0.0314 - val_root_mean_squared_error: 0.1772\n",
      "Epoch 48/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1157 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1252\n",
      "Epoch 49/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1155 - val_loss: 0.0192 - val_root_mean_squared_error: 0.1387\n",
      "Epoch 50/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0135 - root_mean_squared_error: 0.1162 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1378\n",
      "Epoch 51/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0133 - root_mean_squared_error: 0.1155 - val_loss: 0.0186 - val_root_mean_squared_error: 0.1364\n",
      "Epoch 52/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0136 - root_mean_squared_error: 0.1167 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1220\n",
      "Epoch 53/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0137 - root_mean_squared_error: 0.1171 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1233\n",
      "Epoch 54/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0138 - root_mean_squared_error: 0.1174 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1265\n",
      "Epoch 55/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0133 - root_mean_squared_error: 0.1152 - val_loss: 0.0204 - val_root_mean_squared_error: 0.1427\n",
      "Epoch 56/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1158 - val_loss: 0.0185 - val_root_mean_squared_error: 0.1361\n",
      "Epoch 57/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0133 - root_mean_squared_error: 0.1153 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1242\n",
      "Epoch 58/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1158 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1311\n",
      "Epoch 59/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0130 - root_mean_squared_error: 0.1142 - val_loss: 0.0189 - val_root_mean_squared_error: 0.1374\n",
      "Epoch 60/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0133 - root_mean_squared_error: 0.1152 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1228\n",
      "Epoch 61/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0133 - root_mean_squared_error: 0.1155 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1274\n",
      "Epoch 62/200\n",
      "\u001b[1m5186/5186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 0.0133 - root_mean_squared_error: 0.1154 - val_loss: 0.0226 - val_root_mean_squared_error: 0.1505\n",
      "PROCESANDO ARCHIVO: Pinus roxburghii\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m12,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m27,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m27,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,621</span> (842.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m215,621\u001b[0m (842.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,681</span> (280.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m71,681\u001b[0m (280.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> (2.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m576\u001b[0m (2.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">143,364</span> (560.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m143,364\u001b[0m (560.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0127 - root_mean_squared_error: 0.1129 - val_loss: 0.0194 - val_root_mean_squared_error: 0.1394\n",
      "Epoch 2/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0127 - root_mean_squared_error: 0.1125 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1339\n",
      "Epoch 3/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0126 - root_mean_squared_error: 0.1123 - val_loss: 0.0193 - val_root_mean_squared_error: 0.1391\n",
      "Epoch 4/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0126 - root_mean_squared_error: 0.1122 - val_loss: 0.0195 - val_root_mean_squared_error: 0.1397\n",
      "Epoch 5/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0125 - root_mean_squared_error: 0.1118 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1379\n",
      "Epoch 6/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0124 - root_mean_squared_error: 0.1115 - val_loss: 0.0169 - val_root_mean_squared_error: 0.1299\n",
      "Epoch 7/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0124 - root_mean_squared_error: 0.1113 - val_loss: 0.0194 - val_root_mean_squared_error: 0.1392\n",
      "Epoch 8/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0123 - root_mean_squared_error: 0.1109 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1228\n",
      "Epoch 9/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0123 - root_mean_squared_error: 0.1108 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1225\n",
      "Epoch 10/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0122 - root_mean_squared_error: 0.1104 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1298\n",
      "Epoch 11/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0121 - root_mean_squared_error: 0.1102 - val_loss: 0.0233 - val_root_mean_squared_error: 0.1528\n",
      "Epoch 12/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0121 - root_mean_squared_error: 0.1100 - val_loss: 0.0230 - val_root_mean_squared_error: 0.1517\n",
      "Epoch 13/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0120 - root_mean_squared_error: 0.1095 - val_loss: 0.0182 - val_root_mean_squared_error: 0.1348\n",
      "Epoch 14/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0120 - root_mean_squared_error: 0.1095 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1286\n",
      "Epoch 15/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0119 - root_mean_squared_error: 0.1092 - val_loss: 0.0181 - val_root_mean_squared_error: 0.1344\n",
      "Epoch 16/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0119 - root_mean_squared_error: 0.1089 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1307\n",
      "Epoch 17/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0118 - root_mean_squared_error: 0.1088 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1286\n",
      "Epoch 18/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0118 - root_mean_squared_error: 0.1086 - val_loss: 0.0196 - val_root_mean_squared_error: 0.1401\n",
      "Epoch 19/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 0.0118 - root_mean_squared_error: 0.1085 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1288\n",
      "Epoch 20/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0117 - root_mean_squared_error: 0.1083 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1377\n",
      "Epoch 21/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0116 - root_mean_squared_error: 0.1077 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1377\n",
      "Epoch 22/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0116 - root_mean_squared_error: 0.1076 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1264\n",
      "Epoch 23/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0115 - root_mean_squared_error: 0.1074 - val_loss: 0.0191 - val_root_mean_squared_error: 0.1383\n",
      "Epoch 24/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0115 - root_mean_squared_error: 0.1073 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1254\n",
      "Epoch 25/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0115 - root_mean_squared_error: 0.1071 - val_loss: 0.0204 - val_root_mean_squared_error: 0.1428\n",
      "Epoch 26/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0115 - root_mean_squared_error: 0.1070 - val_loss: 0.0207 - val_root_mean_squared_error: 0.1437\n",
      "Epoch 27/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0114 - root_mean_squared_error: 0.1069 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1338\n",
      "Epoch 28/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0114 - root_mean_squared_error: 0.1067 - val_loss: 0.0217 - val_root_mean_squared_error: 0.1473\n",
      "Epoch 29/200\n",
      "\u001b[1m6799/6799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0113 - root_mean_squared_error: 0.1064 - val_loss: 0.0214 - val_root_mean_squared_error: 0.1464\n",
      "PROCESANDO ARCHIVO: Juniperus excelsa M\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │        \u001b[38;5;34m12,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │         \u001b[38;5;34m6,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m49\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">591,813</span> (2.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m591,813\u001b[0m (2.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">196,801</span> (768.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m196,801\u001b[0m (768.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> (5.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,408\u001b[0m (5.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">393,604</span> (1.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m393,604\u001b[0m (1.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 4ms/step - loss: 0.0149 - root_mean_squared_error: 0.1221 - val_loss: 0.0177 - val_root_mean_squared_error: 0.1329\n",
      "Epoch 2/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0148 - root_mean_squared_error: 0.1218 - val_loss: 0.0411 - val_root_mean_squared_error: 0.2028\n",
      "Epoch 3/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0147 - root_mean_squared_error: 0.1214 - val_loss: 0.0206 - val_root_mean_squared_error: 0.1436\n",
      "Epoch 4/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0146 - root_mean_squared_error: 0.1210 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1287\n",
      "Epoch 5/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0145 - root_mean_squared_error: 0.1205 - val_loss: 0.0276 - val_root_mean_squared_error: 0.1660\n",
      "Epoch 6/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0144 - root_mean_squared_error: 0.1201 - val_loss: 0.0267 - val_root_mean_squared_error: 0.1634\n",
      "Epoch 7/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0144 - root_mean_squared_error: 0.1199 - val_loss: 0.0289 - val_root_mean_squared_error: 0.1701\n",
      "Epoch 8/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194 - val_loss: 0.0239 - val_root_mean_squared_error: 0.1547\n",
      "Epoch 9/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0142 - root_mean_squared_error: 0.1192 - val_loss: 0.0321 - val_root_mean_squared_error: 0.1790\n",
      "Epoch 10/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0141 - root_mean_squared_error: 0.1188 - val_loss: 0.0349 - val_root_mean_squared_error: 0.1869\n",
      "Epoch 11/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0140 - root_mean_squared_error: 0.1183 - val_loss: 0.0342 - val_root_mean_squared_error: 0.1850\n",
      "Epoch 12/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0139 - root_mean_squared_error: 0.1180 - val_loss: 0.0226 - val_root_mean_squared_error: 0.1503\n",
      "Epoch 13/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0138 - root_mean_squared_error: 0.1176 - val_loss: 0.0364 - val_root_mean_squared_error: 0.1907\n",
      "Epoch 14/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0137 - root_mean_squared_error: 0.1172 - val_loss: 0.0205 - val_root_mean_squared_error: 0.1432\n",
      "Epoch 15/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0137 - root_mean_squared_error: 0.1170 - val_loss: 0.0320 - val_root_mean_squared_error: 0.1789\n",
      "Epoch 16/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0136 - root_mean_squared_error: 0.1167 - val_loss: 0.0311 - val_root_mean_squared_error: 0.1765\n",
      "Epoch 17/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0135 - root_mean_squared_error: 0.1162 - val_loss: 0.0184 - val_root_mean_squared_error: 0.1357\n",
      "Epoch 18/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0134 - root_mean_squared_error: 0.1157 - val_loss: 0.0342 - val_root_mean_squared_error: 0.1849\n",
      "Epoch 19/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0134 - root_mean_squared_error: 0.1158 - val_loss: 0.0221 - val_root_mean_squared_error: 0.1487\n",
      "Epoch 20/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0133 - root_mean_squared_error: 0.1152 - val_loss: 0.0249 - val_root_mean_squared_error: 0.1578\n",
      "Epoch 21/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0132 - root_mean_squared_error: 0.1147 - val_loss: 0.0203 - val_root_mean_squared_error: 0.1424\n",
      "Epoch 22/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0130 - root_mean_squared_error: 0.1141 - val_loss: 0.0377 - val_root_mean_squared_error: 0.1941\n",
      "Epoch 23/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0130 - root_mean_squared_error: 0.1139 - val_loss: 0.0297 - val_root_mean_squared_error: 0.1722\n",
      "Epoch 24/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0128 - root_mean_squared_error: 0.1132 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1280\n",
      "Epoch 25/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0128 - root_mean_squared_error: 0.1133 - val_loss: 0.0360 - val_root_mean_squared_error: 0.1897\n",
      "Epoch 26/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0128 - root_mean_squared_error: 0.1129 - val_loss: 0.0357 - val_root_mean_squared_error: 0.1889\n",
      "Epoch 27/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0127 - root_mean_squared_error: 0.1125 - val_loss: 0.0282 - val_root_mean_squared_error: 0.1679\n",
      "Epoch 28/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0126 - root_mean_squared_error: 0.1120 - val_loss: 0.0224 - val_root_mean_squared_error: 0.1498\n",
      "Epoch 29/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0124 - root_mean_squared_error: 0.1115 - val_loss: 0.0214 - val_root_mean_squared_error: 0.1461\n",
      "Epoch 30/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0123 - root_mean_squared_error: 0.1110 - val_loss: 0.0243 - val_root_mean_squared_error: 0.1560\n",
      "Epoch 31/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0123 - root_mean_squared_error: 0.1109 - val_loss: 0.0217 - val_root_mean_squared_error: 0.1474\n",
      "Epoch 32/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0122 - root_mean_squared_error: 0.1105 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1277\n",
      "Epoch 33/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0121 - root_mean_squared_error: 0.1099 - val_loss: 0.0176 - val_root_mean_squared_error: 0.1325\n",
      "Epoch 34/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0120 - root_mean_squared_error: 0.1096 - val_loss: 0.0222 - val_root_mean_squared_error: 0.1489\n",
      "Epoch 35/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0120 - root_mean_squared_error: 0.1094 - val_loss: 0.0219 - val_root_mean_squared_error: 0.1480\n",
      "Epoch 36/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0119 - root_mean_squared_error: 0.1092 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1251\n",
      "Epoch 37/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0118 - root_mean_squared_error: 0.1087 - val_loss: 0.0208 - val_root_mean_squared_error: 0.1441\n",
      "Epoch 38/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0117 - root_mean_squared_error: 0.1081 - val_loss: 0.0250 - val_root_mean_squared_error: 0.1581\n",
      "Epoch 39/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0118 - root_mean_squared_error: 0.1084 - val_loss: 0.0214 - val_root_mean_squared_error: 0.1464\n",
      "Epoch 40/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0116 - root_mean_squared_error: 0.1079 - val_loss: 0.0262 - val_root_mean_squared_error: 0.1619\n",
      "Epoch 41/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0116 - root_mean_squared_error: 0.1076 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1305\n",
      "Epoch 42/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0115 - root_mean_squared_error: 0.1070 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1378\n",
      "Epoch 43/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0115 - root_mean_squared_error: 0.1071 - val_loss: 0.0308 - val_root_mean_squared_error: 0.1755\n",
      "Epoch 44/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0113 - root_mean_squared_error: 0.1064 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1279\n",
      "Epoch 45/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0114 - root_mean_squared_error: 0.1065 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1247\n",
      "Epoch 46/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0113 - root_mean_squared_error: 0.1061 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1303\n",
      "Epoch 47/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0112 - root_mean_squared_error: 0.1056 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1313\n",
      "Epoch 48/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0110 - root_mean_squared_error: 0.1051 - val_loss: 0.0185 - val_root_mean_squared_error: 0.1362\n",
      "Epoch 49/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0111 - root_mean_squared_error: 0.1053 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1286\n",
      "Epoch 50/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0109 - root_mean_squared_error: 0.1043 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1312\n",
      "Epoch 51/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0110 - root_mean_squared_error: 0.1047 - val_loss: 0.0184 - val_root_mean_squared_error: 0.1355\n",
      "Epoch 52/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0108 - root_mean_squared_error: 0.1039 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1318\n",
      "Epoch 53/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0108 - root_mean_squared_error: 0.1039 - val_loss: 0.0169 - val_root_mean_squared_error: 0.1299\n",
      "Epoch 54/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0108 - root_mean_squared_error: 0.1037 - val_loss: 0.0207 - val_root_mean_squared_error: 0.1440\n",
      "Epoch 55/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0106 - root_mean_squared_error: 0.1032 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1307\n",
      "Epoch 56/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0105 - root_mean_squared_error: 0.1027 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1284\n",
      "Epoch 57/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0105 - root_mean_squared_error: 0.1023 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1338\n",
      "Epoch 58/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0105 - root_mean_squared_error: 0.1024 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1285\n",
      "Epoch 59/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0103 - root_mean_squared_error: 0.1017 - val_loss: 0.0169 - val_root_mean_squared_error: 0.1300\n",
      "Epoch 60/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0103 - root_mean_squared_error: 0.1016 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1305\n",
      "Epoch 61/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0104 - root_mean_squared_error: 0.1019 - val_loss: 0.0198 - val_root_mean_squared_error: 0.1406\n",
      "Epoch 62/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0103 - root_mean_squared_error: 0.1013 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1291\n",
      "Epoch 63/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0102 - root_mean_squared_error: 0.1009 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1276\n",
      "Epoch 64/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0102 - root_mean_squared_error: 0.1010 - val_loss: 0.0192 - val_root_mean_squared_error: 0.1384\n",
      "Epoch 65/200\n",
      "\u001b[1m10338/10338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0102 - root_mean_squared_error: 0.1008 - val_loss: 0.0204 - val_root_mean_squared_error: 0.1429\n",
      "PROCESANDO ARCHIVO: Juniperus spp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m24,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m2,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">703,013</span> (2.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m703,013\u001b[0m (2.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">233,825</span> (913.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m233,825\u001b[0m (913.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">467,652</span> (1.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m467,652\u001b[0m (1.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5ms/step - loss: 0.0136 - root_mean_squared_error: 0.1168 - val_loss: 0.0173 - val_root_mean_squared_error: 0.1315\n",
      "Epoch 2/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0219 - val_root_mean_squared_error: 0.1479\n",
      "Epoch 3/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0173 - root_mean_squared_error: 0.1314 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1197\n",
      "Epoch 4/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0138 - root_mean_squared_error: 0.1174 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1219\n",
      "Epoch 5/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0137 - root_mean_squared_error: 0.1172 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1260\n",
      "Epoch 6/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1156 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1217\n",
      "Epoch 7/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0135 - root_mean_squared_error: 0.1163 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1261\n",
      "Epoch 8/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0141 - root_mean_squared_error: 0.1188 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1196\n",
      "Epoch 9/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0138 - root_mean_squared_error: 0.1175 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1243\n",
      "Epoch 10/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0148 - root_mean_squared_error: 0.1217 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1196\n",
      "Epoch 11/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0132 - root_mean_squared_error: 0.1150 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1207\n",
      "Epoch 12/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0134 - root_mean_squared_error: 0.1157 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1241\n",
      "Epoch 13/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0130 - root_mean_squared_error: 0.1140 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1291\n",
      "Epoch 14/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0155 - root_mean_squared_error: 0.1246 - val_loss: 0.0712 - val_root_mean_squared_error: 0.2668\n",
      "Epoch 15/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0141 - root_mean_squared_error: 0.1189 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1259\n",
      "Epoch 16/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0131 - root_mean_squared_error: 0.1143 - val_loss: 0.0139 - val_root_mean_squared_error: 0.1178\n",
      "Epoch 17/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0129 - root_mean_squared_error: 0.1136 - val_loss: 0.0178 - val_root_mean_squared_error: 0.1335\n",
      "Epoch 18/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0132 - root_mean_squared_error: 0.1150 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1209\n",
      "Epoch 19/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0127 - root_mean_squared_error: 0.1127 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1174\n",
      "Epoch 20/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0128 - root_mean_squared_error: 0.1131 - val_loss: 0.0200 - val_root_mean_squared_error: 0.1416\n",
      "Epoch 21/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0129 - root_mean_squared_error: 0.1137 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1194\n",
      "Epoch 22/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0130 - root_mean_squared_error: 0.1138 - val_loss: 0.0237 - val_root_mean_squared_error: 0.1541\n",
      "Epoch 23/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0129 - root_mean_squared_error: 0.1137 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1282\n",
      "Epoch 24/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0125 - root_mean_squared_error: 0.1120 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1219\n",
      "Epoch 25/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0122 - root_mean_squared_error: 0.1106 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1175\n",
      "Epoch 26/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0123 - root_mean_squared_error: 0.1110 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1318\n",
      "Epoch 27/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0123 - root_mean_squared_error: 0.1111 - val_loss: 0.0185 - val_root_mean_squared_error: 0.1361\n",
      "Epoch 28/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0121 - root_mean_squared_error: 0.1098 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1281\n",
      "Epoch 29/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0121 - root_mean_squared_error: 0.1100 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1237\n",
      "Epoch 30/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0132 - root_mean_squared_error: 0.1148 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1208\n",
      "Epoch 31/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0125 - root_mean_squared_error: 0.1120 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1182\n",
      "Epoch 32/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0119 - root_mean_squared_error: 0.1091 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1240\n",
      "Epoch 33/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0119 - root_mean_squared_error: 0.1092 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1169\n",
      "Epoch 34/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0126 - root_mean_squared_error: 0.1122 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1218\n",
      "Epoch 35/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0116 - root_mean_squared_error: 0.1079 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 36/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0116 - root_mean_squared_error: 0.1077 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1158\n",
      "Epoch 37/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0119 - root_mean_squared_error: 0.1090 - val_loss: 0.0195 - val_root_mean_squared_error: 0.1396\n",
      "Epoch 38/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0121 - root_mean_squared_error: 0.1098 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1204\n",
      "Epoch 39/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0114 - root_mean_squared_error: 0.1070 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1173\n",
      "Epoch 40/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0116 - root_mean_squared_error: 0.1079 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1260\n",
      "Epoch 41/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0116 - root_mean_squared_error: 0.1077 - val_loss: 0.0177 - val_root_mean_squared_error: 0.1332\n",
      "Epoch 42/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0115 - root_mean_squared_error: 0.1073 - val_loss: 0.0141 - val_root_mean_squared_error: 0.1186\n",
      "Epoch 43/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0114 - root_mean_squared_error: 0.1067 - val_loss: 0.0144 - val_root_mean_squared_error: 0.1202\n",
      "Epoch 44/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0113 - root_mean_squared_error: 0.1063 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1183\n",
      "Epoch 45/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0112 - root_mean_squared_error: 0.1058 - val_loss: 0.0219 - val_root_mean_squared_error: 0.1480\n",
      "Epoch 46/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0114 - root_mean_squared_error: 0.1067 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1258\n",
      "Epoch 47/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0109 - root_mean_squared_error: 0.1045 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1177\n",
      "Epoch 48/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0111 - root_mean_squared_error: 0.1053 - val_loss: 0.0189 - val_root_mean_squared_error: 0.1375\n",
      "Epoch 49/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0109 - root_mean_squared_error: 0.1043 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1182\n",
      "Epoch 50/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0107 - root_mean_squared_error: 0.1036 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 51/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0107 - root_mean_squared_error: 0.1033 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1225\n",
      "Epoch 52/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0107 - root_mean_squared_error: 0.1035 - val_loss: 0.0275 - val_root_mean_squared_error: 0.1660\n",
      "Epoch 53/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0108 - root_mean_squared_error: 0.1041 - val_loss: 0.0139 - val_root_mean_squared_error: 0.1178\n",
      "Epoch 54/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0108 - root_mean_squared_error: 0.1041 - val_loss: 0.0142 - val_root_mean_squared_error: 0.1191\n",
      "Epoch 55/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0104 - root_mean_squared_error: 0.1020 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1198\n",
      "Epoch 56/200\n",
      "\u001b[1m4971/4971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - loss: 0.0106 - root_mean_squared_error: 0.1032 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1283\n",
      "PROCESANDO ARCHIVO: Populus ciliata\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m12,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m36,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m36,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">547,685</span> (2.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m547,685\u001b[0m (2.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">182,177</span> (711.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m182,177\u001b[0m (711.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> (4.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,152\u001b[0m (4.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">364,356</span> (1.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m364,356\u001b[0m (1.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 4ms/step - loss: 0.0147 - root_mean_squared_error: 0.1211 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1224\n",
      "Epoch 2/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0147 - root_mean_squared_error: 0.1211 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1217\n",
      "Epoch 3/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0146 - root_mean_squared_error: 0.1209 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1249\n",
      "Epoch 4/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0145 - root_mean_squared_error: 0.1205 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1210\n",
      "Epoch 5/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0144 - root_mean_squared_error: 0.1200 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1198\n",
      "Epoch 6/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0144 - root_mean_squared_error: 0.1199 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1283\n",
      "Epoch 7/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0143 - root_mean_squared_error: 0.1197 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1219\n",
      "Epoch 8/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0142 - root_mean_squared_error: 0.1193 - val_loss: 0.0240 - val_root_mean_squared_error: 0.1550\n",
      "Epoch 9/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0143 - root_mean_squared_error: 0.1196 - val_loss: 0.0258 - val_root_mean_squared_error: 0.1605\n",
      "Epoch 10/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0141 - root_mean_squared_error: 0.1189 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1281\n",
      "Epoch 11/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0141 - root_mean_squared_error: 0.1187 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1259\n",
      "Epoch 12/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0141 - root_mean_squared_error: 0.1186 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1208\n",
      "Epoch 13/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0140 - root_mean_squared_error: 0.1182 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1236\n",
      "Epoch 14/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0139 - root_mean_squared_error: 0.1178 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1378\n",
      "Epoch 15/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4ms/step - loss: 0.0140 - root_mean_squared_error: 0.1183 - val_loss: 0.0209 - val_root_mean_squared_error: 0.1445\n",
      "Epoch 16/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0138 - root_mean_squared_error: 0.1177 - val_loss: 0.0366 - val_root_mean_squared_error: 0.1914\n",
      "Epoch 17/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0138 - root_mean_squared_error: 0.1176 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1277\n",
      "Epoch 18/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0138 - root_mean_squared_error: 0.1174 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1234\n",
      "Epoch 19/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1207\n",
      "Epoch 20/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0136 - root_mean_squared_error: 0.1168 - val_loss: 0.0192 - val_root_mean_squared_error: 0.1384\n",
      "Epoch 21/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0136 - root_mean_squared_error: 0.1167 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1235\n",
      "Epoch 22/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0136 - root_mean_squared_error: 0.1165 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1228\n",
      "Epoch 23/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4ms/step - loss: 0.0136 - root_mean_squared_error: 0.1167 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1278\n",
      "Epoch 24/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0135 - root_mean_squared_error: 0.1163 - val_loss: 0.0193 - val_root_mean_squared_error: 0.1388\n",
      "Epoch 25/200\n",
      "\u001b[1m5283/5283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0134 - root_mean_squared_error: 0.1160 - val_loss: 0.0215 - val_root_mean_squared_error: 0.1465\n",
      "PROCESANDO ARCHIVO: Abies pindrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m36,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m36,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │        \u001b[38;5;34m27,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m96\u001b[0m)          │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │         \u001b[38;5;34m4,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m49\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">484,357</span> (1.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m484,357\u001b[0m (1.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,089</span> (629.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m161,089\u001b[0m (629.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> (4.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,088\u001b[0m (4.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">322,180</span> (1.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m322,180\u001b[0m (1.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 3ms/step - loss: 0.0146 - root_mean_squared_error: 0.1208 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1291\n",
      "Epoch 2/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1205 - val_loss: 0.0184 - val_root_mean_squared_error: 0.1357\n",
      "Epoch 3/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3ms/step - loss: 0.0145 - root_mean_squared_error: 0.1202 - val_loss: 0.0210 - val_root_mean_squared_error: 0.1450\n",
      "Epoch 4/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1202 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1227\n",
      "Epoch 5/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1196 - val_loss: 0.0260 - val_root_mean_squared_error: 0.1612\n",
      "Epoch 6/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0144 - root_mean_squared_error: 0.1199 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
      "Epoch 7/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1267\n",
      "Epoch 8/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1222\n",
      "Epoch 9/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0142 - root_mean_squared_error: 0.1191 - val_loss: 0.0186 - val_root_mean_squared_error: 0.1362\n",
      "Epoch 10/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0142 - root_mean_squared_error: 0.1190 - val_loss: 0.0321 - val_root_mean_squared_error: 0.1792\n",
      "Epoch 11/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0141 - root_mean_squared_error: 0.1189 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1238\n",
      "Epoch 12/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0140 - root_mean_squared_error: 0.1183 - val_loss: 0.0187 - val_root_mean_squared_error: 0.1369\n",
      "Epoch 13/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0140 - root_mean_squared_error: 0.1182 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1294\n",
      "Epoch 14/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0139 - root_mean_squared_error: 0.1179 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1219\n",
      "Epoch 15/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0139 - root_mean_squared_error: 0.1180 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1285\n",
      "Epoch 16/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0139 - root_mean_squared_error: 0.1178 - val_loss: 0.0184 - val_root_mean_squared_error: 0.1357\n",
      "Epoch 17/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0138 - root_mean_squared_error: 0.1176 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1221\n",
      "Epoch 18/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3ms/step - loss: 0.0137 - root_mean_squared_error: 0.1172 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1207\n",
      "Epoch 19/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3ms/step - loss: 0.0137 - root_mean_squared_error: 0.1172 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1246\n",
      "Epoch 20/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0137 - root_mean_squared_error: 0.1171 - val_loss: 0.0254 - val_root_mean_squared_error: 0.1595\n",
      "Epoch 21/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0136 - root_mean_squared_error: 0.1168 - val_loss: 0.0238 - val_root_mean_squared_error: 0.1544\n",
      "Epoch 22/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0136 - root_mean_squared_error: 0.1165 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1236\n",
      "Epoch 23/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1163 - val_loss: 0.0181 - val_root_mean_squared_error: 0.1345\n",
      "Epoch 24/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1164 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1310\n",
      "Epoch 25/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1161 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1226\n",
      "Epoch 26/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0135 - root_mean_squared_error: 0.1162 - val_loss: 0.0206 - val_root_mean_squared_error: 0.1435\n",
      "Epoch 27/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0134 - root_mean_squared_error: 0.1158 - val_loss: 0.0177 - val_root_mean_squared_error: 0.1329\n",
      "Epoch 28/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0134 - root_mean_squared_error: 0.1158 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1222\n",
      "Epoch 29/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0133 - root_mean_squared_error: 0.1155 - val_loss: 0.0180 - val_root_mean_squared_error: 0.1343\n",
      "Epoch 30/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0134 - root_mean_squared_error: 0.1157 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1295\n",
      "Epoch 31/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0133 - root_mean_squared_error: 0.1151 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1224\n",
      "Epoch 32/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0133 - root_mean_squared_error: 0.1152 - val_loss: 0.0169 - val_root_mean_squared_error: 0.1301\n",
      "Epoch 33/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0132 - root_mean_squared_error: 0.1150 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1263\n",
      "Epoch 34/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0132 - root_mean_squared_error: 0.1149 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1246\n",
      "Epoch 35/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3ms/step - loss: 0.0132 - root_mean_squared_error: 0.1148 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1250\n",
      "Epoch 36/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0132 - root_mean_squared_error: 0.1149 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1215\n",
      "Epoch 37/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0131 - root_mean_squared_error: 0.1143 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1243\n",
      "Epoch 38/200\n",
      "\u001b[1m10020/10020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - loss: 0.0131 - root_mean_squared_error: 0.1143 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1289\n"
     ]
    }
   ],
   "source": [
    "for archivo in os.listdir(\"models/TCNNMerged_Option2\"):\n",
    "\n",
    "    # Leemos solo los archivos json para obtener los nombres\n",
    "    if os.path.splitext(archivo)[1] == \".json\":\n",
    "        \n",
    "        # Obtenemos el nombre de archivo\n",
    "        nombreArchivo = archivo.split(\"_best_models.json\")[0]\n",
    "\n",
    "        print(f\"PROCESANDO ARCHIVO: {nombreArchivo}\")\n",
    "\n",
    "        data_train = np.load(f'RCPMergedTransfer/{nombreArchivo}_train.npz', allow_pickle=True)\n",
    "        data_val = np.load(f'RCPMergedTransfer/{nombreArchivo}_val.npz', allow_pickle=True)\n",
    "        data_test = np.load(f'RCPMergedTransfer/{nombreArchivo}_test.npz', allow_pickle=True)\n",
    "\n",
    "        X_train = data_train['X_train']\n",
    "        y_train = data_train['y_train']\n",
    "        X_val = data_val['X_val']\n",
    "        y_val = data_val['y_val']\n",
    "        X_test = data_test['X_test']\n",
    "        y_test = data_test['y_test']\n",
    "\n",
    "        # Los valores de normalización estan incluidos en todos los archivos (train, val, test) y cada uno de ellos tiene todo el contenido del resto\n",
    "        valorNormalizacion = data_train[\"valorNormalizacion\"].item()\n",
    "\n",
    "        # Cargamos el modelo global\n",
    "        modelLSTM = tf.keras.models.load_model(f'models/TCNNMerged_Option2/{nombreArchivo}_model_1.keras')\n",
    "\n",
    "        print(modelLSTM.summary())\n",
    "\n",
    "        # Obtenemos el valor de batch size\n",
    "        with open(f'models/TCNNMerged_Option2/{archivo}') as f:\n",
    "            parameters = json.load(f)\n",
    "\n",
    "        batch_size_LSTM = parameters[0][\"batch_size\"]\n",
    "\n",
    "        historyLSTMTransfer = modelLSTM.fit(X_train, y_train, epochs=200, batch_size=batch_size_LSTM,\n",
    "                            validation_data=(X_val, y_val), callbacks=[EarlyStopping(monitor='val_loss', patience=20)])\n",
    "        \n",
    "        modelLSTM.save(f\"models/TCNNMerged_Option2_Trained/{nombreArchivo}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESANDO ARCHIVO: Pinus wallichiana\n",
      "(9365, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Pinus wallichiana_best_models.json\n",
      "(5508, 4, 43) (1957, 4, 43) (1500, 4, 43)\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_6, built=True> False\n",
      "<BatchNormalization name=batch_normalization_6, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Conv1D name=conv1d_7, built=True> True\n",
      "<BatchNormalization name=batch_normalization_7, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0174 - mape: 26.3330 - mse: 0.0174 - rmse: 0.1319 - val_loss: 0.0150 - val_mape: 27.2882 - val_mse: 0.0151 - val_rmse: 0.1226\n",
      "Epoch 2/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0146 - mape: 24.1038 - mse: 0.0146 - rmse: 0.1207 - val_loss: 0.0140 - val_mape: 26.0522 - val_mse: 0.0141 - val_rmse: 0.1184\n",
      "Epoch 3/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0142 - mape: 23.7619 - mse: 0.0142 - rmse: 0.1192 - val_loss: 0.0130 - val_mape: 24.4581 - val_mse: 0.0131 - val_rmse: 0.1139\n",
      "Epoch 4/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0133 - mape: 22.6362 - mse: 0.0133 - rmse: 0.1152 - val_loss: 0.0125 - val_mape: 23.7881 - val_mse: 0.0127 - val_rmse: 0.1119\n",
      "Epoch 5/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0131 - mape: 22.4257 - mse: 0.0131 - rmse: 0.1144 - val_loss: 0.0121 - val_mape: 23.3307 - val_mse: 0.0123 - val_rmse: 0.1102\n",
      "Epoch 6/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0128 - mape: 22.1024 - mse: 0.0128 - rmse: 0.1130 - val_loss: 0.0119 - val_mape: 23.5011 - val_mse: 0.0120 - val_rmse: 0.1092\n",
      "Epoch 7/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0126 - mape: 22.1169 - mse: 0.0126 - rmse: 0.1122 - val_loss: 0.0116 - val_mape: 23.0377 - val_mse: 0.0118 - val_rmse: 0.1079\n",
      "Epoch 8/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0122 - mape: 21.2133 - mse: 0.0122 - rmse: 0.1103 - val_loss: 0.0115 - val_mape: 23.2000 - val_mse: 0.0117 - val_rmse: 0.1074\n",
      "Epoch 9/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0123 - mape: 21.4597 - mse: 0.0123 - rmse: 0.1107 - val_loss: 0.0111 - val_mape: 22.4358 - val_mse: 0.0112 - val_rmse: 0.1053\n",
      "Epoch 10/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0116 - mape: 20.8470 - mse: 0.0116 - rmse: 0.1077 - val_loss: 0.0110 - val_mape: 22.5179 - val_mse: 0.0112 - val_rmse: 0.1051\n",
      "Epoch 11/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0113 - mape: 20.5379 - mse: 0.0113 - rmse: 0.1064 - val_loss: 0.0109 - val_mape: 22.3781 - val_mse: 0.0111 - val_rmse: 0.1045\n",
      "Epoch 12/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0111 - mape: 20.6038 - mse: 0.0111 - rmse: 0.1055 - val_loss: 0.0109 - val_mape: 22.1919 - val_mse: 0.0111 - val_rmse: 0.1045\n",
      "Epoch 13/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0111 - mape: 20.1859 - mse: 0.0111 - rmse: 0.1051 - val_loss: 0.0108 - val_mape: 22.2003 - val_mse: 0.0109 - val_rmse: 0.1037\n",
      "Epoch 14/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0109 - mape: 20.2703 - mse: 0.0109 - rmse: 0.1043 - val_loss: 0.0107 - val_mape: 22.0429 - val_mse: 0.0108 - val_rmse: 0.1033\n",
      "Epoch 15/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0109 - mape: 20.4040 - mse: 0.0109 - rmse: 0.1045 - val_loss: 0.0103 - val_mape: 21.1498 - val_mse: 0.0104 - val_rmse: 0.1013\n",
      "Epoch 16/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0107 - mape: 19.8787 - mse: 0.0107 - rmse: 0.1032 - val_loss: 0.0102 - val_mape: 20.9817 - val_mse: 0.0103 - val_rmse: 0.1009\n",
      "Epoch 17/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0106 - mape: 19.5247 - mse: 0.0106 - rmse: 0.1029 - val_loss: 0.0102 - val_mape: 21.3572 - val_mse: 0.0103 - val_rmse: 0.1011\n",
      "Epoch 18/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0105 - mape: 19.6662 - mse: 0.0105 - rmse: 0.1022 - val_loss: 0.0099 - val_mape: 20.4009 - val_mse: 0.0100 - val_rmse: 0.0993\n",
      "Epoch 19/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0100 - mape: 19.4268 - mse: 0.0100 - rmse: 0.1002 - val_loss: 0.0103 - val_mape: 21.6091 - val_mse: 0.0104 - val_rmse: 0.1016\n",
      "Epoch 20/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0100 - mape: 19.1083 - mse: 0.0100 - rmse: 0.0998 - val_loss: 0.0102 - val_mape: 21.4276 - val_mse: 0.0103 - val_rmse: 0.1010\n",
      "Epoch 21/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0097 - mape: 18.9641 - mse: 0.0097 - rmse: 0.0983 - val_loss: 0.0099 - val_mape: 20.9236 - val_mse: 0.0100 - val_rmse: 0.0997\n",
      "Epoch 22/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0100 - mape: 19.1777 - mse: 0.0100 - rmse: 0.1001 - val_loss: 0.0101 - val_mape: 21.3746 - val_mse: 0.0103 - val_rmse: 0.1007\n",
      "Epoch 23/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0098 - mape: 19.0942 - mse: 0.0098 - rmse: 0.0991 - val_loss: 0.0097 - val_mape: 20.4045 - val_mse: 0.0098 - val_rmse: 0.0984\n",
      "Epoch 24/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0099 - mape: 18.8492 - mse: 0.0099 - rmse: 0.0995 - val_loss: 0.0098 - val_mape: 20.7708 - val_mse: 0.0099 - val_rmse: 0.0990\n",
      "Epoch 25/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0093 - mape: 18.3725 - mse: 0.0093 - rmse: 0.0964 - val_loss: 0.0100 - val_mape: 21.0787 - val_mse: 0.0101 - val_rmse: 0.1000\n",
      "Epoch 26/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0097 - mape: 18.5928 - mse: 0.0097 - rmse: 0.0986 - val_loss: 0.0097 - val_mape: 20.5236 - val_mse: 0.0098 - val_rmse: 0.0985\n",
      "Epoch 27/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0096 - mape: 18.5528 - mse: 0.0096 - rmse: 0.0978 - val_loss: 0.0099 - val_mape: 20.8350 - val_mse: 0.0101 - val_rmse: 0.0995\n",
      "Epoch 28/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0092 - mape: 18.1534 - mse: 0.0092 - rmse: 0.0958 - val_loss: 0.0096 - val_mape: 20.4283 - val_mse: 0.0098 - val_rmse: 0.0982\n",
      "Epoch 29/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0091 - mape: 18.3352 - mse: 0.0091 - rmse: 0.0954 - val_loss: 0.0097 - val_mape: 20.4247 - val_mse: 0.0098 - val_rmse: 0.0985\n",
      "Epoch 30/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0089 - mape: 18.1139 - mse: 0.0089 - rmse: 0.0944 - val_loss: 0.0100 - val_mape: 20.8544 - val_mse: 0.0101 - val_rmse: 0.0998\n",
      "Epoch 31/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0093 - mape: 18.4381 - mse: 0.0093 - rmse: 0.0965 - val_loss: 0.0097 - val_mape: 20.4296 - val_mse: 0.0098 - val_rmse: 0.0984\n",
      "Epoch 32/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0091 - mape: 18.1924 - mse: 0.0091 - rmse: 0.0956 - val_loss: 0.0098 - val_mape: 20.4701 - val_mse: 0.0099 - val_rmse: 0.0989\n",
      "Epoch 33/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0088 - mape: 17.7783 - mse: 0.0088 - rmse: 0.0939 - val_loss: 0.0096 - val_mape: 20.1355 - val_mse: 0.0098 - val_rmse: 0.0982\n",
      "Epoch 34/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0089 - mape: 17.9057 - mse: 0.0089 - rmse: 0.0943 - val_loss: 0.0093 - val_mape: 19.6043 - val_mse: 0.0094 - val_rmse: 0.0964\n",
      "Epoch 35/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0086 - mape: 17.4372 - mse: 0.0086 - rmse: 0.0928 - val_loss: 0.0094 - val_mape: 19.6522 - val_mse: 0.0095 - val_rmse: 0.0969\n",
      "Epoch 36/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0087 - mape: 17.4623 - mse: 0.0087 - rmse: 0.0934 - val_loss: 0.0093 - val_mape: 19.8603 - val_mse: 0.0094 - val_rmse: 0.0965\n",
      "Epoch 37/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0086 - mape: 17.9036 - mse: 0.0086 - rmse: 0.0928 - val_loss: 0.0092 - val_mape: 19.6487 - val_mse: 0.0093 - val_rmse: 0.0958\n",
      "Epoch 38/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0088 - mape: 17.6210 - mse: 0.0088 - rmse: 0.0938 - val_loss: 0.0095 - val_mape: 20.0616 - val_mse: 0.0096 - val_rmse: 0.0974\n",
      "Epoch 39/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0083 - mape: 17.1759 - mse: 0.0083 - rmse: 0.0910 - val_loss: 0.0092 - val_mape: 19.6394 - val_mse: 0.0093 - val_rmse: 0.0958\n",
      "Epoch 40/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0088 - mape: 17.4329 - mse: 0.0088 - rmse: 0.0936 - val_loss: 0.0096 - val_mape: 20.3940 - val_mse: 0.0097 - val_rmse: 0.0979\n",
      "Epoch 41/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0085 - mape: 17.1938 - mse: 0.0085 - rmse: 0.0921 - val_loss: 0.0093 - val_mape: 19.5102 - val_mse: 0.0095 - val_rmse: 0.0964\n",
      "Epoch 42/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0082 - mape: 16.7421 - mse: 0.0082 - rmse: 0.0906 - val_loss: 0.0095 - val_mape: 20.0833 - val_mse: 0.0096 - val_rmse: 0.0973\n",
      "Epoch 43/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0083 - mape: 17.2840 - mse: 0.0083 - rmse: 0.0909 - val_loss: 0.0092 - val_mape: 19.6951 - val_mse: 0.0094 - val_rmse: 0.0960\n",
      "Epoch 44/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0080 - mape: 16.9985 - mse: 0.0080 - rmse: 0.0893 - val_loss: 0.0090 - val_mape: 19.2956 - val_mse: 0.0091 - val_rmse: 0.0948\n",
      "Epoch 45/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0083 - mape: 16.9187 - mse: 0.0083 - rmse: 0.0910 - val_loss: 0.0090 - val_mape: 19.4908 - val_mse: 0.0091 - val_rmse: 0.0949\n",
      "Epoch 46/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0081 - mape: 16.7795 - mse: 0.0081 - rmse: 0.0901 - val_loss: 0.0092 - val_mape: 19.5789 - val_mse: 0.0094 - val_rmse: 0.0959\n",
      "Epoch 47/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0081 - mape: 17.2400 - mse: 0.0081 - rmse: 0.0898 - val_loss: 0.0091 - val_mape: 19.5268 - val_mse: 0.0093 - val_rmse: 0.0954\n",
      "Epoch 48/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0083 - mape: 17.1872 - mse: 0.0083 - rmse: 0.0913 - val_loss: 0.0093 - val_mape: 20.0127 - val_mse: 0.0094 - val_rmse: 0.0963\n",
      "Epoch 49/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0082 - mape: 17.2386 - mse: 0.0082 - rmse: 0.0905 - val_loss: 0.0093 - val_mape: 19.7617 - val_mse: 0.0095 - val_rmse: 0.0966\n",
      "Epoch 50/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0079 - mape: 16.5815 - mse: 0.0079 - rmse: 0.0887 - val_loss: 0.0093 - val_mape: 19.8544 - val_mse: 0.0094 - val_rmse: 0.0963\n",
      "Epoch 51/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0080 - mape: 16.7743 - mse: 0.0080 - rmse: 0.0895 - val_loss: 0.0092 - val_mape: 19.4920 - val_mse: 0.0094 - val_rmse: 0.0960\n",
      "Epoch 52/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0081 - mape: 16.6560 - mse: 0.0081 - rmse: 0.0898 - val_loss: 0.0093 - val_mape: 19.8451 - val_mse: 0.0094 - val_rmse: 0.0964\n",
      "Epoch 53/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0078 - mape: 16.4007 - mse: 0.0078 - rmse: 0.0885 - val_loss: 0.0090 - val_mape: 19.1862 - val_mse: 0.0092 - val_rmse: 0.0951\n",
      "Epoch 54/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0080 - mape: 16.8183 - mse: 0.0080 - rmse: 0.0896 - val_loss: 0.0089 - val_mape: 19.3005 - val_mse: 0.0091 - val_rmse: 0.0946\n",
      "Epoch 55/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0077 - mape: 16.3918 - mse: 0.0077 - rmse: 0.0878 - val_loss: 0.0092 - val_mape: 19.5714 - val_mse: 0.0093 - val_rmse: 0.0958\n",
      "Epoch 56/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0081 - mape: 17.3678 - mse: 0.0081 - rmse: 0.0900 - val_loss: 0.0090 - val_mape: 19.4478 - val_mse: 0.0091 - val_rmse: 0.0948\n",
      "Epoch 57/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0077 - mape: 16.4408 - mse: 0.0077 - rmse: 0.0877 - val_loss: 0.0092 - val_mape: 19.6812 - val_mse: 0.0093 - val_rmse: 0.0959\n",
      "Epoch 58/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0075 - mape: 16.0323 - mse: 0.0075 - rmse: 0.0866 - val_loss: 0.0091 - val_mape: 19.5307 - val_mse: 0.0093 - val_rmse: 0.0956\n",
      "Epoch 59/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0078 - mape: 16.2092 - mse: 0.0078 - rmse: 0.0886 - val_loss: 0.0091 - val_mape: 19.4501 - val_mse: 0.0092 - val_rmse: 0.0952\n",
      "Epoch 60/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0076 - mape: 16.3262 - mse: 0.0076 - rmse: 0.0874 - val_loss: 0.0090 - val_mape: 19.1730 - val_mse: 0.0092 - val_rmse: 0.0951\n",
      "Epoch 61/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0074 - mape: 15.7819 - mse: 0.0074 - rmse: 0.0861 - val_loss: 0.0091 - val_mape: 19.0682 - val_mse: 0.0092 - val_rmse: 0.0954\n",
      "Epoch 62/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0076 - mape: 15.9785 - mse: 0.0076 - rmse: 0.0872 - val_loss: 0.0091 - val_mape: 19.3384 - val_mse: 0.0093 - val_rmse: 0.0956\n",
      "Epoch 63/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0076 - mape: 16.4838 - mse: 0.0076 - rmse: 0.0873 - val_loss: 0.0090 - val_mape: 19.2958 - val_mse: 0.0091 - val_rmse: 0.0949\n",
      "Epoch 64/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0074 - mape: 16.2862 - mse: 0.0074 - rmse: 0.0862 - val_loss: 0.0089 - val_mape: 18.9313 - val_mse: 0.0090 - val_rmse: 0.0943\n",
      "Epoch 65/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0076 - mape: 16.4536 - mse: 0.0076 - rmse: 0.0871 - val_loss: 0.0090 - val_mape: 19.0060 - val_mse: 0.0092 - val_rmse: 0.0949\n",
      "Epoch 66/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0076 - mape: 16.4629 - mse: 0.0076 - rmse: 0.0870 - val_loss: 0.0089 - val_mape: 19.3235 - val_mse: 0.0091 - val_rmse: 0.0945\n",
      "Epoch 67/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0076 - mape: 16.2975 - mse: 0.0076 - rmse: 0.0873 - val_loss: 0.0092 - val_mape: 19.5525 - val_mse: 0.0093 - val_rmse: 0.0957\n",
      "Epoch 68/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0075 - mape: 15.9036 - mse: 0.0075 - rmse: 0.0863 - val_loss: 0.0095 - val_mape: 19.9501 - val_mse: 0.0097 - val_rmse: 0.0976\n",
      "Epoch 69/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0072 - mape: 15.5185 - mse: 0.0072 - rmse: 0.0847 - val_loss: 0.0089 - val_mape: 18.7915 - val_mse: 0.0091 - val_rmse: 0.0945\n",
      "Epoch 70/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0074 - mape: 15.8674 - mse: 0.0074 - rmse: 0.0861 - val_loss: 0.0090 - val_mape: 18.6251 - val_mse: 0.0091 - val_rmse: 0.0947\n",
      "Epoch 71/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - mape: 15.5795 - mse: 0.0073 - rmse: 0.0853 - val_loss: 0.0089 - val_mape: 18.7693 - val_mse: 0.0091 - val_rmse: 0.0944\n",
      "Epoch 72/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0072 - mape: 15.5893 - mse: 0.0072 - rmse: 0.0846 - val_loss: 0.0087 - val_mape: 18.5788 - val_mse: 0.0088 - val_rmse: 0.0933\n",
      "Epoch 73/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0072 - mape: 15.7790 - mse: 0.0072 - rmse: 0.0847 - val_loss: 0.0090 - val_mape: 19.2418 - val_mse: 0.0092 - val_rmse: 0.0951\n",
      "Epoch 74/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0072 - mape: 15.6973 - mse: 0.0072 - rmse: 0.0848 - val_loss: 0.0091 - val_mape: 19.1538 - val_mse: 0.0092 - val_rmse: 0.0953\n",
      "Epoch 75/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0074 - mape: 16.0609 - mse: 0.0074 - rmse: 0.0858 - val_loss: 0.0090 - val_mape: 19.0758 - val_mse: 0.0091 - val_rmse: 0.0947\n",
      "Epoch 76/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0069 - mape: 15.2832 - mse: 0.0069 - rmse: 0.0833 - val_loss: 0.0094 - val_mape: 19.5740 - val_mse: 0.0095 - val_rmse: 0.0967\n",
      "Epoch 77/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0071 - mape: 15.8715 - mse: 0.0071 - rmse: 0.0844 - val_loss: 0.0092 - val_mape: 19.6626 - val_mse: 0.0093 - val_rmse: 0.0958\n",
      "Epoch 78/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0070 - mape: 15.3952 - mse: 0.0070 - rmse: 0.0838 - val_loss: 0.0090 - val_mape: 18.9064 - val_mse: 0.0092 - val_rmse: 0.0949\n",
      "Epoch 79/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0070 - mape: 15.4719 - mse: 0.0070 - rmse: 0.0839 - val_loss: 0.0095 - val_mape: 20.0242 - val_mse: 0.0096 - val_rmse: 0.0973\n",
      "Epoch 80/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0067 - mape: 15.2751 - mse: 0.0067 - rmse: 0.0818 - val_loss: 0.0088 - val_mape: 18.7228 - val_mse: 0.0090 - val_rmse: 0.0938\n",
      "Epoch 81/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0073 - mape: 16.0096 - mse: 0.0073 - rmse: 0.0852 - val_loss: 0.0092 - val_mape: 19.5200 - val_mse: 0.0093 - val_rmse: 0.0958\n",
      "Epoch 82/200\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0074 - mape: 15.7049 - mse: 0.0074 - rmse: 0.0861 - val_loss: 0.0091 - val_mape: 19.2618 - val_mse: 0.0093 - val_rmse: 0.0955\n",
      "[MODELO CONGELADO]: 46.483277559280396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 4.986299989099817, 2.233002460612128, 0.9260322189000415, 13.043883349882435\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 5.246880380590144, 2.290606989553237, 0.9122350835922024, 15.464001528150199\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 7.116136298363847, 2.6676087228759484, 0.831225766523655, 18.089727467716436\n",
      "PROCESANDO ARCHIVO: Pinus gerardiana\n",
      "(10101, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Pinus gerardiana_best_models.json\n",
      "(5767, 4, 43) (2181, 4, 43) (1745, 4, 43)\n",
      "<Conv1D name=conv1d, built=True> False\n",
      "<BatchNormalization name=batch_normalization, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d, built=True> False\n",
      "<Dropout name=dropout, built=True> True\n",
      "<Conv1D name=conv1d_1, built=True> False\n",
      "<BatchNormalization name=batch_normalization_1, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_1, built=True> False\n",
      "<Conv1D name=conv1d_2, built=True> True\n",
      "<BatchNormalization name=batch_normalization_2, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d, built=True> True\n",
      "<Dense name=dense, built=True> True\n",
      "<Dense name=dense_1, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0221 - mape: 36.4152 - mse: 0.0221 - rmse: 0.1486 - val_loss: 0.0227 - val_mape: 44.9807 - val_mse: 0.0225 - val_rmse: 0.1507\n",
      "Epoch 2/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0198 - mape: 35.9729 - mse: 0.0198 - rmse: 0.1406 - val_loss: 0.0227 - val_mape: 41.6293 - val_mse: 0.0225 - val_rmse: 0.1507\n",
      "Epoch 3/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0190 - mape: 33.9793 - mse: 0.0190 - rmse: 0.1378 - val_loss: 0.0199 - val_mape: 38.3412 - val_mse: 0.0197 - val_rmse: 0.1410\n",
      "Epoch 4/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0189 - mape: 34.1413 - mse: 0.0189 - rmse: 0.1373 - val_loss: 0.0222 - val_mape: 38.5792 - val_mse: 0.0220 - val_rmse: 0.1489\n",
      "Epoch 5/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0182 - mape: 32.7026 - mse: 0.0182 - rmse: 0.1347 - val_loss: 0.0204 - val_mape: 37.4596 - val_mse: 0.0202 - val_rmse: 0.1428\n",
      "Epoch 6/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0177 - mape: 32.5981 - mse: 0.0177 - rmse: 0.1328 - val_loss: 0.0183 - val_mape: 36.0904 - val_mse: 0.0182 - val_rmse: 0.1353\n",
      "Epoch 7/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0179 - mape: 32.2169 - mse: 0.0179 - rmse: 0.1336 - val_loss: 0.0186 - val_mape: 35.6232 - val_mse: 0.0185 - val_rmse: 0.1364\n",
      "Epoch 8/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0174 - mape: 31.9035 - mse: 0.0174 - rmse: 0.1318 - val_loss: 0.0184 - val_mape: 34.6633 - val_mse: 0.0183 - val_rmse: 0.1356\n",
      "Epoch 9/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0171 - mape: 31.1699 - mse: 0.0171 - rmse: 0.1307 - val_loss: 0.0188 - val_mape: 34.4064 - val_mse: 0.0187 - val_rmse: 0.1373\n",
      "Epoch 10/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0173 - mape: 31.7104 - mse: 0.0173 - rmse: 0.1316 - val_loss: 0.0185 - val_mape: 34.3557 - val_mse: 0.0184 - val_rmse: 0.1361\n",
      "Epoch 11/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0163 - mape: 30.7446 - mse: 0.0163 - rmse: 0.1276 - val_loss: 0.0172 - val_mape: 33.2589 - val_mse: 0.0171 - val_rmse: 0.1313\n",
      "Epoch 12/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0166 - mape: 31.0486 - mse: 0.0166 - rmse: 0.1288 - val_loss: 0.0198 - val_mape: 33.7681 - val_mse: 0.0196 - val_rmse: 0.1407\n",
      "Epoch 13/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0169 - mape: 30.9380 - mse: 0.0169 - rmse: 0.1297 - val_loss: 0.0190 - val_mape: 33.4244 - val_mse: 0.0188 - val_rmse: 0.1377\n",
      "Epoch 14/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0164 - mape: 30.0542 - mse: 0.0164 - rmse: 0.1280 - val_loss: 0.0169 - val_mape: 32.7308 - val_mse: 0.0168 - val_rmse: 0.1299\n",
      "Epoch 15/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0161 - mape: 29.7576 - mse: 0.0161 - rmse: 0.1268 - val_loss: 0.0174 - val_mape: 33.4029 - val_mse: 0.0172 - val_rmse: 0.1318\n",
      "Epoch 16/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0160 - mape: 29.7143 - mse: 0.0160 - rmse: 0.1265 - val_loss: 0.0201 - val_mape: 35.4017 - val_mse: 0.0199 - val_rmse: 0.1417\n",
      "Epoch 17/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0160 - mape: 29.7953 - mse: 0.0160 - rmse: 0.1265 - val_loss: 0.0170 - val_mape: 32.1354 - val_mse: 0.0169 - val_rmse: 0.1305\n",
      "Epoch 18/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0157 - mape: 29.3778 - mse: 0.0157 - rmse: 0.1252 - val_loss: 0.0172 - val_mape: 31.7534 - val_mse: 0.0170 - val_rmse: 0.1311\n",
      "Epoch 19/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0157 - mape: 29.6725 - mse: 0.0157 - rmse: 0.1252 - val_loss: 0.0170 - val_mape: 33.3045 - val_mse: 0.0168 - val_rmse: 0.1303\n",
      "Epoch 20/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0150 - mape: 29.3376 - mse: 0.0150 - rmse: 0.1224 - val_loss: 0.0175 - val_mape: 32.7187 - val_mse: 0.0174 - val_rmse: 0.1323\n",
      "Epoch 21/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0152 - mape: 28.6691 - mse: 0.0152 - rmse: 0.1232 - val_loss: 0.0169 - val_mape: 33.6344 - val_mse: 0.0168 - val_rmse: 0.1302\n",
      "Epoch 22/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0154 - mape: 29.1592 - mse: 0.0154 - rmse: 0.1242 - val_loss: 0.0164 - val_mape: 32.5640 - val_mse: 0.0162 - val_rmse: 0.1279\n",
      "Epoch 23/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0157 - mape: 29.3163 - mse: 0.0157 - rmse: 0.1252 - val_loss: 0.0169 - val_mape: 32.3283 - val_mse: 0.0167 - val_rmse: 0.1298\n",
      "Epoch 24/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0153 - mape: 29.1929 - mse: 0.0153 - rmse: 0.1236 - val_loss: 0.0175 - val_mape: 32.2407 - val_mse: 0.0173 - val_rmse: 0.1322\n",
      "Epoch 25/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0154 - mape: 28.9868 - mse: 0.0154 - rmse: 0.1239 - val_loss: 0.0164 - val_mape: 31.1411 - val_mse: 0.0163 - val_rmse: 0.1281\n",
      "Epoch 26/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0152 - mape: 28.7578 - mse: 0.0152 - rmse: 0.1231 - val_loss: 0.0169 - val_mape: 31.7102 - val_mse: 0.0167 - val_rmse: 0.1299\n",
      "Epoch 27/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0148 - mape: 28.5947 - mse: 0.0148 - rmse: 0.1217 - val_loss: 0.0165 - val_mape: 30.9174 - val_mse: 0.0164 - val_rmse: 0.1286\n",
      "Epoch 28/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0150 - mape: 27.8773 - mse: 0.0150 - rmse: 0.1225 - val_loss: 0.0170 - val_mape: 31.2761 - val_mse: 0.0169 - val_rmse: 0.1306\n",
      "Epoch 29/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0149 - mape: 28.5503 - mse: 0.0149 - rmse: 0.1221 - val_loss: 0.0174 - val_mape: 31.7740 - val_mse: 0.0173 - val_rmse: 0.1319\n",
      "Epoch 30/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0148 - mape: 28.2645 - mse: 0.0148 - rmse: 0.1215 - val_loss: 0.0161 - val_mape: 29.9317 - val_mse: 0.0160 - val_rmse: 0.1269\n",
      "Epoch 31/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0146 - mape: 27.5599 - mse: 0.0146 - rmse: 0.1206 - val_loss: 0.0161 - val_mape: 30.4213 - val_mse: 0.0159 - val_rmse: 0.1268\n",
      "Epoch 32/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0143 - mape: 27.4445 - mse: 0.0143 - rmse: 0.1194 - val_loss: 0.0172 - val_mape: 31.6288 - val_mse: 0.0171 - val_rmse: 0.1312\n",
      "Epoch 33/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0146 - mape: 28.1110 - mse: 0.0146 - rmse: 0.1209 - val_loss: 0.0171 - val_mape: 31.2963 - val_mse: 0.0170 - val_rmse: 0.1308\n",
      "Epoch 34/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0145 - mape: 27.9891 - mse: 0.0145 - rmse: 0.1201 - val_loss: 0.0166 - val_mape: 30.6355 - val_mse: 0.0164 - val_rmse: 0.1286\n",
      "Epoch 35/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0150 - mape: 28.9214 - mse: 0.0150 - rmse: 0.1223 - val_loss: 0.0164 - val_mape: 31.0712 - val_mse: 0.0162 - val_rmse: 0.1279\n",
      "Epoch 36/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0148 - mape: 28.1244 - mse: 0.0148 - rmse: 0.1216 - val_loss: 0.0163 - val_mape: 31.6839 - val_mse: 0.0162 - val_rmse: 0.1278\n",
      "Epoch 37/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0142 - mape: 27.6495 - mse: 0.0142 - rmse: 0.1193 - val_loss: 0.0162 - val_mape: 31.1284 - val_mse: 0.0161 - val_rmse: 0.1275\n",
      "Epoch 38/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0142 - mape: 27.9857 - mse: 0.0142 - rmse: 0.1192 - val_loss: 0.0168 - val_mape: 31.0853 - val_mse: 0.0166 - val_rmse: 0.1295\n",
      "Epoch 39/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0143 - mape: 27.3388 - mse: 0.0143 - rmse: 0.1195 - val_loss: 0.0177 - val_mape: 31.6387 - val_mse: 0.0175 - val_rmse: 0.1330\n",
      "Epoch 40/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0147 - mape: 27.9925 - mse: 0.0147 - rmse: 0.1213 - val_loss: 0.0170 - val_mape: 31.3942 - val_mse: 0.0168 - val_rmse: 0.1303\n",
      "Epoch 41/200\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0141 - mape: 26.7589 - mse: 0.0141 - rmse: 0.1185 - val_loss: 0.0162 - val_mape: 30.5440 - val_mse: 0.0161 - val_rmse: 0.1274\n",
      "[MODELO CONGELADO]: 33.42450761795044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 6.414387166299935, 2.532664045289058, 0.8347536811105876, 19.202947935521088\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 9.325058635649649, 3.0536958977032485, 0.7643400760641934, 21.589876042375174\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 7.276594445101172, 2.69751634751324, 0.81887956629712, 19.93298364125001\n",
      "PROCESANDO ARCHIVO: Betula utilis\n",
      "(3473, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Betula utilis_best_models.json\n",
      "(2059, 4, 43) (665, 4, 43) (593, 4, 43)\n",
      "<Conv1D name=conv1d_3, built=True> False\n",
      "<BatchNormalization name=batch_normalization_3, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_2, built=True> False\n",
      "<Conv1D name=conv1d_4, built=True> False\n",
      "<BatchNormalization name=batch_normalization_4, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_3, built=True> False\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_6, built=True> False\n",
      "<BatchNormalization name=batch_normalization_6, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Conv1D name=conv1d_7, built=True> False\n",
      "<BatchNormalization name=batch_normalization_7, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_6, built=True> False\n",
      "<Conv1D name=conv1d_8, built=True> False\n",
      "<BatchNormalization name=batch_normalization_8, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_7, built=True> False\n",
      "<Conv1D name=conv1d_9, built=True> True\n",
      "<BatchNormalization name=batch_normalization_9, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0401 - mape: 136.0024 - mse: 0.0401 - rmse: 0.1995 - val_loss: 0.0211 - val_mape: 45.3074 - val_mse: 0.0213 - val_rmse: 0.1454\n",
      "Epoch 2/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0242 - mape: 87.8529 - mse: 0.0242 - rmse: 0.1555 - val_loss: 0.0178 - val_mape: 46.2241 - val_mse: 0.0179 - val_rmse: 0.1334\n",
      "Epoch 3/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0205 - mape: 75.7859 - mse: 0.0205 - rmse: 0.1430 - val_loss: 0.0172 - val_mape: 41.2241 - val_mse: 0.0173 - val_rmse: 0.1311\n",
      "Epoch 4/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0195 - mape: 72.7632 - mse: 0.0195 - rmse: 0.1396 - val_loss: 0.0166 - val_mape: 37.1227 - val_mse: 0.0167 - val_rmse: 0.1290\n",
      "Epoch 5/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0193 - mape: 69.3339 - mse: 0.0193 - rmse: 0.1390 - val_loss: 0.0168 - val_mape: 36.8960 - val_mse: 0.0169 - val_rmse: 0.1296\n",
      "Epoch 6/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0191 - mape: 72.3138 - mse: 0.0190 - rmse: 0.1379 - val_loss: 0.0168 - val_mape: 36.5984 - val_mse: 0.0169 - val_rmse: 0.1298\n",
      "Epoch 7/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0180 - mape: 72.3753 - mse: 0.0180 - rmse: 0.1341 - val_loss: 0.0170 - val_mape: 35.9119 - val_mse: 0.0171 - val_rmse: 0.1305\n",
      "Epoch 8/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0186 - mape: 68.8521 - mse: 0.0186 - rmse: 0.1363 - val_loss: 0.0164 - val_mape: 36.5332 - val_mse: 0.0165 - val_rmse: 0.1282\n",
      "Epoch 9/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0190 - mape: 70.7278 - mse: 0.0189 - rmse: 0.1376 - val_loss: 0.0166 - val_mape: 36.2023 - val_mse: 0.0166 - val_rmse: 0.1288\n",
      "Epoch 10/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0176 - mape: 64.5895 - mse: 0.0176 - rmse: 0.1324 - val_loss: 0.0163 - val_mape: 35.7082 - val_mse: 0.0163 - val_rmse: 0.1276\n",
      "Epoch 11/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0174 - mape: 69.3973 - mse: 0.0174 - rmse: 0.1318 - val_loss: 0.0160 - val_mape: 35.0908 - val_mse: 0.0160 - val_rmse: 0.1263\n",
      "Epoch 12/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0166 - mape: 61.7998 - mse: 0.0166 - rmse: 0.1287 - val_loss: 0.0161 - val_mape: 34.5185 - val_mse: 0.0161 - val_rmse: 0.1267\n",
      "Epoch 13/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0161 - mape: 67.8055 - mse: 0.0161 - rmse: 0.1268 - val_loss: 0.0159 - val_mape: 33.1539 - val_mse: 0.0159 - val_rmse: 0.1262\n",
      "Epoch 14/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0164 - mape: 61.3866 - mse: 0.0164 - rmse: 0.1280 - val_loss: 0.0158 - val_mape: 33.9013 - val_mse: 0.0158 - val_rmse: 0.1257\n",
      "Epoch 15/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0173 - mape: 69.6339 - mse: 0.0173 - rmse: 0.1316 - val_loss: 0.0157 - val_mape: 34.3550 - val_mse: 0.0157 - val_rmse: 0.1252\n",
      "Epoch 16/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0175 - mape: 63.1725 - mse: 0.0175 - rmse: 0.1322 - val_loss: 0.0157 - val_mape: 33.5623 - val_mse: 0.0157 - val_rmse: 0.1254\n",
      "Epoch 17/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0165 - mape: 57.9442 - mse: 0.0165 - rmse: 0.1284 - val_loss: 0.0153 - val_mape: 34.0263 - val_mse: 0.0153 - val_rmse: 0.1238\n",
      "Epoch 18/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0169 - mape: 58.9274 - mse: 0.0169 - rmse: 0.1300 - val_loss: 0.0150 - val_mape: 33.9555 - val_mse: 0.0150 - val_rmse: 0.1226\n",
      "Epoch 19/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0162 - mape: 58.5773 - mse: 0.0162 - rmse: 0.1273 - val_loss: 0.0155 - val_mape: 35.2668 - val_mse: 0.0155 - val_rmse: 0.1243\n",
      "Epoch 20/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0166 - mape: 61.2873 - mse: 0.0166 - rmse: 0.1287 - val_loss: 0.0149 - val_mape: 33.0512 - val_mse: 0.0149 - val_rmse: 0.1219\n",
      "Epoch 21/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0163 - mape: 63.2357 - mse: 0.0163 - rmse: 0.1277 - val_loss: 0.0155 - val_mape: 34.1887 - val_mse: 0.0155 - val_rmse: 0.1245\n",
      "Epoch 22/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0157 - mape: 56.2367 - mse: 0.0157 - rmse: 0.1253 - val_loss: 0.0151 - val_mape: 30.8243 - val_mse: 0.0151 - val_rmse: 0.1229\n",
      "Epoch 23/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0164 - mape: 62.6244 - mse: 0.0164 - rmse: 0.1278 - val_loss: 0.0151 - val_mape: 33.0806 - val_mse: 0.0151 - val_rmse: 0.1230\n",
      "Epoch 24/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0159 - mape: 57.2038 - mse: 0.0159 - rmse: 0.1259 - val_loss: 0.0151 - val_mape: 32.9855 - val_mse: 0.0151 - val_rmse: 0.1231\n",
      "Epoch 25/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0159 - mape: 55.8237 - mse: 0.0159 - rmse: 0.1259 - val_loss: 0.0152 - val_mape: 34.5684 - val_mse: 0.0152 - val_rmse: 0.1234\n",
      "Epoch 26/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0157 - mape: 53.0981 - mse: 0.0157 - rmse: 0.1253 - val_loss: 0.0150 - val_mape: 32.7973 - val_mse: 0.0151 - val_rmse: 0.1227\n",
      "Epoch 27/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0150 - mape: 60.6108 - mse: 0.0150 - rmse: 0.1222 - val_loss: 0.0148 - val_mape: 31.9913 - val_mse: 0.0148 - val_rmse: 0.1217\n",
      "Epoch 28/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0158 - mape: 60.1332 - mse: 0.0158 - rmse: 0.1255 - val_loss: 0.0148 - val_mape: 32.0962 - val_mse: 0.0148 - val_rmse: 0.1218\n",
      "Epoch 29/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0154 - mape: 59.6395 - mse: 0.0154 - rmse: 0.1242 - val_loss: 0.0145 - val_mape: 32.2326 - val_mse: 0.0145 - val_rmse: 0.1206\n",
      "Epoch 30/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0154 - mape: 54.0066 - mse: 0.0154 - rmse: 0.1242 - val_loss: 0.0156 - val_mape: 33.6264 - val_mse: 0.0156 - val_rmse: 0.1248\n",
      "Epoch 31/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0155 - mape: 55.7224 - mse: 0.0155 - rmse: 0.1243 - val_loss: 0.0148 - val_mape: 31.1069 - val_mse: 0.0148 - val_rmse: 0.1215\n",
      "Epoch 32/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0153 - mape: 55.5846 - mse: 0.0153 - rmse: 0.1236 - val_loss: 0.0150 - val_mape: 31.7247 - val_mse: 0.0150 - val_rmse: 0.1224\n",
      "Epoch 33/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0153 - mape: 56.5749 - mse: 0.0153 - rmse: 0.1236 - val_loss: 0.0148 - val_mape: 32.5060 - val_mse: 0.0148 - val_rmse: 0.1217\n",
      "Epoch 34/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0151 - mape: 57.3961 - mse: 0.0151 - rmse: 0.1229 - val_loss: 0.0146 - val_mape: 32.4990 - val_mse: 0.0147 - val_rmse: 0.1210\n",
      "Epoch 35/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0148 - mape: 52.6017 - mse: 0.0148 - rmse: 0.1218 - val_loss: 0.0146 - val_mape: 30.7681 - val_mse: 0.0146 - val_rmse: 0.1209\n",
      "Epoch 36/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0150 - mape: 46.4172 - mse: 0.0150 - rmse: 0.1224 - val_loss: 0.0146 - val_mape: 32.3332 - val_mse: 0.0146 - val_rmse: 0.1210\n",
      "Epoch 37/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0158 - mape: 55.8405 - mse: 0.0157 - rmse: 0.1254 - val_loss: 0.0142 - val_mape: 31.0387 - val_mse: 0.0142 - val_rmse: 0.1193\n",
      "Epoch 38/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0154 - mape: 57.8436 - mse: 0.0154 - rmse: 0.1241 - val_loss: 0.0141 - val_mape: 32.4271 - val_mse: 0.0141 - val_rmse: 0.1188\n",
      "Epoch 39/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0157 - mape: 55.2198 - mse: 0.0157 - rmse: 0.1253 - val_loss: 0.0144 - val_mape: 31.3328 - val_mse: 0.0144 - val_rmse: 0.1200\n",
      "Epoch 40/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0149 - mape: 59.1939 - mse: 0.0149 - rmse: 0.1219 - val_loss: 0.0144 - val_mape: 32.4971 - val_mse: 0.0144 - val_rmse: 0.1201\n",
      "Epoch 41/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0147 - mape: 55.0116 - mse: 0.0147 - rmse: 0.1211 - val_loss: 0.0145 - val_mape: 31.0656 - val_mse: 0.0145 - val_rmse: 0.1203\n",
      "Epoch 42/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0149 - mape: 54.2627 - mse: 0.0148 - rmse: 0.1218 - val_loss: 0.0142 - val_mape: 31.3255 - val_mse: 0.0142 - val_rmse: 0.1193\n",
      "Epoch 43/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0155 - mape: 53.4847 - mse: 0.0155 - rmse: 0.1245 - val_loss: 0.0145 - val_mape: 31.7944 - val_mse: 0.0145 - val_rmse: 0.1204\n",
      "Epoch 44/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0149 - mape: 52.4059 - mse: 0.0149 - rmse: 0.1219 - val_loss: 0.0142 - val_mape: 30.9362 - val_mse: 0.0142 - val_rmse: 0.1191\n",
      "Epoch 45/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0146 - mape: 52.1230 - mse: 0.0146 - rmse: 0.1206 - val_loss: 0.0143 - val_mape: 30.7636 - val_mse: 0.0143 - val_rmse: 0.1195\n",
      "Epoch 46/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0144 - mape: 53.8565 - mse: 0.0144 - rmse: 0.1198 - val_loss: 0.0141 - val_mape: 31.8686 - val_mse: 0.0141 - val_rmse: 0.1186\n",
      "Epoch 47/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0144 - mape: 53.9357 - mse: 0.0144 - rmse: 0.1197 - val_loss: 0.0141 - val_mape: 31.0464 - val_mse: 0.0141 - val_rmse: 0.1189\n",
      "Epoch 48/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0144 - mape: 53.5786 - mse: 0.0144 - rmse: 0.1200 - val_loss: 0.0148 - val_mape: 32.4300 - val_mse: 0.0148 - val_rmse: 0.1215\n",
      "Epoch 49/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0142 - mape: 51.4197 - mse: 0.0142 - rmse: 0.1190 - val_loss: 0.0145 - val_mape: 31.4003 - val_mse: 0.0145 - val_rmse: 0.1203\n",
      "Epoch 50/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0149 - mape: 53.5127 - mse: 0.0149 - rmse: 0.1219 - val_loss: 0.0143 - val_mape: 31.4982 - val_mse: 0.0143 - val_rmse: 0.1195\n",
      "Epoch 51/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0142 - mape: 52.8001 - mse: 0.0142 - rmse: 0.1192 - val_loss: 0.0141 - val_mape: 30.0503 - val_mse: 0.0142 - val_rmse: 0.1188\n",
      "Epoch 52/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0141 - mape: 58.0743 - mse: 0.0141 - rmse: 0.1188 - val_loss: 0.0139 - val_mape: 29.9924 - val_mse: 0.0139 - val_rmse: 0.1180\n",
      "Epoch 53/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0141 - mape: 47.7526 - mse: 0.0141 - rmse: 0.1184 - val_loss: 0.0139 - val_mape: 31.3181 - val_mse: 0.0139 - val_rmse: 0.1180\n",
      "Epoch 54/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0134 - mape: 50.8056 - mse: 0.0134 - rmse: 0.1157 - val_loss: 0.0139 - val_mape: 29.6916 - val_mse: 0.0140 - val_rmse: 0.1181\n",
      "Epoch 55/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0145 - mape: 52.3421 - mse: 0.0145 - rmse: 0.1206 - val_loss: 0.0138 - val_mape: 30.5675 - val_mse: 0.0138 - val_rmse: 0.1176\n",
      "Epoch 56/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0141 - mape: 52.2312 - mse: 0.0141 - rmse: 0.1187 - val_loss: 0.0140 - val_mape: 30.8986 - val_mse: 0.0140 - val_rmse: 0.1184\n",
      "Epoch 57/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0148 - mape: 52.1969 - mse: 0.0148 - rmse: 0.1217 - val_loss: 0.0141 - val_mape: 30.7881 - val_mse: 0.0141 - val_rmse: 0.1187\n",
      "Epoch 58/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0150 - mape: 51.5048 - mse: 0.0150 - rmse: 0.1223 - val_loss: 0.0141 - val_mape: 30.3687 - val_mse: 0.0142 - val_rmse: 0.1189\n",
      "Epoch 59/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0150 - mape: 54.4593 - mse: 0.0150 - rmse: 0.1225 - val_loss: 0.0137 - val_mape: 31.2716 - val_mse: 0.0137 - val_rmse: 0.1171\n",
      "Epoch 60/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0146 - mape: 54.3951 - mse: 0.0146 - rmse: 0.1205 - val_loss: 0.0138 - val_mape: 30.4644 - val_mse: 0.0138 - val_rmse: 0.1174\n",
      "Epoch 61/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0144 - mape: 49.1511 - mse: 0.0144 - rmse: 0.1201 - val_loss: 0.0137 - val_mape: 30.3807 - val_mse: 0.0137 - val_rmse: 0.1171\n",
      "Epoch 62/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0144 - mape: 53.3436 - mse: 0.0144 - rmse: 0.1200 - val_loss: 0.0138 - val_mape: 30.5982 - val_mse: 0.0138 - val_rmse: 0.1175\n",
      "Epoch 63/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0140 - mape: 51.5394 - mse: 0.0140 - rmse: 0.1184 - val_loss: 0.0140 - val_mape: 29.9463 - val_mse: 0.0140 - val_rmse: 0.1181\n",
      "Epoch 64/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0146 - mape: 48.0214 - mse: 0.0146 - rmse: 0.1207 - val_loss: 0.0137 - val_mape: 30.5156 - val_mse: 0.0137 - val_rmse: 0.1172\n",
      "Epoch 65/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0143 - mape: 53.2544 - mse: 0.0143 - rmse: 0.1193 - val_loss: 0.0144 - val_mape: 32.6212 - val_mse: 0.0144 - val_rmse: 0.1198\n",
      "Epoch 66/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0139 - mape: 52.2001 - mse: 0.0139 - rmse: 0.1179 - val_loss: 0.0141 - val_mape: 31.6158 - val_mse: 0.0141 - val_rmse: 0.1186\n",
      "Epoch 67/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0139 - mape: 50.0957 - mse: 0.0139 - rmse: 0.1179 - val_loss: 0.0139 - val_mape: 29.3732 - val_mse: 0.0139 - val_rmse: 0.1180\n",
      "Epoch 68/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0134 - mape: 50.0425 - mse: 0.0134 - rmse: 0.1156 - val_loss: 0.0140 - val_mape: 30.8041 - val_mse: 0.0140 - val_rmse: 0.1183\n",
      "Epoch 69/200\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0135 - mape: 56.3577 - mse: 0.0135 - rmse: 0.1161 - val_loss: 0.0139 - val_mape: 31.1686 - val_mse: 0.0139 - val_rmse: 0.1180\n",
      "[MODELO CONGELADO]: 16.017966270446777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 1.535685086442931, 1.2392276168819556, 0.7732940454828706, 27.676516559182563\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 0.35273553102370264, 0.5939154241335232, 0.8163072464585408, 21.35277120132964\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 0.4047541667948636, 0.6362029289423804, 0.8453233195308876, 23.031007644609147\n",
      "PROCESANDO ARCHIVO: Picea smithiana\n",
      "(34476, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Picea smithiana_best_models.json\n",
      "(19933, 4, 43) (7272, 4, 43) (5863, 4, 43)\n",
      "<Conv1D name=conv1d, built=True> False\n",
      "<BatchNormalization name=batch_normalization, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d, built=True> False\n",
      "<Dropout name=dropout, built=True> True\n",
      "<Conv1D name=conv1d_1, built=True> False\n",
      "<BatchNormalization name=batch_normalization_1, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_1, built=True> False\n",
      "<Conv1D name=conv1d_2, built=True> True\n",
      "<BatchNormalization name=batch_normalization_2, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d, built=True> True\n",
      "<Dense name=dense, built=True> True\n",
      "<Dense name=dense_1, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0200 - mape: 32.6841 - mse: 0.0200 - rmse: 0.1414 - val_loss: 0.0155 - val_mape: 33.2324 - val_mse: 0.0156 - val_rmse: 0.1246\n",
      "Epoch 2/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0170 - mape: 29.6215 - mse: 0.0170 - rmse: 0.1303 - val_loss: 0.0146 - val_mape: 30.9621 - val_mse: 0.0147 - val_rmse: 0.1209\n",
      "Epoch 3/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0162 - mape: 28.5621 - mse: 0.0162 - rmse: 0.1273 - val_loss: 0.0142 - val_mape: 30.4744 - val_mse: 0.0142 - val_rmse: 0.1192\n",
      "Epoch 4/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0157 - mape: 28.4518 - mse: 0.0157 - rmse: 0.1254 - val_loss: 0.0138 - val_mape: 29.1782 - val_mse: 0.0138 - val_rmse: 0.1176\n",
      "Epoch 5/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0154 - mape: 27.8100 - mse: 0.0154 - rmse: 0.1242 - val_loss: 0.0137 - val_mape: 30.1224 - val_mse: 0.0137 - val_rmse: 0.1170\n",
      "Epoch 6/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0151 - mape: 27.4282 - mse: 0.0151 - rmse: 0.1230 - val_loss: 0.0135 - val_mape: 29.5347 - val_mse: 0.0135 - val_rmse: 0.1161\n",
      "Epoch 7/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0149 - mape: 27.0614 - mse: 0.0149 - rmse: 0.1221 - val_loss: 0.0135 - val_mape: 30.7375 - val_mse: 0.0135 - val_rmse: 0.1160\n",
      "Epoch 8/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0146 - mape: 27.3572 - mse: 0.0146 - rmse: 0.1206 - val_loss: 0.0132 - val_mape: 29.6912 - val_mse: 0.0132 - val_rmse: 0.1150\n",
      "Epoch 9/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0146 - mape: 27.1341 - mse: 0.0146 - rmse: 0.1208 - val_loss: 0.0132 - val_mape: 30.0191 - val_mse: 0.0132 - val_rmse: 0.1148\n",
      "Epoch 10/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0144 - mape: 27.1287 - mse: 0.0144 - rmse: 0.1200 - val_loss: 0.0130 - val_mape: 29.8121 - val_mse: 0.0130 - val_rmse: 0.1142\n",
      "Epoch 11/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0141 - mape: 26.5807 - mse: 0.0141 - rmse: 0.1185 - val_loss: 0.0129 - val_mape: 29.4608 - val_mse: 0.0129 - val_rmse: 0.1137\n",
      "Epoch 12/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0141 - mape: 26.6043 - mse: 0.0141 - rmse: 0.1186 - val_loss: 0.0128 - val_mape: 29.0400 - val_mse: 0.0129 - val_rmse: 0.1133\n",
      "Epoch 13/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0138 - mape: 25.9847 - mse: 0.0138 - rmse: 0.1173 - val_loss: 0.0128 - val_mape: 30.1808 - val_mse: 0.0128 - val_rmse: 0.1133\n",
      "Epoch 14/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0139 - mape: 26.6157 - mse: 0.0139 - rmse: 0.1178 - val_loss: 0.0126 - val_mape: 28.0171 - val_mse: 0.0126 - val_rmse: 0.1121\n",
      "Epoch 15/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0137 - mape: 26.1306 - mse: 0.0137 - rmse: 0.1168 - val_loss: 0.0125 - val_mape: 27.6848 - val_mse: 0.0125 - val_rmse: 0.1117\n",
      "Epoch 16/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0136 - mape: 26.0227 - mse: 0.0136 - rmse: 0.1166 - val_loss: 0.0125 - val_mape: 27.5884 - val_mse: 0.0125 - val_rmse: 0.1117\n",
      "Epoch 17/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0135 - mape: 25.8805 - mse: 0.0135 - rmse: 0.1164 - val_loss: 0.0125 - val_mape: 29.0985 - val_mse: 0.0125 - val_rmse: 0.1120\n",
      "Epoch 18/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0133 - mape: 25.8120 - mse: 0.0133 - rmse: 0.1151 - val_loss: 0.0124 - val_mape: 29.1554 - val_mse: 0.0124 - val_rmse: 0.1113\n",
      "Epoch 19/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0132 - mape: 25.8976 - mse: 0.0132 - rmse: 0.1151 - val_loss: 0.0123 - val_mape: 28.1660 - val_mse: 0.0123 - val_rmse: 0.1110\n",
      "Epoch 20/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0131 - mape: 25.4554 - mse: 0.0131 - rmse: 0.1143 - val_loss: 0.0123 - val_mape: 28.4091 - val_mse: 0.0123 - val_rmse: 0.1109\n",
      "Epoch 21/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0129 - mape: 25.7383 - mse: 0.0129 - rmse: 0.1134 - val_loss: 0.0123 - val_mape: 27.9461 - val_mse: 0.0123 - val_rmse: 0.1107\n",
      "Epoch 22/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0130 - mape: 25.4279 - mse: 0.0130 - rmse: 0.1142 - val_loss: 0.0122 - val_mape: 28.0506 - val_mse: 0.0122 - val_rmse: 0.1104\n",
      "Epoch 23/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0128 - mape: 25.1535 - mse: 0.0128 - rmse: 0.1131 - val_loss: 0.0122 - val_mape: 28.6196 - val_mse: 0.0122 - val_rmse: 0.1103\n",
      "Epoch 24/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0129 - mape: 25.1817 - mse: 0.0129 - rmse: 0.1134 - val_loss: 0.0122 - val_mape: 27.9467 - val_mse: 0.0122 - val_rmse: 0.1105\n",
      "Epoch 25/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0128 - mape: 25.1827 - mse: 0.0128 - rmse: 0.1132 - val_loss: 0.0122 - val_mape: 28.5060 - val_mse: 0.0122 - val_rmse: 0.1104\n",
      "Epoch 26/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0127 - mape: 25.1520 - mse: 0.0127 - rmse: 0.1125 - val_loss: 0.0121 - val_mape: 27.8309 - val_mse: 0.0121 - val_rmse: 0.1101\n",
      "Epoch 27/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0127 - mape: 24.9831 - mse: 0.0127 - rmse: 0.1127 - val_loss: 0.0121 - val_mape: 28.3406 - val_mse: 0.0121 - val_rmse: 0.1101\n",
      "Epoch 28/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0128 - mape: 24.7654 - mse: 0.0128 - rmse: 0.1129 - val_loss: 0.0121 - val_mape: 27.8315 - val_mse: 0.0121 - val_rmse: 0.1100\n",
      "Epoch 29/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0127 - mape: 25.0496 - mse: 0.0127 - rmse: 0.1126 - val_loss: 0.0120 - val_mape: 27.9838 - val_mse: 0.0120 - val_rmse: 0.1096\n",
      "Epoch 30/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0125 - mape: 24.8449 - mse: 0.0125 - rmse: 0.1117 - val_loss: 0.0119 - val_mape: 26.3647 - val_mse: 0.0119 - val_rmse: 0.1091\n",
      "Epoch 31/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0124 - mape: 24.6822 - mse: 0.0124 - rmse: 0.1113 - val_loss: 0.0119 - val_mape: 27.3812 - val_mse: 0.0119 - val_rmse: 0.1091\n",
      "Epoch 32/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0123 - mape: 24.5525 - mse: 0.0123 - rmse: 0.1111 - val_loss: 0.0120 - val_mape: 27.9766 - val_mse: 0.0120 - val_rmse: 0.1094\n",
      "Epoch 33/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0123 - mape: 24.5210 - mse: 0.0123 - rmse: 0.1108 - val_loss: 0.0119 - val_mape: 27.0248 - val_mse: 0.0119 - val_rmse: 0.1090\n",
      "Epoch 34/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0123 - mape: 24.4783 - mse: 0.0123 - rmse: 0.1110 - val_loss: 0.0119 - val_mape: 28.0491 - val_mse: 0.0119 - val_rmse: 0.1089\n",
      "Epoch 35/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0122 - mape: 24.4929 - mse: 0.0122 - rmse: 0.1103 - val_loss: 0.0119 - val_mape: 26.7872 - val_mse: 0.0119 - val_rmse: 0.1090\n",
      "Epoch 36/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0123 - mape: 24.6309 - mse: 0.0123 - rmse: 0.1108 - val_loss: 0.0117 - val_mape: 26.3919 - val_mse: 0.0117 - val_rmse: 0.1082\n",
      "Epoch 37/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0122 - mape: 24.0984 - mse: 0.0122 - rmse: 0.1105 - val_loss: 0.0118 - val_mape: 26.7330 - val_mse: 0.0118 - val_rmse: 0.1086\n",
      "Epoch 38/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0121 - mape: 24.4751 - mse: 0.0121 - rmse: 0.1099 - val_loss: 0.0119 - val_mape: 28.1426 - val_mse: 0.0119 - val_rmse: 0.1092\n",
      "Epoch 39/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0121 - mape: 24.2321 - mse: 0.0121 - rmse: 0.1099 - val_loss: 0.0118 - val_mape: 27.1036 - val_mse: 0.0118 - val_rmse: 0.1086\n",
      "Epoch 40/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0120 - mape: 24.3910 - mse: 0.0120 - rmse: 0.1097 - val_loss: 0.0118 - val_mape: 27.5038 - val_mse: 0.0118 - val_rmse: 0.1086\n",
      "Epoch 41/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0123 - mape: 24.3839 - mse: 0.0123 - rmse: 0.1107 - val_loss: 0.0117 - val_mape: 27.5464 - val_mse: 0.0117 - val_rmse: 0.1083\n",
      "Epoch 42/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - mape: 24.3858 - mse: 0.0119 - rmse: 0.1091 - val_loss: 0.0117 - val_mape: 25.9659 - val_mse: 0.0117 - val_rmse: 0.1079\n",
      "Epoch 43/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0118 - mape: 23.9492 - mse: 0.0118 - rmse: 0.1086 - val_loss: 0.0118 - val_mape: 27.8369 - val_mse: 0.0118 - val_rmse: 0.1085\n",
      "Epoch 44/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - mape: 24.2869 - mse: 0.0119 - rmse: 0.1092 - val_loss: 0.0117 - val_mape: 27.5840 - val_mse: 0.0117 - val_rmse: 0.1082\n",
      "Epoch 45/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - mape: 24.1099 - mse: 0.0119 - rmse: 0.1091 - val_loss: 0.0117 - val_mape: 26.5683 - val_mse: 0.0117 - val_rmse: 0.1083\n",
      "Epoch 46/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0118 - mape: 24.2139 - mse: 0.0118 - rmse: 0.1088 - val_loss: 0.0117 - val_mape: 27.8112 - val_mse: 0.0117 - val_rmse: 0.1081\n",
      "Epoch 47/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0118 - mape: 24.2510 - mse: 0.0118 - rmse: 0.1086 - val_loss: 0.0115 - val_mape: 27.0379 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 48/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0116 - mape: 23.9846 - mse: 0.0116 - rmse: 0.1077 - val_loss: 0.0116 - val_mape: 26.5026 - val_mse: 0.0116 - val_rmse: 0.1076\n",
      "Epoch 49/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - mape: 24.0886 - mse: 0.0117 - rmse: 0.1084 - val_loss: 0.0117 - val_mape: 27.5654 - val_mse: 0.0117 - val_rmse: 0.1081\n",
      "Epoch 50/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0116 - mape: 23.8072 - mse: 0.0116 - rmse: 0.1079 - val_loss: 0.0116 - val_mape: 26.7522 - val_mse: 0.0115 - val_rmse: 0.1075\n",
      "Epoch 51/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0116 - mape: 23.9524 - mse: 0.0116 - rmse: 0.1077 - val_loss: 0.0116 - val_mape: 27.2417 - val_mse: 0.0116 - val_rmse: 0.1077\n",
      "Epoch 52/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0116 - mape: 24.0476 - mse: 0.0116 - rmse: 0.1075 - val_loss: 0.0116 - val_mape: 27.0311 - val_mse: 0.0116 - val_rmse: 0.1077\n",
      "Epoch 53/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0114 - mape: 23.6500 - mse: 0.0114 - rmse: 0.1069 - val_loss: 0.0115 - val_mape: 26.5757 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 54/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - mape: 23.7042 - mse: 0.0117 - rmse: 0.1082 - val_loss: 0.0115 - val_mape: 26.2190 - val_mse: 0.0115 - val_rmse: 0.1072\n",
      "Epoch 55/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0115 - mape: 23.6930 - mse: 0.0115 - rmse: 0.1074 - val_loss: 0.0116 - val_mape: 27.2056 - val_mse: 0.0116 - val_rmse: 0.1078\n",
      "Epoch 56/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0116 - mape: 24.0857 - mse: 0.0116 - rmse: 0.1075 - val_loss: 0.0116 - val_mape: 26.6285 - val_mse: 0.0116 - val_rmse: 0.1077\n",
      "Epoch 57/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - mape: 23.7143 - mse: 0.0112 - rmse: 0.1059 - val_loss: 0.0115 - val_mape: 27.6250 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 58/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - mape: 23.6183 - mse: 0.0114 - rmse: 0.1070 - val_loss: 0.0115 - val_mape: 27.2553 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 59/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - mape: 23.5683 - mse: 0.0114 - rmse: 0.1069 - val_loss: 0.0116 - val_mape: 27.2849 - val_mse: 0.0116 - val_rmse: 0.1076\n",
      "Epoch 60/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0114 - mape: 23.8673 - mse: 0.0114 - rmse: 0.1067 - val_loss: 0.0115 - val_mape: 27.3645 - val_mse: 0.0115 - val_rmse: 0.1072\n",
      "Epoch 61/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - mape: 23.6018 - mse: 0.0113 - rmse: 0.1065 - val_loss: 0.0115 - val_mape: 27.8108 - val_mse: 0.0115 - val_rmse: 0.1073\n",
      "Epoch 62/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - mape: 23.4510 - mse: 0.0114 - rmse: 0.1067 - val_loss: 0.0116 - val_mape: 27.0313 - val_mse: 0.0116 - val_rmse: 0.1076\n",
      "Epoch 63/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0113 - mape: 23.4572 - mse: 0.0113 - rmse: 0.1064 - val_loss: 0.0115 - val_mape: 26.4902 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 64/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0112 - mape: 23.3978 - mse: 0.0112 - rmse: 0.1057 - val_loss: 0.0115 - val_mape: 26.6557 - val_mse: 0.0115 - val_rmse: 0.1070\n",
      "Epoch 65/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0112 - mape: 23.5107 - mse: 0.0112 - rmse: 0.1059 - val_loss: 0.0115 - val_mape: 25.6371 - val_mse: 0.0114 - val_rmse: 0.1070\n",
      "Epoch 66/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0112 - mape: 23.4636 - mse: 0.0112 - rmse: 0.1060 - val_loss: 0.0115 - val_mape: 26.0333 - val_mse: 0.0115 - val_rmse: 0.1071\n",
      "Epoch 67/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0112 - mape: 23.3159 - mse: 0.0112 - rmse: 0.1059 - val_loss: 0.0115 - val_mape: 26.6559 - val_mse: 0.0115 - val_rmse: 0.1073\n",
      "Epoch 68/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - mape: 23.3943 - mse: 0.0111 - rmse: 0.1054 - val_loss: 0.0116 - val_mape: 27.6846 - val_mse: 0.0116 - val_rmse: 0.1077\n",
      "Epoch 69/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - mape: 23.4592 - mse: 0.0111 - rmse: 0.1051 - val_loss: 0.0115 - val_mape: 26.5894 - val_mse: 0.0115 - val_rmse: 0.1073\n",
      "Epoch 70/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - mape: 23.2223 - mse: 0.0111 - rmse: 0.1055 - val_loss: 0.0115 - val_mape: 26.3506 - val_mse: 0.0115 - val_rmse: 0.1071\n",
      "Epoch 71/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - mape: 23.3277 - mse: 0.0112 - rmse: 0.1059 - val_loss: 0.0115 - val_mape: 26.8082 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 72/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - mape: 23.3771 - mse: 0.0112 - rmse: 0.1058 - val_loss: 0.0114 - val_mape: 26.4839 - val_mse: 0.0114 - val_rmse: 0.1068\n",
      "Epoch 73/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - mape: 23.4686 - mse: 0.0111 - rmse: 0.1051 - val_loss: 0.0114 - val_mape: 26.0355 - val_mse: 0.0114 - val_rmse: 0.1069\n",
      "Epoch 74/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - mape: 23.2567 - mse: 0.0110 - rmse: 0.1050 - val_loss: 0.0115 - val_mape: 27.3590 - val_mse: 0.0115 - val_rmse: 0.1073\n",
      "Epoch 75/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - mape: 23.3237 - mse: 0.0110 - rmse: 0.1049 - val_loss: 0.0116 - val_mape: 27.6567 - val_mse: 0.0115 - val_rmse: 0.1075\n",
      "Epoch 76/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - mape: 22.8630 - mse: 0.0108 - rmse: 0.1041 - val_loss: 0.0115 - val_mape: 26.8782 - val_mse: 0.0115 - val_rmse: 0.1071\n",
      "Epoch 77/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - mape: 23.3162 - mse: 0.0111 - rmse: 0.1055 - val_loss: 0.0116 - val_mape: 27.5048 - val_mse: 0.0116 - val_rmse: 0.1076\n",
      "Epoch 78/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0109 - mape: 22.8891 - mse: 0.0109 - rmse: 0.1044 - val_loss: 0.0114 - val_mape: 26.4100 - val_mse: 0.0114 - val_rmse: 0.1069\n",
      "Epoch 79/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0109 - mape: 23.1018 - mse: 0.0109 - rmse: 0.1046 - val_loss: 0.0115 - val_mape: 26.8679 - val_mse: 0.0115 - val_rmse: 0.1072\n",
      "Epoch 80/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - mape: 22.9421 - mse: 0.0110 - rmse: 0.1046 - val_loss: 0.0115 - val_mape: 26.7700 - val_mse: 0.0115 - val_rmse: 0.1072\n",
      "Epoch 81/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - mape: 23.1653 - mse: 0.0110 - rmse: 0.1049 - val_loss: 0.0115 - val_mape: 26.8031 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 82/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0107 - mape: 23.0879 - mse: 0.0107 - rmse: 0.1036 - val_loss: 0.0114 - val_mape: 25.5716 - val_mse: 0.0114 - val_rmse: 0.1067\n",
      "Epoch 83/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0109 - mape: 23.2949 - mse: 0.0109 - rmse: 0.1044 - val_loss: 0.0114 - val_mape: 26.2666 - val_mse: 0.0114 - val_rmse: 0.1065\n",
      "Epoch 84/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - mape: 23.0915 - mse: 0.0110 - rmse: 0.1048 - val_loss: 0.0115 - val_mape: 26.4938 - val_mse: 0.0115 - val_rmse: 0.1071\n",
      "Epoch 85/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - mape: 23.0191 - mse: 0.0108 - rmse: 0.1042 - val_loss: 0.0115 - val_mape: 27.1895 - val_mse: 0.0115 - val_rmse: 0.1073\n",
      "Epoch 86/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - mape: 23.0880 - mse: 0.0108 - rmse: 0.1041 - val_loss: 0.0114 - val_mape: 27.3761 - val_mse: 0.0114 - val_rmse: 0.1070\n",
      "Epoch 87/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - mape: 22.9095 - mse: 0.0108 - rmse: 0.1041 - val_loss: 0.0113 - val_mape: 26.2884 - val_mse: 0.0113 - val_rmse: 0.1065\n",
      "Epoch 88/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0107 - mape: 23.2508 - mse: 0.0107 - rmse: 0.1033 - val_loss: 0.0114 - val_mape: 26.3533 - val_mse: 0.0114 - val_rmse: 0.1068\n",
      "Epoch 89/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0106 - mape: 22.7574 - mse: 0.0106 - rmse: 0.1030 - val_loss: 0.0114 - val_mape: 27.4448 - val_mse: 0.0114 - val_rmse: 0.1067\n",
      "Epoch 90/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0106 - mape: 23.0015 - mse: 0.0106 - rmse: 0.1030 - val_loss: 0.0113 - val_mape: 26.4796 - val_mse: 0.0113 - val_rmse: 0.1064\n",
      "Epoch 91/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0107 - mape: 23.0743 - mse: 0.0107 - rmse: 0.1035 - val_loss: 0.0114 - val_mape: 26.5313 - val_mse: 0.0114 - val_rmse: 0.1069\n",
      "Epoch 92/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0105 - mape: 22.4339 - mse: 0.0105 - rmse: 0.1024 - val_loss: 0.0114 - val_mape: 26.5930 - val_mse: 0.0114 - val_rmse: 0.1066\n",
      "Epoch 93/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0106 - mape: 22.8076 - mse: 0.0106 - rmse: 0.1030 - val_loss: 0.0113 - val_mape: 27.3763 - val_mse: 0.0113 - val_rmse: 0.1065\n",
      "Epoch 94/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0107 - mape: 22.6248 - mse: 0.0107 - rmse: 0.1033 - val_loss: 0.0114 - val_mape: 27.3526 - val_mse: 0.0114 - val_rmse: 0.1069\n",
      "Epoch 95/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0105 - mape: 22.6789 - mse: 0.0105 - rmse: 0.1024 - val_loss: 0.0115 - val_mape: 27.1626 - val_mse: 0.0114 - val_rmse: 0.1070\n",
      "Epoch 96/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0105 - mape: 22.6934 - mse: 0.0105 - rmse: 0.1022 - val_loss: 0.0114 - val_mape: 27.5043 - val_mse: 0.0114 - val_rmse: 0.1069\n",
      "Epoch 97/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0105 - mape: 22.6320 - mse: 0.0105 - rmse: 0.1025 - val_loss: 0.0114 - val_mape: 26.2319 - val_mse: 0.0114 - val_rmse: 0.1066\n",
      "Epoch 98/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0106 - mape: 22.7772 - mse: 0.0106 - rmse: 0.1029 - val_loss: 0.0115 - val_mape: 26.7459 - val_mse: 0.0115 - val_rmse: 0.1071\n",
      "Epoch 99/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0106 - mape: 22.6023 - mse: 0.0106 - rmse: 0.1027 - val_loss: 0.0114 - val_mape: 26.2630 - val_mse: 0.0114 - val_rmse: 0.1069\n",
      "Epoch 100/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0104 - mape: 22.7556 - mse: 0.0104 - rmse: 0.1019 - val_loss: 0.0112 - val_mape: 25.7176 - val_mse: 0.0112 - val_rmse: 0.1059\n",
      "Epoch 101/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0106 - mape: 22.4706 - mse: 0.0106 - rmse: 0.1027 - val_loss: 0.0113 - val_mape: 26.3449 - val_mse: 0.0113 - val_rmse: 0.1064\n",
      "Epoch 102/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0105 - mape: 22.6415 - mse: 0.0105 - rmse: 0.1023 - val_loss: 0.0114 - val_mape: 27.0015 - val_mse: 0.0114 - val_rmse: 0.1067\n",
      "Epoch 103/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0105 - mape: 22.6087 - mse: 0.0105 - rmse: 0.1024 - val_loss: 0.0114 - val_mape: 27.8601 - val_mse: 0.0114 - val_rmse: 0.1069\n",
      "Epoch 104/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0104 - mape: 22.6569 - mse: 0.0104 - rmse: 0.1021 - val_loss: 0.0114 - val_mape: 27.0749 - val_mse: 0.0114 - val_rmse: 0.1067\n",
      "Epoch 105/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0103 - mape: 23.0209 - mse: 0.0103 - rmse: 0.1017 - val_loss: 0.0113 - val_mape: 26.4247 - val_mse: 0.0113 - val_rmse: 0.1062\n",
      "Epoch 106/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0104 - mape: 22.5135 - mse: 0.0104 - rmse: 0.1022 - val_loss: 0.0114 - val_mape: 26.6770 - val_mse: 0.0114 - val_rmse: 0.1068\n",
      "Epoch 107/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0107 - mape: 22.9220 - mse: 0.0107 - rmse: 0.1033 - val_loss: 0.0114 - val_mape: 27.1393 - val_mse: 0.0114 - val_rmse: 0.1069\n",
      "Epoch 108/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0104 - mape: 22.7014 - mse: 0.0104 - rmse: 0.1018 - val_loss: 0.0113 - val_mape: 26.5292 - val_mse: 0.0113 - val_rmse: 0.1064\n",
      "Epoch 109/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0105 - mape: 22.6504 - mse: 0.0105 - rmse: 0.1024 - val_loss: 0.0114 - val_mape: 27.4009 - val_mse: 0.0114 - val_rmse: 0.1070\n",
      "Epoch 110/200\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0102 - mape: 22.4197 - mse: 0.0102 - rmse: 0.1010 - val_loss: 0.0113 - val_mape: 27.2243 - val_mse: 0.0113 - val_rmse: 0.1063\n",
      "[MODELO CONGELADO]: 160.19806838035583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 43.76036366904013, 6.615161651013536, 0.8717570272545695, 18.38765095911638\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 54.852302671703875, 7.406234041110494, 0.8417108080982794, 20.32950505755585\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 47.61422864585188, 6.900306416808741, 0.8392087936800582, 21.09215221543176\n",
      "PROCESANDO ARCHIVO: Abies spectabilis\n",
      "(96429, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Abies spectabilis_best_models.json\n",
      "(56618, 4, 43) (19637, 4, 43) (15689, 4, 43)\n",
      "<Conv1D name=conv1d, built=True> False\n",
      "<BatchNormalization name=batch_normalization, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d, built=True> False\n",
      "<Conv1D name=conv1d_1, built=True> False\n",
      "<BatchNormalization name=batch_normalization_1, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_1, built=True> False\n",
      "<Conv1D name=conv1d_2, built=True> False\n",
      "<BatchNormalization name=batch_normalization_2, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_2, built=True> False\n",
      "<Dropout name=dropout, built=True> True\n",
      "<Conv1D name=conv1d_3, built=True> False\n",
      "<BatchNormalization name=batch_normalization_3, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_3, built=True> False\n",
      "<Conv1D name=conv1d_4, built=True> False\n",
      "<BatchNormalization name=batch_normalization_4, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Conv1D name=conv1d_6, built=True> True\n",
      "<BatchNormalization name=batch_normalization_6, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d, built=True> True\n",
      "<Dense name=dense, built=True> True\n",
      "<Dense name=dense_1, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.0262 - mape: 57.0010 - mse: 0.0262 - rmse: 0.1611 - val_loss: 0.0208 - val_mape: 41.7162 - val_mse: 0.0208 - val_rmse: 0.1442\n",
      "Epoch 2/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0228 - mape: 51.1803 - mse: 0.0228 - rmse: 0.1511 - val_loss: 0.0207 - val_mape: 39.8398 - val_mse: 0.0207 - val_rmse: 0.1437\n",
      "Epoch 3/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0226 - mape: 50.4404 - mse: 0.0226 - rmse: 0.1503 - val_loss: 0.0207 - val_mape: 39.3964 - val_mse: 0.0207 - val_rmse: 0.1439\n",
      "Epoch 4/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0224 - mape: 49.9986 - mse: 0.0224 - rmse: 0.1497 - val_loss: 0.0208 - val_mape: 38.0938 - val_mse: 0.0208 - val_rmse: 0.1443\n",
      "Epoch 5/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0224 - mape: 50.3079 - mse: 0.0224 - rmse: 0.1498 - val_loss: 0.0207 - val_mape: 37.3197 - val_mse: 0.0207 - val_rmse: 0.1437\n",
      "Epoch 6/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0223 - mape: 49.3635 - mse: 0.0223 - rmse: 0.1492 - val_loss: 0.0207 - val_mape: 38.2954 - val_mse: 0.0207 - val_rmse: 0.1439\n",
      "Epoch 7/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0222 - mape: 49.5958 - mse: 0.0222 - rmse: 0.1491 - val_loss: 0.0207 - val_mape: 37.3100 - val_mse: 0.0207 - val_rmse: 0.1438\n",
      "Epoch 8/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0222 - mape: 49.5385 - mse: 0.0222 - rmse: 0.1491 - val_loss: 0.0205 - val_mape: 38.3106 - val_mse: 0.0205 - val_rmse: 0.1430\n",
      "Epoch 9/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0222 - mape: 49.2919 - mse: 0.0222 - rmse: 0.1491 - val_loss: 0.0206 - val_mape: 37.7293 - val_mse: 0.0206 - val_rmse: 0.1435\n",
      "Epoch 10/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0223 - mape: 49.8504 - mse: 0.0223 - rmse: 0.1492 - val_loss: 0.0207 - val_mape: 36.6782 - val_mse: 0.0207 - val_rmse: 0.1440\n",
      "Epoch 11/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0221 - mape: 49.1692 - mse: 0.0221 - rmse: 0.1487 - val_loss: 0.0205 - val_mape: 37.2397 - val_mse: 0.0205 - val_rmse: 0.1433\n",
      "Epoch 12/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0221 - mape: 49.8440 - mse: 0.0221 - rmse: 0.1487 - val_loss: 0.0208 - val_mape: 36.6693 - val_mse: 0.0208 - val_rmse: 0.1442\n",
      "Epoch 13/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0220 - mape: 49.4656 - mse: 0.0220 - rmse: 0.1484 - val_loss: 0.0204 - val_mape: 38.4345 - val_mse: 0.0204 - val_rmse: 0.1430\n",
      "Epoch 14/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.0219 - mape: 49.5605 - mse: 0.0219 - rmse: 0.1481 - val_loss: 0.0207 - val_mape: 37.3593 - val_mse: 0.0207 - val_rmse: 0.1439\n",
      "Epoch 15/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.0219 - mape: 49.5094 - mse: 0.0219 - rmse: 0.1481 - val_loss: 0.0206 - val_mape: 39.0337 - val_mse: 0.0206 - val_rmse: 0.1437\n",
      "Epoch 16/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0218 - mape: 49.3360 - mse: 0.0218 - rmse: 0.1477 - val_loss: 0.0204 - val_mape: 39.0682 - val_mse: 0.0204 - val_rmse: 0.1429\n",
      "Epoch 17/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.0217 - mape: 48.8891 - mse: 0.0217 - rmse: 0.1473 - val_loss: 0.0204 - val_mape: 39.9530 - val_mse: 0.0204 - val_rmse: 0.1430\n",
      "Epoch 18/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0216 - mape: 48.5841 - mse: 0.0216 - rmse: 0.1470 - val_loss: 0.0204 - val_mape: 40.5159 - val_mse: 0.0204 - val_rmse: 0.1429\n",
      "Epoch 19/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0216 - mape: 48.3613 - mse: 0.0216 - rmse: 0.1469 - val_loss: 0.0207 - val_mape: 37.9982 - val_mse: 0.0207 - val_rmse: 0.1440\n",
      "Epoch 20/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0214 - mape: 48.0588 - mse: 0.0214 - rmse: 0.1463 - val_loss: 0.0205 - val_mape: 38.0583 - val_mse: 0.0205 - val_rmse: 0.1433\n",
      "Epoch 21/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0213 - mape: 48.2894 - mse: 0.0213 - rmse: 0.1459 - val_loss: 0.0208 - val_mape: 38.2484 - val_mse: 0.0208 - val_rmse: 0.1442\n",
      "Epoch 22/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.0213 - mape: 47.9673 - mse: 0.0213 - rmse: 0.1460 - val_loss: 0.0209 - val_mape: 39.5265 - val_mse: 0.0209 - val_rmse: 0.1446\n",
      "Epoch 23/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0213 - mape: 48.3504 - mse: 0.0213 - rmse: 0.1459 - val_loss: 0.0207 - val_mape: 38.9933 - val_mse: 0.0207 - val_rmse: 0.1438\n",
      "Epoch 24/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0213 - mape: 48.0592 - mse: 0.0213 - rmse: 0.1458 - val_loss: 0.0211 - val_mape: 39.5728 - val_mse: 0.0211 - val_rmse: 0.1452\n",
      "Epoch 25/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0212 - mape: 47.7288 - mse: 0.0212 - rmse: 0.1455 - val_loss: 0.0207 - val_mape: 38.2547 - val_mse: 0.0207 - val_rmse: 0.1440\n",
      "Epoch 26/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0212 - mape: 47.8030 - mse: 0.0212 - rmse: 0.1455 - val_loss: 0.0211 - val_mape: 38.1984 - val_mse: 0.0211 - val_rmse: 0.1451\n",
      "Epoch 27/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0211 - mape: 48.2508 - mse: 0.0211 - rmse: 0.1453 - val_loss: 0.0207 - val_mape: 39.3720 - val_mse: 0.0207 - val_rmse: 0.1437\n",
      "Epoch 28/200\n",
      "\u001b[1m2360/2360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0211 - mape: 47.9221 - mse: 0.0211 - rmse: 0.1452 - val_loss: 0.0208 - val_mape: 38.4021 - val_mse: 0.0208 - val_rmse: 0.1442\n",
      "[MODELO CONGELADO]: 173.47018814086914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 28.24121767954608, 5.314246670935221, 0.7522268854163504, 27.257396530688283\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 28.124862404414127, 5.303287886246996, 0.7437362344955234, 27.49576046984188\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 28.792482947876575, 5.365862740312743, 0.7864675774249603, 26.704003459944104\n",
      "PROCESANDO ARCHIVO: Juniperus excelsa M.-Bieb\n",
      "(7449, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Juniperus excelsa M.-Bieb_best_models.json\n",
      "(4189, 4, 43) (1707, 4, 43) (1228, 4, 43)\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Conv1D name=conv1d_6, built=True> False\n",
      "<BatchNormalization name=batch_normalization_6, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Conv1D name=conv1d_7, built=True> False\n",
      "<BatchNormalization name=batch_normalization_7, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_6, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_8, built=True> False\n",
      "<BatchNormalization name=batch_normalization_8, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_7, built=True> False\n",
      "<Conv1D name=conv1d_9, built=True> False\n",
      "<BatchNormalization name=batch_normalization_9, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_8, built=True> False\n",
      "<Conv1D name=conv1d_10, built=True> False\n",
      "<BatchNormalization name=batch_normalization_10, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_9, built=True> False\n",
      "<Conv1D name=conv1d_11, built=True> True\n",
      "<BatchNormalization name=batch_normalization_11, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0217 - mape: 47.6204 - mse: 0.0217 - rmse: 0.1471 - val_loss: 0.0195 - val_mape: 56.9481 - val_mse: 0.0195 - val_rmse: 0.1398\n",
      "Epoch 2/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0206 - mape: 45.6882 - mse: 0.0206 - rmse: 0.1434 - val_loss: 0.0174 - val_mape: 46.9923 - val_mse: 0.0175 - val_rmse: 0.1321\n",
      "Epoch 3/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0203 - mape: 44.7449 - mse: 0.0203 - rmse: 0.1424 - val_loss: 0.0174 - val_mape: 46.5726 - val_mse: 0.0174 - val_rmse: 0.1318\n",
      "Epoch 4/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0202 - mape: 44.7112 - mse: 0.0202 - rmse: 0.1421 - val_loss: 0.0179 - val_mape: 50.4937 - val_mse: 0.0179 - val_rmse: 0.1339\n",
      "Epoch 5/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0200 - mape: 44.4136 - mse: 0.0200 - rmse: 0.1412 - val_loss: 0.0170 - val_mape: 42.9523 - val_mse: 0.0170 - val_rmse: 0.1303\n",
      "Epoch 6/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0197 - mape: 44.5901 - mse: 0.0197 - rmse: 0.1403 - val_loss: 0.0168 - val_mape: 42.1116 - val_mse: 0.0169 - val_rmse: 0.1298\n",
      "Epoch 7/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0195 - mape: 44.3155 - mse: 0.0195 - rmse: 0.1396 - val_loss: 0.0171 - val_mape: 44.8726 - val_mse: 0.0171 - val_rmse: 0.1308\n",
      "Epoch 8/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0194 - mape: 44.1576 - mse: 0.0194 - rmse: 0.1392 - val_loss: 0.0167 - val_mape: 42.2512 - val_mse: 0.0168 - val_rmse: 0.1294\n",
      "Epoch 9/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0192 - mape: 43.7838 - mse: 0.0192 - rmse: 0.1386 - val_loss: 0.0170 - val_mape: 44.8063 - val_mse: 0.0170 - val_rmse: 0.1304\n",
      "Epoch 10/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0192 - mape: 43.4625 - mse: 0.0192 - rmse: 0.1383 - val_loss: 0.0165 - val_mape: 38.5680 - val_mse: 0.0165 - val_rmse: 0.1283\n",
      "Epoch 11/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0190 - mape: 43.2964 - mse: 0.0190 - rmse: 0.1379 - val_loss: 0.0165 - val_mape: 40.1194 - val_mse: 0.0166 - val_rmse: 0.1286\n",
      "Epoch 12/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0189 - mape: 42.7742 - mse: 0.0189 - rmse: 0.1375 - val_loss: 0.0166 - val_mape: 40.1009 - val_mse: 0.0166 - val_rmse: 0.1287\n",
      "Epoch 13/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0189 - mape: 42.4121 - mse: 0.0189 - rmse: 0.1374 - val_loss: 0.0164 - val_mape: 39.3438 - val_mse: 0.0164 - val_rmse: 0.1279\n",
      "Epoch 14/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0188 - mape: 42.0477 - mse: 0.0188 - rmse: 0.1371 - val_loss: 0.0164 - val_mape: 39.8901 - val_mse: 0.0165 - val_rmse: 0.1282\n",
      "Epoch 15/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0188 - mape: 41.9177 - mse: 0.0188 - rmse: 0.1370 - val_loss: 0.0166 - val_mape: 38.8256 - val_mse: 0.0166 - val_rmse: 0.1287\n",
      "Epoch 16/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0187 - mape: 41.5976 - mse: 0.0187 - rmse: 0.1367 - val_loss: 0.0166 - val_mape: 40.2943 - val_mse: 0.0166 - val_rmse: 0.1288\n",
      "Epoch 17/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0186 - mape: 41.4369 - mse: 0.0186 - rmse: 0.1363 - val_loss: 0.0164 - val_mape: 39.6889 - val_mse: 0.0164 - val_rmse: 0.1280\n",
      "Epoch 18/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0185 - mape: 41.0672 - mse: 0.0185 - rmse: 0.1360 - val_loss: 0.0166 - val_mape: 39.8284 - val_mse: 0.0166 - val_rmse: 0.1289\n",
      "Epoch 19/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0185 - mape: 41.0116 - mse: 0.0185 - rmse: 0.1359 - val_loss: 0.0167 - val_mape: 39.7335 - val_mse: 0.0167 - val_rmse: 0.1293\n",
      "Epoch 20/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0185 - mape: 40.5921 - mse: 0.0185 - rmse: 0.1358 - val_loss: 0.0165 - val_mape: 39.0355 - val_mse: 0.0165 - val_rmse: 0.1284\n",
      "Epoch 21/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0184 - mape: 40.5025 - mse: 0.0184 - rmse: 0.1354 - val_loss: 0.0166 - val_mape: 39.5980 - val_mse: 0.0166 - val_rmse: 0.1288\n",
      "Epoch 22/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0183 - mape: 40.4326 - mse: 0.0183 - rmse: 0.1353 - val_loss: 0.0166 - val_mape: 39.4935 - val_mse: 0.0167 - val_rmse: 0.1290\n",
      "Epoch 23/200\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0183 - mape: 40.4402 - mse: 0.0183 - rmse: 0.1352 - val_loss: 0.0167 - val_mape: 40.2157 - val_mse: 0.0167 - val_rmse: 0.1293\n",
      "[MODELO CONGELADO]: 19.521233558654785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 7.12111022379993, 2.6685408416960628, 0.7105410924755257, 27.201018896668806\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 7.075896642845179, 2.6600557593488863, 0.7301518607376122, 27.860886184235266\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 17.71574221320469, 4.209007271697768, 0.7373494000420675, 29.5896152167504\n",
      "PROCESANDO ARCHIVO: Cedrus deodara\n",
      "(37273, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Cedrus deodara_best_models.json\n",
      "(22017, 4, 43) (7410, 4, 43) (6282, 4, 43)\n",
      "<Conv1D name=conv1d, built=True> False\n",
      "<BatchNormalization name=batch_normalization, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d, built=True> False\n",
      "<Conv1D name=conv1d_1, built=True> False\n",
      "<BatchNormalization name=batch_normalization_1, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_1, built=True> False\n",
      "<Conv1D name=conv1d_2, built=True> False\n",
      "<BatchNormalization name=batch_normalization_2, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_2, built=True> False\n",
      "<Dropout name=dropout, built=True> True\n",
      "<Conv1D name=conv1d_3, built=True> False\n",
      "<BatchNormalization name=batch_normalization_3, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_3, built=True> False\n",
      "<Conv1D name=conv1d_4, built=True> False\n",
      "<BatchNormalization name=batch_normalization_4, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Conv1D name=conv1d_6, built=True> True\n",
      "<BatchNormalization name=batch_normalization_6, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d, built=True> True\n",
      "<Dense name=dense, built=True> True\n",
      "<Dense name=dense_1, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0250 - mape: 48.9793 - mse: 0.0250 - rmse: 0.1581 - val_loss: 0.0251 - val_mape: 53.5420 - val_mse: 0.0250 - val_rmse: 0.1583\n",
      "Epoch 2/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0252 - mape: 50.6366 - mse: 0.0252 - rmse: 0.1587 - val_loss: 0.0246 - val_mape: 52.0744 - val_mse: 0.0246 - val_rmse: 0.1568\n",
      "Epoch 3/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0248 - mape: 50.4178 - mse: 0.0248 - rmse: 0.1576 - val_loss: 0.0244 - val_mape: 51.2030 - val_mse: 0.0244 - val_rmse: 0.1561\n",
      "Epoch 4/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0249 - mape: 50.4371 - mse: 0.0249 - rmse: 0.1579 - val_loss: 0.0243 - val_mape: 51.3690 - val_mse: 0.0243 - val_rmse: 0.1559\n",
      "Epoch 5/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0247 - mape: 50.5964 - mse: 0.0247 - rmse: 0.1573 - val_loss: 0.0245 - val_mape: 51.4158 - val_mse: 0.0244 - val_rmse: 0.1564\n",
      "Epoch 6/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0249 - mape: 50.5545 - mse: 0.0249 - rmse: 0.1576 - val_loss: 0.0243 - val_mape: 51.0071 - val_mse: 0.0243 - val_rmse: 0.1560\n",
      "Epoch 7/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0247 - mape: 50.0890 - mse: 0.0247 - rmse: 0.1573 - val_loss: 0.0244 - val_mape: 51.4934 - val_mse: 0.0244 - val_rmse: 0.1562\n",
      "Epoch 8/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0248 - mape: 50.0188 - mse: 0.0248 - rmse: 0.1573 - val_loss: 0.0242 - val_mape: 50.6010 - val_mse: 0.0242 - val_rmse: 0.1555\n",
      "Epoch 9/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0247 - mape: 50.1017 - mse: 0.0247 - rmse: 0.1573 - val_loss: 0.0243 - val_mape: 51.3357 - val_mse: 0.0243 - val_rmse: 0.1558\n",
      "Epoch 10/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0249 - mape: 50.2084 - mse: 0.0249 - rmse: 0.1577 - val_loss: 0.0244 - val_mape: 51.3432 - val_mse: 0.0244 - val_rmse: 0.1562\n",
      "Epoch 11/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0247 - mape: 50.0759 - mse: 0.0247 - rmse: 0.1573 - val_loss: 0.0243 - val_mape: 51.0059 - val_mse: 0.0243 - val_rmse: 0.1558\n",
      "Epoch 12/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0247 - mape: 50.0984 - mse: 0.0247 - rmse: 0.1571 - val_loss: 0.0241 - val_mape: 50.5374 - val_mse: 0.0241 - val_rmse: 0.1554\n",
      "Epoch 13/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0248 - mape: 50.1917 - mse: 0.0248 - rmse: 0.1574 - val_loss: 0.0241 - val_mape: 50.3066 - val_mse: 0.0241 - val_rmse: 0.1553\n",
      "Epoch 14/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0246 - mape: 49.8928 - mse: 0.0246 - rmse: 0.1569 - val_loss: 0.0239 - val_mape: 49.8283 - val_mse: 0.0239 - val_rmse: 0.1547\n",
      "Epoch 15/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0246 - mape: 49.8955 - mse: 0.0246 - rmse: 0.1569 - val_loss: 0.0240 - val_mape: 49.6942 - val_mse: 0.0240 - val_rmse: 0.1550\n",
      "Epoch 16/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0247 - mape: 49.6123 - mse: 0.0247 - rmse: 0.1571 - val_loss: 0.0242 - val_mape: 50.4794 - val_mse: 0.0242 - val_rmse: 0.1556\n",
      "Epoch 17/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0246 - mape: 49.9190 - mse: 0.0246 - rmse: 0.1567 - val_loss: 0.0242 - val_mape: 51.3659 - val_mse: 0.0242 - val_rmse: 0.1557\n",
      "Epoch 18/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0246 - mape: 50.0445 - mse: 0.0246 - rmse: 0.1568 - val_loss: 0.0240 - val_mape: 50.0838 - val_mse: 0.0240 - val_rmse: 0.1551\n",
      "Epoch 19/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0245 - mape: 49.5147 - mse: 0.0245 - rmse: 0.1564 - val_loss: 0.0236 - val_mape: 47.5789 - val_mse: 0.0236 - val_rmse: 0.1537\n",
      "Epoch 20/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0245 - mape: 49.3489 - mse: 0.0245 - rmse: 0.1565 - val_loss: 0.0238 - val_mape: 49.5862 - val_mse: 0.0238 - val_rmse: 0.1544\n",
      "Epoch 21/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0246 - mape: 49.7871 - mse: 0.0246 - rmse: 0.1567 - val_loss: 0.0236 - val_mape: 48.0882 - val_mse: 0.0236 - val_rmse: 0.1536\n",
      "Epoch 22/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0244 - mape: 49.4464 - mse: 0.0244 - rmse: 0.1563 - val_loss: 0.0236 - val_mape: 44.5728 - val_mse: 0.0236 - val_rmse: 0.1535\n",
      "Epoch 23/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0245 - mape: 49.1998 - mse: 0.0245 - rmse: 0.1565 - val_loss: 0.0235 - val_mape: 43.7774 - val_mse: 0.0235 - val_rmse: 0.1533\n",
      "Epoch 24/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0243 - mape: 48.9979 - mse: 0.0243 - rmse: 0.1558 - val_loss: 0.0235 - val_mape: 45.5700 - val_mse: 0.0235 - val_rmse: 0.1532\n",
      "Epoch 25/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0241 - mape: 48.8207 - mse: 0.0241 - rmse: 0.1553 - val_loss: 0.0236 - val_mape: 43.7096 - val_mse: 0.0235 - val_rmse: 0.1535\n",
      "Epoch 26/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0240 - mape: 48.3095 - mse: 0.0240 - rmse: 0.1549 - val_loss: 0.0242 - val_mape: 40.9256 - val_mse: 0.0242 - val_rmse: 0.1557\n",
      "Epoch 27/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0240 - mape: 48.1679 - mse: 0.0240 - rmse: 0.1549 - val_loss: 0.0248 - val_mape: 40.7125 - val_mse: 0.0248 - val_rmse: 0.1574\n",
      "Epoch 28/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0240 - mape: 47.7381 - mse: 0.0240 - rmse: 0.1549 - val_loss: 0.0362 - val_mape: 40.5040 - val_mse: 0.0362 - val_rmse: 0.1902\n",
      "Epoch 29/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0240 - mape: 47.9408 - mse: 0.0240 - rmse: 0.1548 - val_loss: 0.0259 - val_mape: 39.2549 - val_mse: 0.0259 - val_rmse: 0.1611\n",
      "Epoch 30/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0239 - mape: 47.7215 - mse: 0.0239 - rmse: 0.1546 - val_loss: 0.0254 - val_mape: 39.1506 - val_mse: 0.0254 - val_rmse: 0.1594\n",
      "Epoch 31/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0238 - mape: 47.6126 - mse: 0.0238 - rmse: 0.1544 - val_loss: 0.0265 - val_mape: 38.9264 - val_mse: 0.0265 - val_rmse: 0.1628\n",
      "Epoch 32/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0237 - mape: 47.4504 - mse: 0.0237 - rmse: 0.1540 - val_loss: 0.0236 - val_mape: 42.0511 - val_mse: 0.0236 - val_rmse: 0.1537\n",
      "Epoch 33/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0238 - mape: 47.4057 - mse: 0.0238 - rmse: 0.1542 - val_loss: 0.0264 - val_mape: 39.1323 - val_mse: 0.0264 - val_rmse: 0.1625\n",
      "Epoch 34/200\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0237 - mape: 47.1383 - mse: 0.0237 - rmse: 0.1538 - val_loss: 0.0255 - val_mape: 39.3302 - val_mse: 0.0255 - val_rmse: 0.1596\n",
      "[MODELO CONGELADO]: 82.6887457370758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 125.8096807762736, 11.216491464636952, 0.7054533259657845, 28.802798337144246\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 150.1287953260812, 12.252705632882934, 0.663494143907865, 29.70403276249924\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 107.17994644358166, 10.35277481854897, 0.6958812303229707, 28.507593670054394\n",
      "PROCESANDO ARCHIVO: Tsuga dumosa\n",
      "(20493, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Tsuga dumosa_best_models.json\n",
      "(11935, 4, 43) (4144, 4, 43) (3469, 4, 43)\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Conv1D name=conv1d_6, built=True> False\n",
      "<BatchNormalization name=batch_normalization_6, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Conv1D name=conv1d_7, built=True> False\n",
      "<BatchNormalization name=batch_normalization_7, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_6, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_8, built=True> False\n",
      "<BatchNormalization name=batch_normalization_8, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_7, built=True> False\n",
      "<Conv1D name=conv1d_9, built=True> False\n",
      "<BatchNormalization name=batch_normalization_9, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_8, built=True> False\n",
      "<Conv1D name=conv1d_10, built=True> False\n",
      "<BatchNormalization name=batch_normalization_10, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_9, built=True> False\n",
      "<Conv1D name=conv1d_11, built=True> True\n",
      "<BatchNormalization name=batch_normalization_11, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0224 - mape: 42.5547 - mse: 0.0224 - rmse: 0.1495 - val_loss: 0.0199 - val_mape: 34.5047 - val_mse: 0.0199 - val_rmse: 0.1410\n",
      "Epoch 2/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0195 - mape: 38.4045 - mse: 0.0195 - rmse: 0.1396 - val_loss: 0.0192 - val_mape: 33.1707 - val_mse: 0.0192 - val_rmse: 0.1386\n",
      "Epoch 3/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0189 - mape: 37.0039 - mse: 0.0189 - rmse: 0.1375 - val_loss: 0.0190 - val_mape: 32.6390 - val_mse: 0.0190 - val_rmse: 0.1377\n",
      "Epoch 4/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0186 - mape: 36.3090 - mse: 0.0186 - rmse: 0.1362 - val_loss: 0.0188 - val_mape: 32.8035 - val_mse: 0.0188 - val_rmse: 0.1372\n",
      "Epoch 5/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0184 - mape: 36.0137 - mse: 0.0184 - rmse: 0.1355 - val_loss: 0.0187 - val_mape: 32.7255 - val_mse: 0.0187 - val_rmse: 0.1366\n",
      "Epoch 6/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0180 - mape: 35.3617 - mse: 0.0180 - rmse: 0.1341 - val_loss: 0.0185 - val_mape: 32.8324 - val_mse: 0.0185 - val_rmse: 0.1360\n",
      "Epoch 7/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0182 - mape: 35.7349 - mse: 0.0182 - rmse: 0.1347 - val_loss: 0.0185 - val_mape: 32.3822 - val_mse: 0.0185 - val_rmse: 0.1359\n",
      "Epoch 8/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0178 - mape: 34.9670 - mse: 0.0178 - rmse: 0.1333 - val_loss: 0.0184 - val_mape: 32.4212 - val_mse: 0.0184 - val_rmse: 0.1356\n",
      "Epoch 9/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0179 - mape: 35.2757 - mse: 0.0179 - rmse: 0.1337 - val_loss: 0.0183 - val_mape: 32.0594 - val_mse: 0.0184 - val_rmse: 0.1354\n",
      "Epoch 10/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0178 - mape: 34.8413 - mse: 0.0178 - rmse: 0.1334 - val_loss: 0.0184 - val_mape: 32.0791 - val_mse: 0.0184 - val_rmse: 0.1355\n",
      "Epoch 11/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0175 - mape: 34.5132 - mse: 0.0175 - rmse: 0.1321 - val_loss: 0.0182 - val_mape: 31.9137 - val_mse: 0.0183 - val_rmse: 0.1351\n",
      "Epoch 12/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0173 - mape: 34.4611 - mse: 0.0173 - rmse: 0.1316 - val_loss: 0.0182 - val_mape: 31.7215 - val_mse: 0.0183 - val_rmse: 0.1350\n",
      "Epoch 13/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0175 - mape: 34.5904 - mse: 0.0175 - rmse: 0.1322 - val_loss: 0.0181 - val_mape: 32.0524 - val_mse: 0.0182 - val_rmse: 0.1347\n",
      "Epoch 14/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0172 - mape: 34.4284 - mse: 0.0172 - rmse: 0.1312 - val_loss: 0.0182 - val_mape: 31.6439 - val_mse: 0.0182 - val_rmse: 0.1349\n",
      "Epoch 15/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0171 - mape: 33.8068 - mse: 0.0171 - rmse: 0.1307 - val_loss: 0.0181 - val_mape: 31.5957 - val_mse: 0.0181 - val_rmse: 0.1345\n",
      "Epoch 16/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0173 - mape: 34.3618 - mse: 0.0173 - rmse: 0.1316 - val_loss: 0.0182 - val_mape: 31.4657 - val_mse: 0.0182 - val_rmse: 0.1349\n",
      "Epoch 17/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0171 - mape: 33.9013 - mse: 0.0171 - rmse: 0.1308 - val_loss: 0.0181 - val_mape: 31.2706 - val_mse: 0.0182 - val_rmse: 0.1346\n",
      "Epoch 18/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0170 - mape: 33.8917 - mse: 0.0170 - rmse: 0.1305 - val_loss: 0.0180 - val_mape: 31.5020 - val_mse: 0.0180 - val_rmse: 0.1342\n",
      "Epoch 19/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0169 - mape: 34.2693 - mse: 0.0169 - rmse: 0.1301 - val_loss: 0.0180 - val_mape: 31.4983 - val_mse: 0.0180 - val_rmse: 0.1342\n",
      "Epoch 20/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0169 - mape: 34.3440 - mse: 0.0169 - rmse: 0.1299 - val_loss: 0.0181 - val_mape: 31.2885 - val_mse: 0.0182 - val_rmse: 0.1346\n",
      "Epoch 21/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0168 - mape: 33.5985 - mse: 0.0168 - rmse: 0.1297 - val_loss: 0.0180 - val_mape: 31.2934 - val_mse: 0.0180 - val_rmse: 0.1341\n",
      "Epoch 22/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0166 - mape: 33.4047 - mse: 0.0166 - rmse: 0.1289 - val_loss: 0.0179 - val_mape: 31.4537 - val_mse: 0.0180 - val_rmse: 0.1339\n",
      "Epoch 23/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0164 - mape: 33.4240 - mse: 0.0164 - rmse: 0.1282 - val_loss: 0.0180 - val_mape: 31.2818 - val_mse: 0.0180 - val_rmse: 0.1340\n",
      "Epoch 24/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0168 - mape: 33.3632 - mse: 0.0168 - rmse: 0.1295 - val_loss: 0.0179 - val_mape: 31.1564 - val_mse: 0.0179 - val_rmse: 0.1338\n",
      "Epoch 25/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0166 - mape: 33.3541 - mse: 0.0166 - rmse: 0.1288 - val_loss: 0.0178 - val_mape: 31.2133 - val_mse: 0.0179 - val_rmse: 0.1335\n",
      "Epoch 26/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0167 - mape: 33.3659 - mse: 0.0167 - rmse: 0.1290 - val_loss: 0.0178 - val_mape: 31.1978 - val_mse: 0.0178 - val_rmse: 0.1334\n",
      "Epoch 27/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0169 - mape: 33.6582 - mse: 0.0169 - rmse: 0.1299 - val_loss: 0.0178 - val_mape: 31.6277 - val_mse: 0.0178 - val_rmse: 0.1333\n",
      "Epoch 28/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0166 - mape: 33.4248 - mse: 0.0166 - rmse: 0.1288 - val_loss: 0.0177 - val_mape: 31.2115 - val_mse: 0.0178 - val_rmse: 0.1332\n",
      "Epoch 29/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0166 - mape: 33.4639 - mse: 0.0166 - rmse: 0.1289 - val_loss: 0.0178 - val_mape: 30.8960 - val_mse: 0.0178 - val_rmse: 0.1334\n",
      "Epoch 30/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0165 - mape: 33.2001 - mse: 0.0165 - rmse: 0.1286 - val_loss: 0.0177 - val_mape: 31.5768 - val_mse: 0.0177 - val_rmse: 0.1329\n",
      "Epoch 31/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0165 - mape: 33.2149 - mse: 0.0165 - rmse: 0.1286 - val_loss: 0.0176 - val_mape: 31.1610 - val_mse: 0.0177 - val_rmse: 0.1328\n",
      "Epoch 32/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0164 - mape: 32.7409 - mse: 0.0164 - rmse: 0.1281 - val_loss: 0.0178 - val_mape: 30.6397 - val_mse: 0.0179 - val_rmse: 0.1336\n",
      "Epoch 33/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0164 - mape: 32.7542 - mse: 0.0164 - rmse: 0.1280 - val_loss: 0.0178 - val_mape: 30.9493 - val_mse: 0.0178 - val_rmse: 0.1333\n",
      "Epoch 34/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0162 - mape: 32.4990 - mse: 0.0162 - rmse: 0.1273 - val_loss: 0.0178 - val_mape: 30.5901 - val_mse: 0.0178 - val_rmse: 0.1333\n",
      "Epoch 35/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0162 - mape: 32.9142 - mse: 0.0162 - rmse: 0.1273 - val_loss: 0.0175 - val_mape: 31.6404 - val_mse: 0.0176 - val_rmse: 0.1325\n",
      "Epoch 36/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0164 - mape: 32.9638 - mse: 0.0164 - rmse: 0.1279 - val_loss: 0.0177 - val_mape: 30.7997 - val_mse: 0.0177 - val_rmse: 0.1330\n",
      "Epoch 37/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0160 - mape: 32.6969 - mse: 0.0160 - rmse: 0.1265 - val_loss: 0.0177 - val_mape: 30.7567 - val_mse: 0.0177 - val_rmse: 0.1330\n",
      "Epoch 38/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0163 - mape: 32.7975 - mse: 0.0163 - rmse: 0.1278 - val_loss: 0.0176 - val_mape: 30.9034 - val_mse: 0.0177 - val_rmse: 0.1327\n",
      "Epoch 39/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0161 - mape: 32.7039 - mse: 0.0161 - rmse: 0.1270 - val_loss: 0.0175 - val_mape: 30.9719 - val_mse: 0.0175 - val_rmse: 0.1323\n",
      "Epoch 40/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0161 - mape: 32.5568 - mse: 0.0161 - rmse: 0.1270 - val_loss: 0.0175 - val_mape: 30.8715 - val_mse: 0.0176 - val_rmse: 0.1324\n",
      "Epoch 41/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0162 - mape: 32.7424 - mse: 0.0162 - rmse: 0.1273 - val_loss: 0.0176 - val_mape: 30.8968 - val_mse: 0.0176 - val_rmse: 0.1327\n",
      "Epoch 42/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0159 - mape: 32.4013 - mse: 0.0159 - rmse: 0.1260 - val_loss: 0.0176 - val_mape: 30.2039 - val_mse: 0.0176 - val_rmse: 0.1327\n",
      "Epoch 43/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0161 - mape: 32.6298 - mse: 0.0161 - rmse: 0.1270 - val_loss: 0.0177 - val_mape: 30.5278 - val_mse: 0.0177 - val_rmse: 0.1329\n",
      "Epoch 44/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0160 - mape: 32.4323 - mse: 0.0160 - rmse: 0.1264 - val_loss: 0.0175 - val_mape: 30.8302 - val_mse: 0.0175 - val_rmse: 0.1323\n",
      "Epoch 45/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0160 - mape: 32.4812 - mse: 0.0160 - rmse: 0.1264 - val_loss: 0.0176 - val_mape: 30.2061 - val_mse: 0.0176 - val_rmse: 0.1327\n",
      "Epoch 46/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0160 - mape: 32.0813 - mse: 0.0160 - rmse: 0.1263 - val_loss: 0.0176 - val_mape: 30.4786 - val_mse: 0.0176 - val_rmse: 0.1327\n",
      "Epoch 47/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0160 - mape: 32.6493 - mse: 0.0160 - rmse: 0.1266 - val_loss: 0.0175 - val_mape: 30.4040 - val_mse: 0.0175 - val_rmse: 0.1324\n",
      "Epoch 48/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0157 - mape: 32.0896 - mse: 0.0157 - rmse: 0.1252 - val_loss: 0.0175 - val_mape: 30.8370 - val_mse: 0.0175 - val_rmse: 0.1323\n",
      "Epoch 49/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0162 - mape: 32.5894 - mse: 0.0162 - rmse: 0.1274 - val_loss: 0.0177 - val_mape: 29.8416 - val_mse: 0.0177 - val_rmse: 0.1329\n",
      "Epoch 50/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0159 - mape: 32.2061 - mse: 0.0159 - rmse: 0.1261 - val_loss: 0.0175 - val_mape: 30.5529 - val_mse: 0.0175 - val_rmse: 0.1321\n",
      "Epoch 51/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0158 - mape: 32.1966 - mse: 0.0158 - rmse: 0.1255 - val_loss: 0.0175 - val_mape: 30.2602 - val_mse: 0.0175 - val_rmse: 0.1323\n",
      "Epoch 52/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0161 - mape: 32.3091 - mse: 0.0161 - rmse: 0.1267 - val_loss: 0.0175 - val_mape: 30.5707 - val_mse: 0.0175 - val_rmse: 0.1322\n",
      "Epoch 53/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0159 - mape: 32.2796 - mse: 0.0159 - rmse: 0.1261 - val_loss: 0.0174 - val_mape: 30.3564 - val_mse: 0.0174 - val_rmse: 0.1319\n",
      "Epoch 54/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0158 - mape: 31.8540 - mse: 0.0158 - rmse: 0.1255 - val_loss: 0.0175 - val_mape: 30.4555 - val_mse: 0.0175 - val_rmse: 0.1323\n",
      "Epoch 55/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0157 - mape: 32.1957 - mse: 0.0157 - rmse: 0.1255 - val_loss: 0.0175 - val_mape: 30.4297 - val_mse: 0.0175 - val_rmse: 0.1323\n",
      "Epoch 56/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0158 - mape: 32.4527 - mse: 0.0158 - rmse: 0.1258 - val_loss: 0.0175 - val_mape: 30.1109 - val_mse: 0.0176 - val_rmse: 0.1324\n",
      "Epoch 57/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0156 - mape: 32.4036 - mse: 0.0156 - rmse: 0.1249 - val_loss: 0.0175 - val_mape: 30.2015 - val_mse: 0.0175 - val_rmse: 0.1322\n",
      "Epoch 58/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0157 - mape: 31.9740 - mse: 0.0157 - rmse: 0.1251 - val_loss: 0.0176 - val_mape: 29.8877 - val_mse: 0.0176 - val_rmse: 0.1325\n",
      "Epoch 59/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0155 - mape: 31.5751 - mse: 0.0155 - rmse: 0.1245 - val_loss: 0.0174 - val_mape: 30.3584 - val_mse: 0.0174 - val_rmse: 0.1318\n",
      "Epoch 60/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0157 - mape: 31.8344 - mse: 0.0157 - rmse: 0.1252 - val_loss: 0.0174 - val_mape: 30.2734 - val_mse: 0.0175 - val_rmse: 0.1321\n",
      "Epoch 61/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0157 - mape: 31.8695 - mse: 0.0157 - rmse: 0.1251 - val_loss: 0.0174 - val_mape: 30.6033 - val_mse: 0.0174 - val_rmse: 0.1318\n",
      "Epoch 62/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0157 - mape: 31.8718 - mse: 0.0157 - rmse: 0.1252 - val_loss: 0.0176 - val_mape: 29.6865 - val_mse: 0.0176 - val_rmse: 0.1326\n",
      "Epoch 63/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0156 - mape: 31.6965 - mse: 0.0156 - rmse: 0.1248 - val_loss: 0.0172 - val_mape: 30.2653 - val_mse: 0.0173 - val_rmse: 0.1313\n",
      "Epoch 64/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0156 - mape: 31.9628 - mse: 0.0156 - rmse: 0.1250 - val_loss: 0.0173 - val_mape: 30.4318 - val_mse: 0.0174 - val_rmse: 0.1317\n",
      "Epoch 65/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0156 - mape: 31.7891 - mse: 0.0156 - rmse: 0.1248 - val_loss: 0.0175 - val_mape: 29.9364 - val_mse: 0.0175 - val_rmse: 0.1321\n",
      "Epoch 66/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0154 - mape: 31.8098 - mse: 0.0154 - rmse: 0.1241 - val_loss: 0.0174 - val_mape: 29.8856 - val_mse: 0.0174 - val_rmse: 0.1319\n",
      "Epoch 67/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0154 - mape: 31.6896 - mse: 0.0154 - rmse: 0.1242 - val_loss: 0.0173 - val_mape: 29.9731 - val_mse: 0.0174 - val_rmse: 0.1317\n",
      "Epoch 68/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0153 - mape: 31.6411 - mse: 0.0153 - rmse: 0.1236 - val_loss: 0.0175 - val_mape: 30.1960 - val_mse: 0.0175 - val_rmse: 0.1322\n",
      "Epoch 69/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0156 - mape: 31.7864 - mse: 0.0156 - rmse: 0.1250 - val_loss: 0.0173 - val_mape: 30.2219 - val_mse: 0.0173 - val_rmse: 0.1316\n",
      "Epoch 70/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0154 - mape: 31.7752 - mse: 0.0154 - rmse: 0.1240 - val_loss: 0.0173 - val_mape: 30.3037 - val_mse: 0.0173 - val_rmse: 0.1316\n",
      "Epoch 71/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0154 - mape: 31.7194 - mse: 0.0154 - rmse: 0.1240 - val_loss: 0.0173 - val_mape: 30.9303 - val_mse: 0.0173 - val_rmse: 0.1315\n",
      "Epoch 72/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0152 - mape: 31.3044 - mse: 0.0152 - rmse: 0.1231 - val_loss: 0.0176 - val_mape: 30.0236 - val_mse: 0.0176 - val_rmse: 0.1326\n",
      "Epoch 73/200\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0154 - mape: 31.7207 - mse: 0.0154 - rmse: 0.1241 - val_loss: 0.0175 - val_mape: 29.6898 - val_mse: 0.0175 - val_rmse: 0.1322\n",
      "[MODELO CONGELADO]: 114.38027024269104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 81.74779871946106, 9.04144892810113, 0.790088734271871, 23.12126856291755\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 83.36024293631141, 9.130183072442271, 0.7354684299324914, 23.883406003329263\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 128.09726554929253, 11.318006253280325, 0.7909118540021594, 24.16697702187056\n",
      "PROCESANDO ARCHIVO: Juniperus spp. \n",
      "(17976, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Juniperus spp. _best_models.json\n",
      "(10218, 4, 43) (3900, 4, 43) (3018, 4, 43)\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Conv1D name=conv1d_6, built=True> False\n",
      "<BatchNormalization name=batch_normalization_6, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Conv1D name=conv1d_7, built=True> False\n",
      "<BatchNormalization name=batch_normalization_7, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_6, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_8, built=True> False\n",
      "<BatchNormalization name=batch_normalization_8, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_7, built=True> False\n",
      "<Conv1D name=conv1d_9, built=True> False\n",
      "<BatchNormalization name=batch_normalization_9, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_8, built=True> False\n",
      "<Conv1D name=conv1d_10, built=True> False\n",
      "<BatchNormalization name=batch_normalization_10, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_9, built=True> False\n",
      "<Conv1D name=conv1d_11, built=True> True\n",
      "<BatchNormalization name=batch_normalization_11, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0202 - mape: 39.0258 - mse: 0.0202 - rmse: 0.1421 - val_loss: 0.0200 - val_mape: 35.2529 - val_mse: 0.0199 - val_rmse: 0.1413\n",
      "Epoch 2/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0190 - mape: 38.3144 - mse: 0.0190 - rmse: 0.1378 - val_loss: 0.0198 - val_mape: 35.5508 - val_mse: 0.0196 - val_rmse: 0.1405\n",
      "Epoch 3/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0186 - mape: 37.8044 - mse: 0.0186 - rmse: 0.1363 - val_loss: 0.0197 - val_mape: 35.6937 - val_mse: 0.0196 - val_rmse: 0.1403\n",
      "Epoch 4/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0185 - mape: 36.9906 - mse: 0.0185 - rmse: 0.1358 - val_loss: 0.0195 - val_mape: 35.4986 - val_mse: 0.0194 - val_rmse: 0.1397\n",
      "Epoch 5/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0183 - mape: 35.9084 - mse: 0.0183 - rmse: 0.1353 - val_loss: 0.0195 - val_mape: 35.4689 - val_mse: 0.0194 - val_rmse: 0.1396\n",
      "Epoch 6/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0182 - mape: 35.9513 - mse: 0.0182 - rmse: 0.1347 - val_loss: 0.0194 - val_mape: 35.5026 - val_mse: 0.0193 - val_rmse: 0.1394\n",
      "Epoch 7/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0181 - mape: 37.0032 - mse: 0.0181 - rmse: 0.1345 - val_loss: 0.0194 - val_mape: 35.1928 - val_mse: 0.0193 - val_rmse: 0.1391\n",
      "Epoch 8/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0178 - mape: 36.4703 - mse: 0.0178 - rmse: 0.1335 - val_loss: 0.0194 - val_mape: 34.7828 - val_mse: 0.0193 - val_rmse: 0.1392\n",
      "Epoch 9/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0180 - mape: 36.2201 - mse: 0.0180 - rmse: 0.1341 - val_loss: 0.0193 - val_mape: 35.3281 - val_mse: 0.0192 - val_rmse: 0.1391\n",
      "Epoch 10/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0178 - mape: 35.9480 - mse: 0.0178 - rmse: 0.1336 - val_loss: 0.0193 - val_mape: 35.1341 - val_mse: 0.0192 - val_rmse: 0.1389\n",
      "Epoch 11/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0177 - mape: 35.0475 - mse: 0.0177 - rmse: 0.1329 - val_loss: 0.0193 - val_mape: 34.9547 - val_mse: 0.0192 - val_rmse: 0.1388\n",
      "Epoch 12/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0177 - mape: 35.2671 - mse: 0.0177 - rmse: 0.1331 - val_loss: 0.0192 - val_mape: 34.9672 - val_mse: 0.0191 - val_rmse: 0.1387\n",
      "Epoch 13/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0177 - mape: 35.2047 - mse: 0.0177 - rmse: 0.1331 - val_loss: 0.0193 - val_mape: 35.3159 - val_mse: 0.0192 - val_rmse: 0.1388\n",
      "Epoch 14/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0175 - mape: 35.3467 - mse: 0.0175 - rmse: 0.1323 - val_loss: 0.0192 - val_mape: 34.9663 - val_mse: 0.0191 - val_rmse: 0.1385\n",
      "Epoch 15/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0176 - mape: 35.0545 - mse: 0.0176 - rmse: 0.1326 - val_loss: 0.0192 - val_mape: 34.7907 - val_mse: 0.0191 - val_rmse: 0.1384\n",
      "Epoch 16/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0175 - mape: 34.6265 - mse: 0.0175 - rmse: 0.1321 - val_loss: 0.0192 - val_mape: 34.8261 - val_mse: 0.0191 - val_rmse: 0.1384\n",
      "Epoch 17/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0174 - mape: 35.0460 - mse: 0.0174 - rmse: 0.1318 - val_loss: 0.0192 - val_mape: 35.0260 - val_mse: 0.0191 - val_rmse: 0.1385\n",
      "Epoch 18/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0175 - mape: 34.6448 - mse: 0.0175 - rmse: 0.1321 - val_loss: 0.0192 - val_mape: 34.8550 - val_mse: 0.0191 - val_rmse: 0.1385\n",
      "Epoch 19/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0173 - mape: 34.3735 - mse: 0.0173 - rmse: 0.1314 - val_loss: 0.0191 - val_mape: 34.8808 - val_mse: 0.0190 - val_rmse: 0.1382\n",
      "Epoch 20/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0172 - mape: 35.1081 - mse: 0.0172 - rmse: 0.1312 - val_loss: 0.0191 - val_mape: 34.8255 - val_mse: 0.0190 - val_rmse: 0.1382\n",
      "Epoch 21/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0173 - mape: 34.2438 - mse: 0.0173 - rmse: 0.1314 - val_loss: 0.0191 - val_mape: 34.9343 - val_mse: 0.0190 - val_rmse: 0.1383\n",
      "Epoch 22/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0172 - mape: 34.1226 - mse: 0.0172 - rmse: 0.1313 - val_loss: 0.0191 - val_mape: 35.5912 - val_mse: 0.0190 - val_rmse: 0.1381\n",
      "Epoch 23/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0172 - mape: 33.8907 - mse: 0.0172 - rmse: 0.1312 - val_loss: 0.0191 - val_mape: 34.6302 - val_mse: 0.0190 - val_rmse: 0.1381\n",
      "Epoch 24/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0170 - mape: 33.7341 - mse: 0.0170 - rmse: 0.1304 - val_loss: 0.0190 - val_mape: 35.0451 - val_mse: 0.0190 - val_rmse: 0.1380\n",
      "Epoch 25/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0171 - mape: 34.6648 - mse: 0.0171 - rmse: 0.1309 - val_loss: 0.0190 - val_mape: 34.9007 - val_mse: 0.0189 - val_rmse: 0.1379\n",
      "Epoch 26/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0170 - mape: 34.0686 - mse: 0.0170 - rmse: 0.1304 - val_loss: 0.0190 - val_mape: 34.9279 - val_mse: 0.0190 - val_rmse: 0.1380\n",
      "Epoch 27/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0169 - mape: 33.6915 - mse: 0.0169 - rmse: 0.1301 - val_loss: 0.0190 - val_mape: 35.0679 - val_mse: 0.0189 - val_rmse: 0.1379\n",
      "Epoch 28/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0170 - mape: 33.3297 - mse: 0.0170 - rmse: 0.1305 - val_loss: 0.0190 - val_mape: 34.6127 - val_mse: 0.0189 - val_rmse: 0.1378\n",
      "Epoch 29/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0170 - mape: 34.3349 - mse: 0.0170 - rmse: 0.1305 - val_loss: 0.0190 - val_mape: 34.8483 - val_mse: 0.0189 - val_rmse: 0.1377\n",
      "Epoch 30/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0170 - mape: 34.4429 - mse: 0.0170 - rmse: 0.1302 - val_loss: 0.0190 - val_mape: 34.7471 - val_mse: 0.0189 - val_rmse: 0.1378\n",
      "Epoch 31/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0170 - mape: 33.7643 - mse: 0.0170 - rmse: 0.1304 - val_loss: 0.0189 - val_mape: 34.6055 - val_mse: 0.0189 - val_rmse: 0.1376\n",
      "Epoch 32/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0170 - mape: 34.7145 - mse: 0.0170 - rmse: 0.1303 - val_loss: 0.0190 - val_mape: 35.3008 - val_mse: 0.0189 - val_rmse: 0.1377\n",
      "Epoch 33/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0169 - mape: 34.2356 - mse: 0.0169 - rmse: 0.1298 - val_loss: 0.0190 - val_mape: 34.8663 - val_mse: 0.0189 - val_rmse: 0.1377\n",
      "Epoch 34/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0168 - mape: 33.1620 - mse: 0.0168 - rmse: 0.1297 - val_loss: 0.0189 - val_mape: 34.9151 - val_mse: 0.0189 - val_rmse: 0.1376\n",
      "Epoch 35/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0167 - mape: 33.1555 - mse: 0.0167 - rmse: 0.1292 - val_loss: 0.0189 - val_mape: 34.7836 - val_mse: 0.0189 - val_rmse: 0.1376\n",
      "Epoch 36/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0169 - mape: 34.0357 - mse: 0.0169 - rmse: 0.1298 - val_loss: 0.0189 - val_mape: 35.0719 - val_mse: 0.0188 - val_rmse: 0.1374\n",
      "Epoch 37/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0167 - mape: 33.3732 - mse: 0.0167 - rmse: 0.1290 - val_loss: 0.0189 - val_mape: 35.5139 - val_mse: 0.0188 - val_rmse: 0.1375\n",
      "Epoch 38/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0168 - mape: 32.9692 - mse: 0.0168 - rmse: 0.1294 - val_loss: 0.0189 - val_mape: 35.4097 - val_mse: 0.0188 - val_rmse: 0.1373\n",
      "Epoch 39/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0167 - mape: 33.3860 - mse: 0.0167 - rmse: 0.1292 - val_loss: 0.0190 - val_mape: 35.0192 - val_mse: 0.0189 - val_rmse: 0.1378\n",
      "Epoch 40/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0168 - mape: 32.6154 - mse: 0.0168 - rmse: 0.1294 - val_loss: 0.0189 - val_mape: 35.2300 - val_mse: 0.0188 - val_rmse: 0.1373\n",
      "Epoch 41/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0168 - mape: 33.2904 - mse: 0.0168 - rmse: 0.1294 - val_loss: 0.0189 - val_mape: 34.6481 - val_mse: 0.0188 - val_rmse: 0.1374\n",
      "Epoch 42/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0170 - mape: 34.0780 - mse: 0.0170 - rmse: 0.1303 - val_loss: 0.0189 - val_mape: 35.1610 - val_mse: 0.0188 - val_rmse: 0.1374\n",
      "Epoch 43/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0167 - mape: 33.2117 - mse: 0.0167 - rmse: 0.1293 - val_loss: 0.0188 - val_mape: 35.0326 - val_mse: 0.0187 - val_rmse: 0.1371\n",
      "Epoch 44/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0168 - mape: 32.9371 - mse: 0.0168 - rmse: 0.1295 - val_loss: 0.0188 - val_mape: 35.0763 - val_mse: 0.0187 - val_rmse: 0.1370\n",
      "Epoch 45/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0166 - mape: 34.1048 - mse: 0.0166 - rmse: 0.1289 - val_loss: 0.0188 - val_mape: 34.9875 - val_mse: 0.0187 - val_rmse: 0.1371\n",
      "Epoch 46/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0165 - mape: 32.6751 - mse: 0.0165 - rmse: 0.1286 - val_loss: 0.0188 - val_mape: 34.6677 - val_mse: 0.0187 - val_rmse: 0.1371\n",
      "Epoch 47/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0165 - mape: 32.5603 - mse: 0.0165 - rmse: 0.1285 - val_loss: 0.0188 - val_mape: 34.7597 - val_mse: 0.0187 - val_rmse: 0.1371\n",
      "Epoch 48/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0165 - mape: 32.4014 - mse: 0.0165 - rmse: 0.1284 - val_loss: 0.0189 - val_mape: 35.1311 - val_mse: 0.0188 - val_rmse: 0.1373\n",
      "Epoch 49/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0166 - mape: 33.7412 - mse: 0.0166 - rmse: 0.1286 - val_loss: 0.0188 - val_mape: 35.1130 - val_mse: 0.0188 - val_rmse: 0.1372\n",
      "Epoch 50/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0164 - mape: 32.5009 - mse: 0.0164 - rmse: 0.1282 - val_loss: 0.0188 - val_mape: 34.7100 - val_mse: 0.0187 - val_rmse: 0.1371\n",
      "Epoch 51/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0164 - mape: 32.1224 - mse: 0.0164 - rmse: 0.1281 - val_loss: 0.0188 - val_mape: 34.7828 - val_mse: 0.0187 - val_rmse: 0.1371\n",
      "Epoch 52/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0165 - mape: 33.2669 - mse: 0.0165 - rmse: 0.1285 - val_loss: 0.0188 - val_mape: 34.6290 - val_mse: 0.0187 - val_rmse: 0.1370\n",
      "Epoch 53/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0164 - mape: 33.5702 - mse: 0.0164 - rmse: 0.1280 - val_loss: 0.0188 - val_mape: 35.4807 - val_mse: 0.0187 - val_rmse: 0.1371\n",
      "Epoch 54/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0165 - mape: 34.0669 - mse: 0.0165 - rmse: 0.1285 - val_loss: 0.0188 - val_mape: 34.5165 - val_mse: 0.0187 - val_rmse: 0.1371\n",
      "Epoch 55/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0165 - mape: 32.8978 - mse: 0.0165 - rmse: 0.1282 - val_loss: 0.0187 - val_mape: 35.0522 - val_mse: 0.0187 - val_rmse: 0.1369\n",
      "Epoch 56/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0163 - mape: 32.8598 - mse: 0.0163 - rmse: 0.1276 - val_loss: 0.0187 - val_mape: 34.4734 - val_mse: 0.0186 - val_rmse: 0.1368\n",
      "Epoch 57/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0165 - mape: 33.1713 - mse: 0.0165 - rmse: 0.1282 - val_loss: 0.0188 - val_mape: 35.0652 - val_mse: 0.0187 - val_rmse: 0.1371\n",
      "Epoch 58/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0165 - mape: 33.5031 - mse: 0.0165 - rmse: 0.1285 - val_loss: 0.0187 - val_mape: 34.8518 - val_mse: 0.0186 - val_rmse: 0.1368\n",
      "Epoch 59/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0164 - mape: 33.1093 - mse: 0.0164 - rmse: 0.1280 - val_loss: 0.0188 - val_mape: 35.3464 - val_mse: 0.0187 - val_rmse: 0.1372\n",
      "Epoch 60/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0163 - mape: 32.2518 - mse: 0.0163 - rmse: 0.1275 - val_loss: 0.0188 - val_mape: 35.5701 - val_mse: 0.0188 - val_rmse: 0.1372\n",
      "Epoch 61/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0163 - mape: 32.7300 - mse: 0.0163 - rmse: 0.1278 - val_loss: 0.0187 - val_mape: 35.1382 - val_mse: 0.0187 - val_rmse: 0.1369\n",
      "Epoch 62/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0161 - mape: 32.4762 - mse: 0.0161 - rmse: 0.1270 - val_loss: 0.0188 - val_mape: 34.5927 - val_mse: 0.0187 - val_rmse: 0.1369\n",
      "Epoch 63/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0164 - mape: 32.9297 - mse: 0.0164 - rmse: 0.1279 - val_loss: 0.0187 - val_mape: 35.2245 - val_mse: 0.0187 - val_rmse: 0.1369\n",
      "Epoch 64/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0162 - mape: 32.0086 - mse: 0.0162 - rmse: 0.1272 - val_loss: 0.0187 - val_mape: 34.7212 - val_mse: 0.0186 - val_rmse: 0.1368\n",
      "Epoch 65/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0164 - mape: 33.3693 - mse: 0.0164 - rmse: 0.1280 - val_loss: 0.0187 - val_mape: 35.5891 - val_mse: 0.0187 - val_rmse: 0.1368\n",
      "Epoch 66/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0164 - mape: 33.1642 - mse: 0.0164 - rmse: 0.1279 - val_loss: 0.0187 - val_mape: 35.0593 - val_mse: 0.0187 - val_rmse: 0.1369\n",
      "Epoch 67/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0162 - mape: 33.1399 - mse: 0.0162 - rmse: 0.1271 - val_loss: 0.0187 - val_mape: 34.8057 - val_mse: 0.0186 - val_rmse: 0.1368\n",
      "Epoch 68/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0162 - mape: 32.0755 - mse: 0.0162 - rmse: 0.1272 - val_loss: 0.0188 - val_mape: 35.0416 - val_mse: 0.0187 - val_rmse: 0.1370\n",
      "Epoch 69/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0162 - mape: 33.7011 - mse: 0.0162 - rmse: 0.1274 - val_loss: 0.0188 - val_mape: 35.0274 - val_mse: 0.0187 - val_rmse: 0.1370\n",
      "Epoch 70/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0161 - mape: 32.6142 - mse: 0.0161 - rmse: 0.1269 - val_loss: 0.0187 - val_mape: 35.1228 - val_mse: 0.0187 - val_rmse: 0.1369\n",
      "Epoch 71/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0162 - mape: 32.2562 - mse: 0.0162 - rmse: 0.1273 - val_loss: 0.0188 - val_mape: 34.8850 - val_mse: 0.0187 - val_rmse: 0.1372\n",
      "Epoch 72/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0161 - mape: 32.6342 - mse: 0.0161 - rmse: 0.1270 - val_loss: 0.0188 - val_mape: 35.2754 - val_mse: 0.0187 - val_rmse: 0.1370\n",
      "Epoch 73/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0161 - mape: 31.7943 - mse: 0.0161 - rmse: 0.1269 - val_loss: 0.0188 - val_mape: 35.2924 - val_mse: 0.0187 - val_rmse: 0.1369\n",
      "Epoch 74/200\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0161 - mape: 32.3147 - mse: 0.0161 - rmse: 0.1268 - val_loss: 0.0188 - val_mape: 35.3589 - val_mse: 0.0188 - val_rmse: 0.1372\n",
      "[MODELO CONGELADO]: 96.38913655281067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 6.5107586137910145, 2.551618822197198, 0.8938139878995275, 19.662199522901197\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 2.652979798057617, 1.6287970401672571, 0.8651443614282859, 22.20667847560513\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 8.87198032206718, 2.978586967349985, 0.9150976790953419, 20.109449600751788\n",
      "PROCESANDO ARCHIVO: Juniperus recurva\n",
      "(5316, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Juniperus recurva_best_models.json\n",
      "(3188, 4, 43) (1082, 4, 43) (810, 4, 43)\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Conv1D name=conv1d_6, built=True> False\n",
      "<BatchNormalization name=batch_normalization_6, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Conv1D name=conv1d_7, built=True> False\n",
      "<BatchNormalization name=batch_normalization_7, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_6, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_8, built=True> False\n",
      "<BatchNormalization name=batch_normalization_8, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_7, built=True> False\n",
      "<Conv1D name=conv1d_9, built=True> False\n",
      "<BatchNormalization name=batch_normalization_9, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_8, built=True> False\n",
      "<Conv1D name=conv1d_10, built=True> False\n",
      "<BatchNormalization name=batch_normalization_10, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_9, built=True> False\n",
      "<Conv1D name=conv1d_11, built=True> True\n",
      "<BatchNormalization name=batch_normalization_11, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0193 - mape: 28.9405 - mse: 0.0193 - rmse: 0.1387 - val_loss: 0.0197 - val_mape: 26.1853 - val_mse: 0.0195 - val_rmse: 0.1403\n",
      "Epoch 2/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0164 - mape: 27.9158 - mse: 0.0164 - rmse: 0.1282 - val_loss: 0.0187 - val_mape: 26.7277 - val_mse: 0.0186 - val_rmse: 0.1369\n",
      "Epoch 3/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0160 - mape: 27.6161 - mse: 0.0160 - rmse: 0.1266 - val_loss: 0.0184 - val_mape: 26.9034 - val_mse: 0.0183 - val_rmse: 0.1357\n",
      "Epoch 4/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0158 - mape: 27.6023 - mse: 0.0158 - rmse: 0.1255 - val_loss: 0.0182 - val_mape: 26.9371 - val_mse: 0.0180 - val_rmse: 0.1347\n",
      "Epoch 5/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0156 - mape: 27.2976 - mse: 0.0156 - rmse: 0.1247 - val_loss: 0.0181 - val_mape: 26.7312 - val_mse: 0.0179 - val_rmse: 0.1344\n",
      "Epoch 6/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0155 - mape: 27.3537 - mse: 0.0155 - rmse: 0.1246 - val_loss: 0.0179 - val_mape: 26.5987 - val_mse: 0.0178 - val_rmse: 0.1339\n",
      "Epoch 7/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0151 - mape: 26.6559 - mse: 0.0151 - rmse: 0.1227 - val_loss: 0.0178 - val_mape: 26.5297 - val_mse: 0.0177 - val_rmse: 0.1335\n",
      "Epoch 8/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0151 - mape: 26.7643 - mse: 0.0151 - rmse: 0.1229 - val_loss: 0.0178 - val_mape: 26.3389 - val_mse: 0.0176 - val_rmse: 0.1333\n",
      "Epoch 9/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0151 - mape: 26.5858 - mse: 0.0151 - rmse: 0.1228 - val_loss: 0.0177 - val_mape: 26.2441 - val_mse: 0.0176 - val_rmse: 0.1332\n",
      "Epoch 10/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0147 - mape: 26.2646 - mse: 0.0147 - rmse: 0.1211 - val_loss: 0.0176 - val_mape: 26.1660 - val_mse: 0.0175 - val_rmse: 0.1327\n",
      "Epoch 11/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0148 - mape: 26.5794 - mse: 0.0148 - rmse: 0.1218 - val_loss: 0.0176 - val_mape: 25.9744 - val_mse: 0.0174 - val_rmse: 0.1325\n",
      "Epoch 12/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0145 - mape: 25.9949 - mse: 0.0145 - rmse: 0.1203 - val_loss: 0.0175 - val_mape: 25.8334 - val_mse: 0.0174 - val_rmse: 0.1323\n",
      "Epoch 13/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0145 - mape: 26.2169 - mse: 0.0145 - rmse: 0.1203 - val_loss: 0.0175 - val_mape: 25.8012 - val_mse: 0.0173 - val_rmse: 0.1322\n",
      "Epoch 14/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0143 - mape: 25.8652 - mse: 0.0143 - rmse: 0.1196 - val_loss: 0.0174 - val_mape: 25.8881 - val_mse: 0.0173 - val_rmse: 0.1319\n",
      "Epoch 15/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0142 - mape: 25.8496 - mse: 0.0142 - rmse: 0.1191 - val_loss: 0.0174 - val_mape: 25.7918 - val_mse: 0.0173 - val_rmse: 0.1320\n",
      "Epoch 16/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0141 - mape: 26.0361 - mse: 0.0141 - rmse: 0.1189 - val_loss: 0.0173 - val_mape: 25.7457 - val_mse: 0.0172 - val_rmse: 0.1317\n",
      "Epoch 17/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0141 - mape: 25.5207 - mse: 0.0141 - rmse: 0.1188 - val_loss: 0.0173 - val_mape: 25.6475 - val_mse: 0.0172 - val_rmse: 0.1315\n",
      "Epoch 18/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0141 - mape: 25.4988 - mse: 0.0141 - rmse: 0.1186 - val_loss: 0.0173 - val_mape: 25.7456 - val_mse: 0.0171 - val_rmse: 0.1314\n",
      "Epoch 19/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0143 - mape: 25.8987 - mse: 0.0143 - rmse: 0.1196 - val_loss: 0.0173 - val_mape: 25.5643 - val_mse: 0.0171 - val_rmse: 0.1314\n",
      "Epoch 20/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0140 - mape: 25.4738 - mse: 0.0140 - rmse: 0.1183 - val_loss: 0.0173 - val_mape: 25.7525 - val_mse: 0.0172 - val_rmse: 0.1315\n",
      "Epoch 21/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0139 - mape: 25.4083 - mse: 0.0139 - rmse: 0.1181 - val_loss: 0.0172 - val_mape: 25.4019 - val_mse: 0.0171 - val_rmse: 0.1312\n",
      "Epoch 22/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0134 - mape: 25.0804 - mse: 0.0134 - rmse: 0.1157 - val_loss: 0.0172 - val_mape: 25.5508 - val_mse: 0.0170 - val_rmse: 0.1310\n",
      "Epoch 23/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0136 - mape: 24.9641 - mse: 0.0136 - rmse: 0.1166 - val_loss: 0.0171 - val_mape: 25.5860 - val_mse: 0.0170 - val_rmse: 0.1308\n",
      "Epoch 24/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0134 - mape: 24.9589 - mse: 0.0134 - rmse: 0.1159 - val_loss: 0.0170 - val_mape: 25.5117 - val_mse: 0.0169 - val_rmse: 0.1305\n",
      "Epoch 25/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0138 - mape: 24.9414 - mse: 0.0138 - rmse: 0.1175 - val_loss: 0.0172 - val_mape: 25.6259 - val_mse: 0.0170 - val_rmse: 0.1310\n",
      "Epoch 26/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0134 - mape: 24.6675 - mse: 0.0134 - rmse: 0.1159 - val_loss: 0.0171 - val_mape: 25.5656 - val_mse: 0.0170 - val_rmse: 0.1306\n",
      "Epoch 27/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0135 - mape: 25.0487 - mse: 0.0135 - rmse: 0.1161 - val_loss: 0.0172 - val_mape: 25.5752 - val_mse: 0.0170 - val_rmse: 0.1310\n",
      "Epoch 28/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0132 - mape: 24.7617 - mse: 0.0132 - rmse: 0.1151 - val_loss: 0.0171 - val_mape: 25.4590 - val_mse: 0.0170 - val_rmse: 0.1309\n",
      "Epoch 29/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0135 - mape: 24.9461 - mse: 0.0135 - rmse: 0.1163 - val_loss: 0.0172 - val_mape: 25.4777 - val_mse: 0.0170 - val_rmse: 0.1311\n",
      "Epoch 30/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0134 - mape: 24.7413 - mse: 0.0134 - rmse: 0.1159 - val_loss: 0.0172 - val_mape: 25.7408 - val_mse: 0.0170 - val_rmse: 0.1310\n",
      "Epoch 31/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0132 - mape: 24.7223 - mse: 0.0132 - rmse: 0.1150 - val_loss: 0.0171 - val_mape: 25.5750 - val_mse: 0.0170 - val_rmse: 0.1307\n",
      "Epoch 32/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0132 - mape: 24.6970 - mse: 0.0132 - rmse: 0.1151 - val_loss: 0.0172 - val_mape: 25.4354 - val_mse: 0.0170 - val_rmse: 0.1310\n",
      "Epoch 33/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0132 - mape: 24.6228 - mse: 0.0132 - rmse: 0.1147 - val_loss: 0.0170 - val_mape: 25.4286 - val_mse: 0.0169 - val_rmse: 0.1305\n",
      "Epoch 34/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0133 - mape: 24.6018 - mse: 0.0133 - rmse: 0.1152 - val_loss: 0.0169 - val_mape: 25.5950 - val_mse: 0.0169 - val_rmse: 0.1302\n",
      "Epoch 35/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0128 - mape: 24.2808 - mse: 0.0128 - rmse: 0.1131 - val_loss: 0.0170 - val_mape: 25.3713 - val_mse: 0.0169 - val_rmse: 0.1303\n",
      "Epoch 36/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0130 - mape: 24.6119 - mse: 0.0130 - rmse: 0.1141 - val_loss: 0.0170 - val_mape: 25.5494 - val_mse: 0.0169 - val_rmse: 0.1304\n",
      "Epoch 37/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0130 - mape: 24.2662 - mse: 0.0130 - rmse: 0.1141 - val_loss: 0.0170 - val_mape: 25.4298 - val_mse: 0.0169 - val_rmse: 0.1304\n",
      "Epoch 38/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0129 - mape: 24.4164 - mse: 0.0129 - rmse: 0.1137 - val_loss: 0.0169 - val_mape: 25.2941 - val_mse: 0.0168 - val_rmse: 0.1300\n",
      "Epoch 39/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0132 - mape: 24.6996 - mse: 0.0132 - rmse: 0.1150 - val_loss: 0.0170 - val_mape: 25.2963 - val_mse: 0.0169 - val_rmse: 0.1304\n",
      "Epoch 40/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0131 - mape: 24.9573 - mse: 0.0131 - rmse: 0.1146 - val_loss: 0.0169 - val_mape: 25.3944 - val_mse: 0.0168 - val_rmse: 0.1299\n",
      "Epoch 41/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0128 - mape: 24.2221 - mse: 0.0128 - rmse: 0.1130 - val_loss: 0.0169 - val_mape: 25.0563 - val_mse: 0.0168 - val_rmse: 0.1301\n",
      "Epoch 42/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0129 - mape: 24.4137 - mse: 0.0129 - rmse: 0.1136 - val_loss: 0.0170 - val_mape: 25.2933 - val_mse: 0.0169 - val_rmse: 0.1306\n",
      "Epoch 43/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0129 - mape: 24.2417 - mse: 0.0129 - rmse: 0.1136 - val_loss: 0.0169 - val_mape: 25.4718 - val_mse: 0.0169 - val_rmse: 0.1302\n",
      "Epoch 44/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0128 - mape: 24.2343 - mse: 0.0128 - rmse: 0.1131 - val_loss: 0.0169 - val_mape: 25.2491 - val_mse: 0.0168 - val_rmse: 0.1300\n",
      "Epoch 45/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0128 - mape: 24.0661 - mse: 0.0128 - rmse: 0.1129 - val_loss: 0.0170 - val_mape: 25.3343 - val_mse: 0.0169 - val_rmse: 0.1302\n",
      "Epoch 46/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0128 - mape: 24.2623 - mse: 0.0128 - rmse: 0.1131 - val_loss: 0.0169 - val_mape: 25.3378 - val_mse: 0.0168 - val_rmse: 0.1299\n",
      "Epoch 47/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0130 - mape: 24.6161 - mse: 0.0130 - rmse: 0.1139 - val_loss: 0.0169 - val_mape: 25.2820 - val_mse: 0.0168 - val_rmse: 0.1299\n",
      "Epoch 48/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0131 - mape: 24.4654 - mse: 0.0131 - rmse: 0.1145 - val_loss: 0.0169 - val_mape: 25.2705 - val_mse: 0.0168 - val_rmse: 0.1301\n",
      "Epoch 49/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0127 - mape: 23.7212 - mse: 0.0127 - rmse: 0.1127 - val_loss: 0.0168 - val_mape: 25.2605 - val_mse: 0.0167 - val_rmse: 0.1295\n",
      "Epoch 50/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0129 - mape: 24.1008 - mse: 0.0129 - rmse: 0.1136 - val_loss: 0.0167 - val_mape: 25.1698 - val_mse: 0.0167 - val_rmse: 0.1294\n",
      "Epoch 51/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0123 - mape: 23.5416 - mse: 0.0123 - rmse: 0.1108 - val_loss: 0.0168 - val_mape: 24.8492 - val_mse: 0.0167 - val_rmse: 0.1297\n",
      "Epoch 52/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0124 - mape: 23.9029 - mse: 0.0124 - rmse: 0.1113 - val_loss: 0.0168 - val_mape: 25.0187 - val_mse: 0.0167 - val_rmse: 0.1296\n",
      "Epoch 53/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0126 - mape: 23.8381 - mse: 0.0126 - rmse: 0.1124 - val_loss: 0.0168 - val_mape: 25.1974 - val_mse: 0.0167 - val_rmse: 0.1295\n",
      "Epoch 54/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0125 - mape: 23.9970 - mse: 0.0125 - rmse: 0.1119 - val_loss: 0.0167 - val_mape: 25.0933 - val_mse: 0.0166 - val_rmse: 0.1293\n",
      "Epoch 55/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0125 - mape: 23.9504 - mse: 0.0125 - rmse: 0.1117 - val_loss: 0.0167 - val_mape: 25.0144 - val_mse: 0.0166 - val_rmse: 0.1291\n",
      "Epoch 56/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0127 - mape: 23.8719 - mse: 0.0127 - rmse: 0.1128 - val_loss: 0.0167 - val_mape: 25.0740 - val_mse: 0.0166 - val_rmse: 0.1292\n",
      "Epoch 57/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0128 - mape: 23.8881 - mse: 0.0128 - rmse: 0.1131 - val_loss: 0.0166 - val_mape: 25.0630 - val_mse: 0.0166 - val_rmse: 0.1289\n",
      "Epoch 58/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0122 - mape: 23.4851 - mse: 0.0122 - rmse: 0.1104 - val_loss: 0.0166 - val_mape: 24.9203 - val_mse: 0.0165 - val_rmse: 0.1289\n",
      "Epoch 59/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0127 - mape: 23.8704 - mse: 0.0127 - rmse: 0.1125 - val_loss: 0.0165 - val_mape: 24.7218 - val_mse: 0.0164 - val_rmse: 0.1286\n",
      "Epoch 60/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0122 - mape: 23.4350 - mse: 0.0122 - rmse: 0.1104 - val_loss: 0.0165 - val_mape: 24.6136 - val_mse: 0.0164 - val_rmse: 0.1285\n",
      "Epoch 61/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0122 - mape: 23.4962 - mse: 0.0122 - rmse: 0.1103 - val_loss: 0.0165 - val_mape: 24.6919 - val_mse: 0.0164 - val_rmse: 0.1285\n",
      "Epoch 62/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0122 - mape: 23.3451 - mse: 0.0122 - rmse: 0.1103 - val_loss: 0.0164 - val_mape: 25.0400 - val_mse: 0.0163 - val_rmse: 0.1279\n",
      "Epoch 63/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0120 - mape: 23.0475 - mse: 0.0120 - rmse: 0.1096 - val_loss: 0.0164 - val_mape: 25.0674 - val_mse: 0.0163 - val_rmse: 0.1279\n",
      "Epoch 64/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0123 - mape: 23.4935 - mse: 0.0123 - rmse: 0.1109 - val_loss: 0.0166 - val_mape: 24.6101 - val_mse: 0.0165 - val_rmse: 0.1289\n",
      "Epoch 65/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0119 - mape: 23.0469 - mse: 0.0119 - rmse: 0.1089 - val_loss: 0.0165 - val_mape: 24.9532 - val_mse: 0.0164 - val_rmse: 0.1284\n",
      "Epoch 66/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0122 - mape: 23.3141 - mse: 0.0122 - rmse: 0.1105 - val_loss: 0.0165 - val_mape: 24.7745 - val_mse: 0.0165 - val_rmse: 0.1286\n",
      "Epoch 67/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0122 - mape: 23.7601 - mse: 0.0122 - rmse: 0.1104 - val_loss: 0.0165 - val_mape: 24.6561 - val_mse: 0.0164 - val_rmse: 0.1283\n",
      "Epoch 68/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0127 - mape: 23.9146 - mse: 0.0127 - rmse: 0.1126 - val_loss: 0.0166 - val_mape: 24.9355 - val_mse: 0.0165 - val_rmse: 0.1288\n",
      "Epoch 69/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0123 - mape: 23.6049 - mse: 0.0123 - rmse: 0.1110 - val_loss: 0.0164 - val_mape: 25.0406 - val_mse: 0.0163 - val_rmse: 0.1280\n",
      "Epoch 70/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0121 - mape: 23.2467 - mse: 0.0121 - rmse: 0.1100 - val_loss: 0.0164 - val_mape: 24.8767 - val_mse: 0.0163 - val_rmse: 0.1281\n",
      "Epoch 71/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0124 - mape: 23.8022 - mse: 0.0124 - rmse: 0.1114 - val_loss: 0.0165 - val_mape: 24.6465 - val_mse: 0.0164 - val_rmse: 0.1284\n",
      "Epoch 72/200\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0118 - mape: 23.1571 - mse: 0.0118 - rmse: 0.1087 - val_loss: 0.0165 - val_mape: 24.5002 - val_mse: 0.0165 - val_rmse: 0.1285\n",
      "[MODELO CONGELADO]: 23.635557651519775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 2.393117487845362, 1.5469704224209855, 0.9049063598826056, 16.97824121959352\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 3.0678905258533673, 1.7515394731074054, 0.88860054947882, 19.489910632805902\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 1.051666171661052, 1.025507762847777, 0.8881950040610317, 15.258030314491823\n",
      "PROCESANDO ARCHIVO: Juniperus spp. L.\n",
      "(7264, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Juniperus spp. L._best_models.json\n",
      "(4216, 4, 43) (1524, 4, 43) (1188, 4, 43)\n",
      "<Conv1D name=conv1d, built=True> False\n",
      "<BatchNormalization name=batch_normalization, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d, built=True> False\n",
      "<Conv1D name=conv1d_1, built=True> False\n",
      "<BatchNormalization name=batch_normalization_1, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_1, built=True> False\n",
      "<Dropout name=dropout, built=True> True\n",
      "<Conv1D name=conv1d_2, built=True> False\n",
      "<BatchNormalization name=batch_normalization_2, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_2, built=True> False\n",
      "<Conv1D name=conv1d_3, built=True> False\n",
      "<BatchNormalization name=batch_normalization_3, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_3, built=True> False\n",
      "<Conv1D name=conv1d_4, built=True> True\n",
      "<BatchNormalization name=batch_normalization_4, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d, built=True> True\n",
      "<Dense name=dense, built=True> True\n",
      "<Dense name=dense_1, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0250 - mape: 77.2335 - mse: 0.0250 - rmse: 0.1580 - val_loss: 0.0243 - val_mape: 49.1166 - val_mse: 0.0242 - val_rmse: 0.1558\n",
      "Epoch 2/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0240 - mape: 73.8043 - mse: 0.0240 - rmse: 0.1548 - val_loss: 0.0241 - val_mape: 49.4154 - val_mse: 0.0240 - val_rmse: 0.1552\n",
      "Epoch 3/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0237 - mape: 71.6138 - mse: 0.0237 - rmse: 0.1540 - val_loss: 0.0240 - val_mape: 49.1243 - val_mse: 0.0239 - val_rmse: 0.1549\n",
      "Epoch 4/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0238 - mape: 65.5708 - mse: 0.0238 - rmse: 0.1541 - val_loss: 0.0239 - val_mape: 49.1618 - val_mse: 0.0238 - val_rmse: 0.1547\n",
      "Epoch 5/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0234 - mape: 58.5836 - mse: 0.0234 - rmse: 0.1528 - val_loss: 0.0240 - val_mape: 49.5393 - val_mse: 0.0239 - val_rmse: 0.1548\n",
      "Epoch 6/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0236 - mape: 69.6256 - mse: 0.0236 - rmse: 0.1537 - val_loss: 0.0238 - val_mape: 49.0673 - val_mse: 0.0237 - val_rmse: 0.1544\n",
      "Epoch 7/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0235 - mape: 64.3962 - mse: 0.0235 - rmse: 0.1533 - val_loss: 0.0238 - val_mape: 49.3319 - val_mse: 0.0237 - val_rmse: 0.1543\n",
      "Epoch 8/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0233 - mape: 64.1048 - mse: 0.0233 - rmse: 0.1525 - val_loss: 0.0237 - val_mape: 48.5415 - val_mse: 0.0236 - val_rmse: 0.1539\n",
      "Epoch 9/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0234 - mape: 63.8430 - mse: 0.0234 - rmse: 0.1531 - val_loss: 0.0237 - val_mape: 48.7926 - val_mse: 0.0236 - val_rmse: 0.1540\n",
      "Epoch 10/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0237 - mape: 64.1191 - mse: 0.0237 - rmse: 0.1538 - val_loss: 0.0238 - val_mape: 49.6301 - val_mse: 0.0237 - val_rmse: 0.1543\n",
      "Epoch 11/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0232 - mape: 68.8555 - mse: 0.0232 - rmse: 0.1523 - val_loss: 0.0238 - val_mape: 49.2638 - val_mse: 0.0237 - val_rmse: 0.1543\n",
      "Epoch 12/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0233 - mape: 68.4110 - mse: 0.0233 - rmse: 0.1526 - val_loss: 0.0237 - val_mape: 48.9578 - val_mse: 0.0236 - val_rmse: 0.1539\n",
      "Epoch 13/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0233 - mape: 59.4998 - mse: 0.0233 - rmse: 0.1526 - val_loss: 0.0236 - val_mape: 48.9407 - val_mse: 0.0236 - val_rmse: 0.1538\n",
      "Epoch 14/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0233 - mape: 60.0030 - mse: 0.0233 - rmse: 0.1527 - val_loss: 0.0237 - val_mape: 49.2885 - val_mse: 0.0236 - val_rmse: 0.1540\n",
      "Epoch 15/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0230 - mape: 63.1035 - mse: 0.0230 - rmse: 0.1518 - val_loss: 0.0236 - val_mape: 48.9155 - val_mse: 0.0236 - val_rmse: 0.1538\n",
      "Epoch 16/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0235 - mape: 61.2138 - mse: 0.0235 - rmse: 0.1531 - val_loss: 0.0237 - val_mape: 49.3579 - val_mse: 0.0236 - val_rmse: 0.1539\n",
      "Epoch 17/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0231 - mape: 57.3946 - mse: 0.0231 - rmse: 0.1518 - val_loss: 0.0236 - val_mape: 48.7859 - val_mse: 0.0235 - val_rmse: 0.1537\n",
      "Epoch 18/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0233 - mape: 58.0335 - mse: 0.0233 - rmse: 0.1527 - val_loss: 0.0236 - val_mape: 48.7878 - val_mse: 0.0235 - val_rmse: 0.1536\n",
      "Epoch 19/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0231 - mape: 55.1401 - mse: 0.0231 - rmse: 0.1521 - val_loss: 0.0236 - val_mape: 48.6751 - val_mse: 0.0235 - val_rmse: 0.1535\n",
      "Epoch 20/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0231 - mape: 63.0646 - mse: 0.0231 - rmse: 0.1521 - val_loss: 0.0235 - val_mape: 48.7057 - val_mse: 0.0234 - val_rmse: 0.1534\n",
      "Epoch 21/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0229 - mape: 58.5240 - mse: 0.0229 - rmse: 0.1514 - val_loss: 0.0235 - val_mape: 48.4094 - val_mse: 0.0234 - val_rmse: 0.1532\n",
      "Epoch 22/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0228 - mape: 64.8016 - mse: 0.0228 - rmse: 0.1511 - val_loss: 0.0236 - val_mape: 49.1254 - val_mse: 0.0235 - val_rmse: 0.1535\n",
      "Epoch 23/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0228 - mape: 54.9645 - mse: 0.0228 - rmse: 0.1511 - val_loss: 0.0237 - val_mape: 49.5075 - val_mse: 0.0236 - val_rmse: 0.1538\n",
      "Epoch 24/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0228 - mape: 58.8286 - mse: 0.0228 - rmse: 0.1510 - val_loss: 0.0236 - val_mape: 48.5854 - val_mse: 0.0235 - val_rmse: 0.1535\n",
      "Epoch 25/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0232 - mape: 67.4171 - mse: 0.0232 - rmse: 0.1522 - val_loss: 0.0235 - val_mape: 48.4825 - val_mse: 0.0234 - val_rmse: 0.1533\n",
      "Epoch 26/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0233 - mape: 59.1699 - mse: 0.0233 - rmse: 0.1525 - val_loss: 0.0235 - val_mape: 48.9495 - val_mse: 0.0234 - val_rmse: 0.1534\n",
      "Epoch 27/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0230 - mape: 65.4502 - mse: 0.0230 - rmse: 0.1516 - val_loss: 0.0235 - val_mape: 48.9678 - val_mse: 0.0234 - val_rmse: 0.1534\n",
      "Epoch 28/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0231 - mape: 57.7309 - mse: 0.0231 - rmse: 0.1521 - val_loss: 0.0235 - val_mape: 49.0451 - val_mse: 0.0234 - val_rmse: 0.1532\n",
      "Epoch 29/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0230 - mape: 64.7254 - mse: 0.0230 - rmse: 0.1517 - val_loss: 0.0235 - val_mape: 48.8365 - val_mse: 0.0234 - val_rmse: 0.1532\n",
      "Epoch 30/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0232 - mape: 59.8502 - mse: 0.0232 - rmse: 0.1522 - val_loss: 0.0235 - val_mape: 48.8523 - val_mse: 0.0234 - val_rmse: 0.1533\n",
      "Epoch 31/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0230 - mape: 59.2063 - mse: 0.0230 - rmse: 0.1516 - val_loss: 0.0235 - val_mape: 48.9133 - val_mse: 0.0234 - val_rmse: 0.1532\n",
      "Epoch 32/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0231 - mape: 56.8474 - mse: 0.0231 - rmse: 0.1520 - val_loss: 0.0235 - val_mape: 49.4770 - val_mse: 0.0234 - val_rmse: 0.1534\n",
      "Epoch 33/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0232 - mape: 62.6634 - mse: 0.0232 - rmse: 0.1521 - val_loss: 0.0234 - val_mape: 48.2708 - val_mse: 0.0233 - val_rmse: 0.1528\n",
      "Epoch 34/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0227 - mape: 58.7148 - mse: 0.0227 - rmse: 0.1508 - val_loss: 0.0233 - val_mape: 48.1515 - val_mse: 0.0232 - val_rmse: 0.1527\n",
      "Epoch 35/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0231 - mape: 68.6540 - mse: 0.0231 - rmse: 0.1520 - val_loss: 0.0234 - val_mape: 49.0756 - val_mse: 0.0233 - val_rmse: 0.1530\n",
      "Epoch 36/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0228 - mape: 66.7419 - mse: 0.0228 - rmse: 0.1510 - val_loss: 0.0233 - val_mape: 48.4110 - val_mse: 0.0232 - val_rmse: 0.1527\n",
      "Epoch 37/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0226 - mape: 71.8114 - mse: 0.0226 - rmse: 0.1504 - val_loss: 0.0234 - val_mape: 49.0520 - val_mse: 0.0233 - val_rmse: 0.1530\n",
      "Epoch 38/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0228 - mape: 58.5670 - mse: 0.0228 - rmse: 0.1510 - val_loss: 0.0235 - val_mape: 49.4800 - val_mse: 0.0234 - val_rmse: 0.1533\n",
      "Epoch 39/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0228 - mape: 59.0262 - mse: 0.0228 - rmse: 0.1508 - val_loss: 0.0234 - val_mape: 48.5837 - val_mse: 0.0233 - val_rmse: 0.1529\n",
      "Epoch 40/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0228 - mape: 63.3380 - mse: 0.0228 - rmse: 0.1508 - val_loss: 0.0233 - val_mape: 48.1811 - val_mse: 0.0232 - val_rmse: 0.1526\n",
      "Epoch 41/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0229 - mape: 53.2128 - mse: 0.0229 - rmse: 0.1513 - val_loss: 0.0234 - val_mape: 48.8913 - val_mse: 0.0233 - val_rmse: 0.1529\n",
      "Epoch 42/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0224 - mape: 61.8937 - mse: 0.0224 - rmse: 0.1497 - val_loss: 0.0232 - val_mape: 48.1018 - val_mse: 0.0231 - val_rmse: 0.1525\n",
      "Epoch 43/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0226 - mape: 54.9223 - mse: 0.0226 - rmse: 0.1503 - val_loss: 0.0232 - val_mape: 48.3675 - val_mse: 0.0231 - val_rmse: 0.1523\n",
      "Epoch 44/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0222 - mape: 53.7809 - mse: 0.0222 - rmse: 0.1489 - val_loss: 0.0234 - val_mape: 48.3593 - val_mse: 0.0233 - val_rmse: 0.1531\n",
      "Epoch 45/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0226 - mape: 63.5206 - mse: 0.0225 - rmse: 0.1501 - val_loss: 0.0234 - val_mape: 48.3548 - val_mse: 0.0233 - val_rmse: 0.1529\n",
      "Epoch 46/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0225 - mape: 55.2401 - mse: 0.0225 - rmse: 0.1501 - val_loss: 0.0233 - val_mape: 47.7784 - val_mse: 0.0232 - val_rmse: 0.1526\n",
      "Epoch 47/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0226 - mape: 65.2775 - mse: 0.0226 - rmse: 0.1502 - val_loss: 0.0232 - val_mape: 47.7535 - val_mse: 0.0231 - val_rmse: 0.1524\n",
      "Epoch 48/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0224 - mape: 58.9230 - mse: 0.0224 - rmse: 0.1495 - val_loss: 0.0233 - val_mape: 48.6044 - val_mse: 0.0232 - val_rmse: 0.1525\n",
      "Epoch 49/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0225 - mape: 65.5458 - mse: 0.0225 - rmse: 0.1499 - val_loss: 0.0231 - val_mape: 48.3937 - val_mse: 0.0230 - val_rmse: 0.1521\n",
      "Epoch 50/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0224 - mape: 68.7702 - mse: 0.0224 - rmse: 0.1495 - val_loss: 0.0231 - val_mape: 47.7927 - val_mse: 0.0230 - val_rmse: 0.1519\n",
      "Epoch 51/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0226 - mape: 66.0639 - mse: 0.0226 - rmse: 0.1502 - val_loss: 0.0233 - val_mape: 48.2459 - val_mse: 0.0232 - val_rmse: 0.1525\n",
      "Epoch 52/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0223 - mape: 62.5125 - mse: 0.0223 - rmse: 0.1493 - val_loss: 0.0233 - val_mape: 49.0384 - val_mse: 0.0232 - val_rmse: 0.1527\n",
      "Epoch 53/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0226 - mape: 58.7099 - mse: 0.0226 - rmse: 0.1504 - val_loss: 0.0232 - val_mape: 48.4251 - val_mse: 0.0231 - val_rmse: 0.1524\n",
      "Epoch 54/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0224 - mape: 55.4461 - mse: 0.0224 - rmse: 0.1498 - val_loss: 0.0230 - val_mape: 47.6305 - val_mse: 0.0229 - val_rmse: 0.1516\n",
      "Epoch 55/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0223 - mape: 66.7868 - mse: 0.0223 - rmse: 0.1494 - val_loss: 0.0230 - val_mape: 47.5109 - val_mse: 0.0229 - val_rmse: 0.1517\n",
      "Epoch 56/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0226 - mape: 61.6467 - mse: 0.0226 - rmse: 0.1502 - val_loss: 0.0229 - val_mape: 48.0138 - val_mse: 0.0228 - val_rmse: 0.1515\n",
      "Epoch 57/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0223 - mape: 58.8211 - mse: 0.0223 - rmse: 0.1492 - val_loss: 0.0230 - val_mape: 47.7392 - val_mse: 0.0229 - val_rmse: 0.1518\n",
      "Epoch 58/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0223 - mape: 55.2643 - mse: 0.0223 - rmse: 0.1492 - val_loss: 0.0230 - val_mape: 47.2028 - val_mse: 0.0229 - val_rmse: 0.1518\n",
      "Epoch 59/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0223 - mape: 53.7867 - mse: 0.0223 - rmse: 0.1494 - val_loss: 0.0232 - val_mape: 48.5289 - val_mse: 0.0231 - val_rmse: 0.1522\n",
      "Epoch 60/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0220 - mape: 58.2381 - mse: 0.0220 - rmse: 0.1483 - val_loss: 0.0230 - val_mape: 47.4435 - val_mse: 0.0229 - val_rmse: 0.1517\n",
      "Epoch 61/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0222 - mape: 58.6937 - mse: 0.0222 - rmse: 0.1489 - val_loss: 0.0231 - val_mape: 48.1408 - val_mse: 0.0230 - val_rmse: 0.1519\n",
      "Epoch 62/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0224 - mape: 59.7220 - mse: 0.0224 - rmse: 0.1496 - val_loss: 0.0229 - val_mape: 48.0901 - val_mse: 0.0228 - val_rmse: 0.1514\n",
      "Epoch 63/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0225 - mape: 57.6619 - mse: 0.0225 - rmse: 0.1500 - val_loss: 0.0230 - val_mape: 47.4086 - val_mse: 0.0229 - val_rmse: 0.1516\n",
      "Epoch 64/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0225 - mape: 55.6263 - mse: 0.0225 - rmse: 0.1499 - val_loss: 0.0229 - val_mape: 47.0880 - val_mse: 0.0228 - val_rmse: 0.1513\n",
      "Epoch 65/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0225 - mape: 58.2518 - mse: 0.0225 - rmse: 0.1501 - val_loss: 0.0230 - val_mape: 48.2231 - val_mse: 0.0229 - val_rmse: 0.1515\n",
      "Epoch 66/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0221 - mape: 59.6667 - mse: 0.0221 - rmse: 0.1486 - val_loss: 0.0229 - val_mape: 46.9235 - val_mse: 0.0228 - val_rmse: 0.1513\n",
      "Epoch 67/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0224 - mape: 66.8466 - mse: 0.0224 - rmse: 0.1497 - val_loss: 0.0229 - val_mape: 46.5774 - val_mse: 0.0228 - val_rmse: 0.1512\n",
      "Epoch 68/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0220 - mape: 54.8179 - mse: 0.0220 - rmse: 0.1484 - val_loss: 0.0232 - val_mape: 48.8058 - val_mse: 0.0231 - val_rmse: 0.1523\n",
      "Epoch 69/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0221 - mape: 50.2585 - mse: 0.0221 - rmse: 0.1486 - val_loss: 0.0232 - val_mape: 48.3005 - val_mse: 0.0232 - val_rmse: 0.1525\n",
      "Epoch 70/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0225 - mape: 56.9976 - mse: 0.0225 - rmse: 0.1501 - val_loss: 0.0230 - val_mape: 46.8448 - val_mse: 0.0229 - val_rmse: 0.1516\n",
      "Epoch 71/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0224 - mape: 66.8869 - mse: 0.0224 - rmse: 0.1496 - val_loss: 0.0233 - val_mape: 47.9628 - val_mse: 0.0232 - val_rmse: 0.1526\n",
      "Epoch 72/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0222 - mape: 58.5733 - mse: 0.0222 - rmse: 0.1490 - val_loss: 0.0229 - val_mape: 46.0508 - val_mse: 0.0228 - val_rmse: 0.1513\n",
      "Epoch 73/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0222 - mape: 55.8522 - mse: 0.0222 - rmse: 0.1489 - val_loss: 0.0229 - val_mape: 46.9127 - val_mse: 0.0228 - val_rmse: 0.1514\n",
      "Epoch 74/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0222 - mape: 64.8550 - mse: 0.0222 - rmse: 0.1489 - val_loss: 0.0231 - val_mape: 47.7651 - val_mse: 0.0230 - val_rmse: 0.1519\n",
      "Epoch 75/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0221 - mape: 55.9198 - mse: 0.0221 - rmse: 0.1487 - val_loss: 0.0229 - val_mape: 47.7859 - val_mse: 0.0228 - val_rmse: 0.1513\n",
      "Epoch 76/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0222 - mape: 69.8855 - mse: 0.0222 - rmse: 0.1489 - val_loss: 0.0228 - val_mape: 46.7193 - val_mse: 0.0227 - val_rmse: 0.1511\n",
      "Epoch 77/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0219 - mape: 52.8619 - mse: 0.0219 - rmse: 0.1481 - val_loss: 0.0232 - val_mape: 46.6966 - val_mse: 0.0231 - val_rmse: 0.1523\n",
      "Epoch 78/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0219 - mape: 56.3892 - mse: 0.0219 - rmse: 0.1478 - val_loss: 0.0230 - val_mape: 46.8775 - val_mse: 0.0229 - val_rmse: 0.1516\n",
      "Epoch 79/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0222 - mape: 60.8053 - mse: 0.0222 - rmse: 0.1491 - val_loss: 0.0232 - val_mape: 48.6475 - val_mse: 0.0231 - val_rmse: 0.1523\n",
      "Epoch 80/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0223 - mape: 69.0902 - mse: 0.0223 - rmse: 0.1492 - val_loss: 0.0228 - val_mape: 47.2979 - val_mse: 0.0227 - val_rmse: 0.1510\n",
      "Epoch 81/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0215 - mape: 57.1113 - mse: 0.0215 - rmse: 0.1467 - val_loss: 0.0229 - val_mape: 46.7118 - val_mse: 0.0228 - val_rmse: 0.1514\n",
      "Epoch 82/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0224 - mape: 59.3105 - mse: 0.0224 - rmse: 0.1495 - val_loss: 0.0229 - val_mape: 47.4840 - val_mse: 0.0228 - val_rmse: 0.1514\n",
      "Epoch 83/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0223 - mape: 56.9765 - mse: 0.0223 - rmse: 0.1492 - val_loss: 0.0231 - val_mape: 47.5058 - val_mse: 0.0230 - val_rmse: 0.1519\n",
      "Epoch 84/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0217 - mape: 63.5947 - mse: 0.0217 - rmse: 0.1474 - val_loss: 0.0229 - val_mape: 47.2936 - val_mse: 0.0228 - val_rmse: 0.1514\n",
      "Epoch 85/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0218 - mape: 57.7712 - mse: 0.0218 - rmse: 0.1477 - val_loss: 0.0229 - val_mape: 47.0149 - val_mse: 0.0228 - val_rmse: 0.1514\n",
      "Epoch 86/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0219 - mape: 68.2342 - mse: 0.0219 - rmse: 0.1481 - val_loss: 0.0229 - val_mape: 47.1470 - val_mse: 0.0228 - val_rmse: 0.1513\n",
      "Epoch 87/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0221 - mape: 56.1261 - mse: 0.0221 - rmse: 0.1485 - val_loss: 0.0238 - val_mape: 47.2840 - val_mse: 0.0237 - val_rmse: 0.1543\n",
      "Epoch 88/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0218 - mape: 61.0799 - mse: 0.0218 - rmse: 0.1476 - val_loss: 0.0234 - val_mape: 47.7111 - val_mse: 0.0233 - val_rmse: 0.1531\n",
      "Epoch 89/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0221 - mape: 62.3170 - mse: 0.0221 - rmse: 0.1485 - val_loss: 0.0234 - val_mape: 46.3161 - val_mse: 0.0233 - val_rmse: 0.1530\n",
      "Epoch 90/200\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0221 - mape: 55.9002 - mse: 0.0221 - rmse: 0.1486 - val_loss: 0.0234 - val_mape: 47.2578 - val_mse: 0.0233 - val_rmse: 0.1528\n",
      "[MODELO CONGELADO]: 35.0481231212616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 1.7613976000335925, 1.3271765519453667, 0.8594702538709806, 23.7106679168087\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 0.7207908535241874, 0.8489940244337338, 0.6801682545661044, 27.590960768782697\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 2.6885784219557602, 1.639688513698794, 0.8581759091934558, 24.61617251121446\n",
      "PROCESANDO ARCHIVO: Pinus roxburghii\n",
      "(10635, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Pinus roxburghii_best_models.json\n",
      "(6309, 4, 43) (2167, 4, 43) (1647, 4, 43)\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_6, built=True> False\n",
      "<BatchNormalization name=batch_normalization_6, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Conv1D name=conv1d_7, built=True> True\n",
      "<BatchNormalization name=batch_normalization_7, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0408 - mape: 70.3253 - mse: 0.0408 - rmse: 0.2014 - val_loss: 0.0338 - val_mape: 43.8385 - val_mse: 0.0337 - val_rmse: 0.1838\n",
      "Epoch 2/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0302 - mape: 56.4434 - mse: 0.0302 - rmse: 0.1737 - val_loss: 0.0327 - val_mape: 42.9189 - val_mse: 0.0326 - val_rmse: 0.1807\n",
      "Epoch 3/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0287 - mape: 53.8782 - mse: 0.0287 - rmse: 0.1693 - val_loss: 0.0326 - val_mape: 41.8275 - val_mse: 0.0326 - val_rmse: 0.1807\n",
      "Epoch 4/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0278 - mape: 52.5148 - mse: 0.0278 - rmse: 0.1669 - val_loss: 0.0331 - val_mape: 40.9306 - val_mse: 0.0330 - val_rmse: 0.1819\n",
      "Epoch 5/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0272 - mape: 51.3137 - mse: 0.0272 - rmse: 0.1649 - val_loss: 0.0324 - val_mape: 40.2811 - val_mse: 0.0323 - val_rmse: 0.1801\n",
      "Epoch 6/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0266 - mape: 50.3965 - mse: 0.0266 - rmse: 0.1630 - val_loss: 0.0328 - val_mape: 39.5884 - val_mse: 0.0327 - val_rmse: 0.1812\n",
      "Epoch 7/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0262 - mape: 49.6701 - mse: 0.0262 - rmse: 0.1618 - val_loss: 0.0329 - val_mape: 39.2029 - val_mse: 0.0328 - val_rmse: 0.1814\n",
      "Epoch 8/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0258 - mape: 49.1168 - mse: 0.0258 - rmse: 0.1607 - val_loss: 0.0333 - val_mape: 39.0442 - val_mse: 0.0333 - val_rmse: 0.1826\n",
      "Epoch 9/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0255 - mape: 48.6843 - mse: 0.0255 - rmse: 0.1597 - val_loss: 0.0338 - val_mape: 38.8302 - val_mse: 0.0337 - val_rmse: 0.1837\n",
      "Epoch 10/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0253 - mape: 48.3967 - mse: 0.0253 - rmse: 0.1590 - val_loss: 0.0332 - val_mape: 38.7241 - val_mse: 0.0331 - val_rmse: 0.1823\n",
      "Epoch 11/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0250 - mape: 47.9842 - mse: 0.0250 - rmse: 0.1582 - val_loss: 0.0317 - val_mape: 38.3172 - val_mse: 0.0316 - val_rmse: 0.1779\n",
      "Epoch 12/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0249 - mape: 47.7261 - mse: 0.0249 - rmse: 0.1577 - val_loss: 0.0301 - val_mape: 38.1449 - val_mse: 0.0300 - val_rmse: 0.1735\n",
      "Epoch 13/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0246 - mape: 47.5972 - mse: 0.0246 - rmse: 0.1569 - val_loss: 0.0292 - val_mape: 37.9307 - val_mse: 0.0291 - val_rmse: 0.1709\n",
      "Epoch 14/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0245 - mape: 47.5083 - mse: 0.0245 - rmse: 0.1565 - val_loss: 0.0291 - val_mape: 37.5090 - val_mse: 0.0290 - val_rmse: 0.1705\n",
      "Epoch 15/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0242 - mape: 47.2803 - mse: 0.0242 - rmse: 0.1554 - val_loss: 0.0276 - val_mape: 37.0833 - val_mse: 0.0275 - val_rmse: 0.1661\n",
      "Epoch 16/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0240 - mape: 46.9765 - mse: 0.0240 - rmse: 0.1549 - val_loss: 0.0277 - val_mape: 37.1799 - val_mse: 0.0277 - val_rmse: 0.1665\n",
      "Epoch 17/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0238 - mape: 47.0838 - mse: 0.0238 - rmse: 0.1543 - val_loss: 0.0283 - val_mape: 37.2966 - val_mse: 0.0282 - val_rmse: 0.1681\n",
      "Epoch 18/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0237 - mape: 46.7740 - mse: 0.0237 - rmse: 0.1540 - val_loss: 0.0283 - val_mape: 37.3207 - val_mse: 0.0282 - val_rmse: 0.1683\n",
      "Epoch 19/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0235 - mape: 46.7212 - mse: 0.0235 - rmse: 0.1533 - val_loss: 0.0283 - val_mape: 37.3824 - val_mse: 0.0282 - val_rmse: 0.1682\n",
      "Epoch 20/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0233 - mape: 46.1951 - mse: 0.0233 - rmse: 0.1526 - val_loss: 0.0285 - val_mape: 37.4518 - val_mse: 0.0284 - val_rmse: 0.1687\n",
      "Epoch 21/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0231 - mape: 46.2224 - mse: 0.0231 - rmse: 0.1520 - val_loss: 0.0289 - val_mape: 37.6227 - val_mse: 0.0288 - val_rmse: 0.1700\n",
      "Epoch 22/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0230 - mape: 46.0291 - mse: 0.0230 - rmse: 0.1515 - val_loss: 0.0288 - val_mape: 37.5466 - val_mse: 0.0287 - val_rmse: 0.1696\n",
      "Epoch 23/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0227 - mape: 45.8164 - mse: 0.0227 - rmse: 0.1508 - val_loss: 0.0289 - val_mape: 37.6725 - val_mse: 0.0288 - val_rmse: 0.1700\n",
      "Epoch 24/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0225 - mape: 45.5155 - mse: 0.0225 - rmse: 0.1501 - val_loss: 0.0290 - val_mape: 37.7446 - val_mse: 0.0289 - val_rmse: 0.1704\n",
      "Epoch 25/200\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0224 - mape: 45.4765 - mse: 0.0224 - rmse: 0.1495 - val_loss: 0.0282 - val_mape: 38.3754 - val_mse: 0.0281 - val_rmse: 0.1680\n",
      "[MODELO CONGELADO]: 15.77542519569397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 117.93502957426952, 10.859789573203964, 0.5578574417507602, 32.51223066103115\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 77.1481907726898, 8.783404281523753, 0.6556012510211603, 30.895510999371155\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 92.11851601685343, 9.597839132682598, 0.6134514578019596, 31.702248868143812\n",
      "PROCESANDO ARCHIVO: Juniperus turkestanica Komar.\n",
      "(6305, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Juniperus turkestanica Komar._best_models.json\n",
      "(3664, 4, 43) (1368, 4, 43) (1001, 4, 43)\n",
      "<Conv1D name=conv1d_7, built=True> False\n",
      "<BatchNormalization name=batch_normalization_7, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_6, built=True> False\n",
      "<Conv1D name=conv1d_8, built=True> False\n",
      "<BatchNormalization name=batch_normalization_8, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_7, built=True> False\n",
      "<Conv1D name=conv1d_9, built=True> False\n",
      "<BatchNormalization name=batch_normalization_9, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_8, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_10, built=True> False\n",
      "<BatchNormalization name=batch_normalization_10, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_9, built=True> False\n",
      "<Conv1D name=conv1d_11, built=True> False\n",
      "<BatchNormalization name=batch_normalization_11, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_10, built=True> False\n",
      "<Conv1D name=conv1d_12, built=True> False\n",
      "<BatchNormalization name=batch_normalization_12, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_11, built=True> False\n",
      "<Conv1D name=conv1d_13, built=True> True\n",
      "<BatchNormalization name=batch_normalization_13, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0311 - mape: 52.4464 - mse: 0.0311 - rmse: 0.1756 - val_loss: 0.0306 - val_mape: 56.8364 - val_mse: 0.0311 - val_rmse: 0.1750\n",
      "Epoch 2/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0227 - mape: 43.3483 - mse: 0.0227 - rmse: 0.1507 - val_loss: 0.0262 - val_mape: 50.0166 - val_mse: 0.0264 - val_rmse: 0.1617\n",
      "Epoch 3/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0218 - mape: 41.7370 - mse: 0.0218 - rmse: 0.1474 - val_loss: 0.0247 - val_mape: 46.6575 - val_mse: 0.0249 - val_rmse: 0.1570\n",
      "Epoch 4/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0211 - mape: 40.1426 - mse: 0.0211 - rmse: 0.1450 - val_loss: 0.0221 - val_mape: 41.3871 - val_mse: 0.0222 - val_rmse: 0.1486\n",
      "Epoch 5/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0212 - mape: 38.2813 - mse: 0.0212 - rmse: 0.1455 - val_loss: 0.0237 - val_mape: 45.2750 - val_mse: 0.0239 - val_rmse: 0.1538\n",
      "Epoch 6/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0206 - mape: 37.9979 - mse: 0.0206 - rmse: 0.1435 - val_loss: 0.0251 - val_mape: 48.1156 - val_mse: 0.0254 - val_rmse: 0.1584\n",
      "Epoch 7/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - mape: 37.9185 - mse: 0.0208 - rmse: 0.1442 - val_loss: 0.0235 - val_mape: 44.9856 - val_mse: 0.0237 - val_rmse: 0.1532\n",
      "Epoch 8/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0205 - mape: 36.9189 - mse: 0.0205 - rmse: 0.1433 - val_loss: 0.0237 - val_mape: 45.8037 - val_mse: 0.0240 - val_rmse: 0.1541\n",
      "Epoch 9/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0206 - mape: 37.5607 - mse: 0.0206 - rmse: 0.1435 - val_loss: 0.0219 - val_mape: 41.4999 - val_mse: 0.0220 - val_rmse: 0.1479\n",
      "Epoch 10/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0206 - mape: 36.5660 - mse: 0.0206 - rmse: 0.1434 - val_loss: 0.0233 - val_mape: 44.9137 - val_mse: 0.0234 - val_rmse: 0.1525\n",
      "Epoch 11/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0210 - mape: 36.8093 - mse: 0.0210 - rmse: 0.1450 - val_loss: 0.0237 - val_mape: 45.6511 - val_mse: 0.0239 - val_rmse: 0.1539\n",
      "Epoch 12/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0207 - mape: 36.8354 - mse: 0.0207 - rmse: 0.1438 - val_loss: 0.0217 - val_mape: 40.9571 - val_mse: 0.0218 - val_rmse: 0.1474\n",
      "Epoch 13/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0204 - mape: 35.8191 - mse: 0.0204 - rmse: 0.1428 - val_loss: 0.0215 - val_mape: 40.5710 - val_mse: 0.0217 - val_rmse: 0.1467\n",
      "Epoch 14/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0205 - mape: 35.5208 - mse: 0.0205 - rmse: 0.1430 - val_loss: 0.0220 - val_mape: 41.8947 - val_mse: 0.0221 - val_rmse: 0.1483\n",
      "Epoch 15/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0206 - mape: 35.1591 - mse: 0.0206 - rmse: 0.1433 - val_loss: 0.0216 - val_mape: 41.0036 - val_mse: 0.0218 - val_rmse: 0.1469\n",
      "Epoch 16/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0203 - mape: 35.1429 - mse: 0.0203 - rmse: 0.1425 - val_loss: 0.0211 - val_mape: 39.8371 - val_mse: 0.0213 - val_rmse: 0.1454\n",
      "Epoch 17/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0201 - mape: 34.4416 - mse: 0.0201 - rmse: 0.1418 - val_loss: 0.0220 - val_mape: 42.2071 - val_mse: 0.0221 - val_rmse: 0.1485\n",
      "Epoch 18/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0204 - mape: 35.0966 - mse: 0.0204 - rmse: 0.1428 - val_loss: 0.0217 - val_mape: 41.6058 - val_mse: 0.0218 - val_rmse: 0.1472\n",
      "Epoch 19/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0202 - mape: 35.9750 - mse: 0.0202 - rmse: 0.1421 - val_loss: 0.0216 - val_mape: 41.4332 - val_mse: 0.0217 - val_rmse: 0.1470\n",
      "Epoch 20/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0204 - mape: 35.1729 - mse: 0.0204 - rmse: 0.1427 - val_loss: 0.0226 - val_mape: 43.4591 - val_mse: 0.0227 - val_rmse: 0.1503\n",
      "Epoch 21/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0200 - mape: 34.7564 - mse: 0.0200 - rmse: 0.1413 - val_loss: 0.0217 - val_mape: 40.9874 - val_mse: 0.0218 - val_rmse: 0.1473\n",
      "Epoch 22/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0201 - mape: 35.1487 - mse: 0.0201 - rmse: 0.1417 - val_loss: 0.0208 - val_mape: 39.2470 - val_mse: 0.0209 - val_rmse: 0.1444\n",
      "Epoch 23/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0201 - mape: 34.7804 - mse: 0.0201 - rmse: 0.1416 - val_loss: 0.0222 - val_mape: 42.7168 - val_mse: 0.0223 - val_rmse: 0.1489\n",
      "Epoch 24/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0203 - mape: 35.6147 - mse: 0.0203 - rmse: 0.1424 - val_loss: 0.0216 - val_mape: 40.9754 - val_mse: 0.0217 - val_rmse: 0.1469\n",
      "Epoch 25/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0204 - mape: 35.8884 - mse: 0.0204 - rmse: 0.1428 - val_loss: 0.0215 - val_mape: 41.2316 - val_mse: 0.0216 - val_rmse: 0.1466\n",
      "Epoch 26/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0202 - mape: 35.9032 - mse: 0.0202 - rmse: 0.1420 - val_loss: 0.0219 - val_mape: 41.9335 - val_mse: 0.0220 - val_rmse: 0.1480\n",
      "Epoch 27/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0203 - mape: 34.8873 - mse: 0.0203 - rmse: 0.1424 - val_loss: 0.0213 - val_mape: 40.4288 - val_mse: 0.0214 - val_rmse: 0.1458\n",
      "Epoch 28/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0202 - mape: 36.4327 - mse: 0.0202 - rmse: 0.1422 - val_loss: 0.0224 - val_mape: 42.8241 - val_mse: 0.0225 - val_rmse: 0.1496\n",
      "Epoch 29/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0202 - mape: 34.9400 - mse: 0.0202 - rmse: 0.1420 - val_loss: 0.0221 - val_mape: 42.2406 - val_mse: 0.0223 - val_rmse: 0.1486\n",
      "Epoch 30/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0202 - mape: 34.9323 - mse: 0.0202 - rmse: 0.1420 - val_loss: 0.0213 - val_mape: 40.4309 - val_mse: 0.0215 - val_rmse: 0.1460\n",
      "Epoch 31/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0201 - mape: 34.6989 - mse: 0.0201 - rmse: 0.1415 - val_loss: 0.0219 - val_mape: 41.7498 - val_mse: 0.0221 - val_rmse: 0.1481\n",
      "Epoch 32/200\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0197 - mape: 34.1624 - mse: 0.0197 - rmse: 0.1404 - val_loss: 0.0219 - val_mape: 42.0427 - val_mse: 0.0220 - val_rmse: 0.1479\n",
      "[MODELO CONGELADO]: 13.830387830734253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 4.550828839659339, 2.1332671749359804, 0.898615379583831, 22.2114461236428\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 3.137267729691672, 1.7712333922133672, 0.8175593808848911, 24.425837937989712\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 1.2584394833373218, 1.1218018913058232, 0.7684822841532443, 22.69464310507794\n",
      "PROCESANDO ARCHIVO: Populus ciliata\n",
      "(1182, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Populus ciliata_best_models.json\n",
      "(705, 4, 43) (156, 4, 43) (265, 4, 43)\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Conv1D name=conv1d_6, built=True> False\n",
      "<BatchNormalization name=batch_normalization_6, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_7, built=True> False\n",
      "<BatchNormalization name=batch_normalization_7, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_6, built=True> False\n",
      "<Conv1D name=conv1d_8, built=True> False\n",
      "<BatchNormalization name=batch_normalization_8, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_7, built=True> False\n",
      "<Conv1D name=conv1d_9, built=True> True\n",
      "<BatchNormalization name=batch_normalization_9, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0280 - mape: 53.1801 - mse: 0.0279 - rmse: 0.1669 - val_loss: 0.0390 - val_mape: 39.6242 - val_mse: 0.0404 - val_rmse: 0.1975\n",
      "Epoch 2/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0244 - mape: 46.0736 - mse: 0.0244 - rmse: 0.1561 - val_loss: 0.0373 - val_mape: 39.1979 - val_mse: 0.0389 - val_rmse: 0.1932\n",
      "Epoch 3/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0241 - mape: 45.6242 - mse: 0.0240 - rmse: 0.1551 - val_loss: 0.0361 - val_mape: 39.0660 - val_mse: 0.0376 - val_rmse: 0.1900\n",
      "Epoch 4/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0240 - mape: 43.9990 - mse: 0.0239 - rmse: 0.1547 - val_loss: 0.0347 - val_mape: 38.9540 - val_mse: 0.0361 - val_rmse: 0.1862\n",
      "Epoch 5/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0238 - mape: 43.0367 - mse: 0.0237 - rmse: 0.1542 - val_loss: 0.0323 - val_mape: 39.0771 - val_mse: 0.0336 - val_rmse: 0.1797\n",
      "Epoch 6/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0234 - mape: 42.7326 - mse: 0.0233 - rmse: 0.1527 - val_loss: 0.0310 - val_mape: 38.3777 - val_mse: 0.0321 - val_rmse: 0.1761\n",
      "Epoch 7/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0235 - mape: 43.5880 - mse: 0.0234 - rmse: 0.1531 - val_loss: 0.0281 - val_mape: 42.6741 - val_mse: 0.0290 - val_rmse: 0.1677\n",
      "Epoch 8/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0223 - mape: 41.7670 - mse: 0.0223 - rmse: 0.1494 - val_loss: 0.0262 - val_mape: 46.3041 - val_mse: 0.0268 - val_rmse: 0.1618\n",
      "Epoch 9/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0225 - mape: 42.6432 - mse: 0.0224 - rmse: 0.1499 - val_loss: 0.0255 - val_mape: 43.3194 - val_mse: 0.0263 - val_rmse: 0.1596\n",
      "Epoch 10/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0224 - mape: 41.9060 - mse: 0.0224 - rmse: 0.1497 - val_loss: 0.0254 - val_mape: 46.8311 - val_mse: 0.0262 - val_rmse: 0.1594\n",
      "Epoch 11/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0220 - mape: 41.6097 - mse: 0.0220 - rmse: 0.1482 - val_loss: 0.0242 - val_mape: 45.5142 - val_mse: 0.0250 - val_rmse: 0.1555\n",
      "Epoch 12/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0219 - mape: 41.9844 - mse: 0.0218 - rmse: 0.1479 - val_loss: 0.0244 - val_mape: 41.4927 - val_mse: 0.0252 - val_rmse: 0.1563\n",
      "Epoch 13/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0219 - mape: 40.7134 - mse: 0.0218 - rmse: 0.1479 - val_loss: 0.0246 - val_mape: 44.8056 - val_mse: 0.0254 - val_rmse: 0.1568\n",
      "Epoch 14/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0211 - mape: 40.2612 - mse: 0.0210 - rmse: 0.1450 - val_loss: 0.0241 - val_mape: 42.8225 - val_mse: 0.0249 - val_rmse: 0.1551\n",
      "Epoch 15/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0213 - mape: 40.3728 - mse: 0.0213 - rmse: 0.1459 - val_loss: 0.0240 - val_mape: 42.3460 - val_mse: 0.0248 - val_rmse: 0.1550\n",
      "Epoch 16/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0217 - mape: 41.4212 - mse: 0.0216 - rmse: 0.1471 - val_loss: 0.0236 - val_mape: 41.7351 - val_mse: 0.0244 - val_rmse: 0.1536\n",
      "Epoch 17/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0210 - mape: 40.6434 - mse: 0.0209 - rmse: 0.1447 - val_loss: 0.0242 - val_mape: 39.0180 - val_mse: 0.0250 - val_rmse: 0.1557\n",
      "Epoch 18/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0212 - mape: 40.1244 - mse: 0.0211 - rmse: 0.1454 - val_loss: 0.0244 - val_mape: 40.1652 - val_mse: 0.0251 - val_rmse: 0.1562\n",
      "Epoch 19/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0208 - mape: 39.1174 - mse: 0.0208 - rmse: 0.1443 - val_loss: 0.0238 - val_mape: 41.9092 - val_mse: 0.0245 - val_rmse: 0.1541\n",
      "Epoch 20/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0211 - mape: 40.2844 - mse: 0.0211 - rmse: 0.1452 - val_loss: 0.0239 - val_mape: 40.2014 - val_mse: 0.0247 - val_rmse: 0.1546\n",
      "Epoch 21/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0215 - mape: 40.9566 - mse: 0.0214 - rmse: 0.1465 - val_loss: 0.0245 - val_mape: 40.7550 - val_mse: 0.0252 - val_rmse: 0.1565\n",
      "Epoch 22/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0213 - mape: 39.1606 - mse: 0.0213 - rmse: 0.1459 - val_loss: 0.0235 - val_mape: 42.4882 - val_mse: 0.0241 - val_rmse: 0.1534\n",
      "Epoch 23/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0212 - mape: 39.8437 - mse: 0.0211 - rmse: 0.1455 - val_loss: 0.0228 - val_mape: 42.2447 - val_mse: 0.0236 - val_rmse: 0.1510\n",
      "Epoch 24/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0211 - mape: 40.7899 - mse: 0.0210 - rmse: 0.1450 - val_loss: 0.0232 - val_mape: 41.8467 - val_mse: 0.0238 - val_rmse: 0.1524\n",
      "Epoch 25/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0208 - mape: 40.2628 - mse: 0.0208 - rmse: 0.1442 - val_loss: 0.0231 - val_mape: 41.5082 - val_mse: 0.0237 - val_rmse: 0.1521\n",
      "Epoch 26/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209 - mape: 40.3644 - mse: 0.0208 - rmse: 0.1444 - val_loss: 0.0233 - val_mape: 41.3168 - val_mse: 0.0238 - val_rmse: 0.1527\n",
      "Epoch 27/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0212 - mape: 39.3343 - mse: 0.0211 - rmse: 0.1453 - val_loss: 0.0233 - val_mape: 44.6959 - val_mse: 0.0239 - val_rmse: 0.1527\n",
      "Epoch 28/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0204 - mape: 38.9607 - mse: 0.0204 - rmse: 0.1428 - val_loss: 0.0241 - val_mape: 41.7858 - val_mse: 0.0246 - val_rmse: 0.1554\n",
      "Epoch 29/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0222 - mape: 39.8397 - mse: 0.0221 - rmse: 0.1487 - val_loss: 0.0261 - val_mape: 60.9334 - val_mse: 0.0269 - val_rmse: 0.1616\n",
      "Epoch 30/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0228 - mape: 43.2020 - mse: 0.0227 - rmse: 0.1507 - val_loss: 0.0237 - val_mape: 44.8602 - val_mse: 0.0243 - val_rmse: 0.1541\n",
      "Epoch 31/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0209 - mape: 40.8032 - mse: 0.0209 - rmse: 0.1445 - val_loss: 0.0237 - val_mape: 46.9972 - val_mse: 0.0242 - val_rmse: 0.1538\n",
      "Epoch 32/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0198 - mape: 40.4785 - mse: 0.0198 - rmse: 0.1407 - val_loss: 0.0235 - val_mape: 48.0004 - val_mse: 0.0241 - val_rmse: 0.1534\n",
      "Epoch 33/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0210 - mape: 41.6867 - mse: 0.0209 - rmse: 0.1446 - val_loss: 0.0242 - val_mape: 46.1737 - val_mse: 0.0248 - val_rmse: 0.1557\n",
      "[MODELO CONGELADO]: 4.645586967468262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 117.85563927062505, 10.856133716504464, 0.6755011994691803, 28.68850322027755\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 129.9810249927029, 11.400922111509354, 0.4461775534180016, 35.161560879602405\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 66.90509856795892, 8.179553690022392, 0.6656915685847457, 29.356277204818642\n",
      "PROCESANDO ARCHIVO: Abies pindrow\n",
      "(16018, 77)\n",
      "SE HA NORMALIZADO EL ARCHIVO: Abies pindrow_best_models.json\n",
      "(9042, 4, 43) (3441, 4, 43) (2815, 4, 43)\n",
      "<Conv1D name=conv1d_5, built=True> False\n",
      "<BatchNormalization name=batch_normalization_5, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_4, built=True> False\n",
      "<Conv1D name=conv1d_6, built=True> False\n",
      "<BatchNormalization name=batch_normalization_6, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_5, built=True> False\n",
      "<Dropout name=dropout_1, built=True> True\n",
      "<Conv1D name=conv1d_7, built=True> False\n",
      "<BatchNormalization name=batch_normalization_7, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_6, built=True> False\n",
      "<Conv1D name=conv1d_8, built=True> False\n",
      "<BatchNormalization name=batch_normalization_8, built=True> False\n",
      "<MaxPooling1D name=max_pooling1d_7, built=True> False\n",
      "<Conv1D name=conv1d_9, built=True> True\n",
      "<BatchNormalization name=batch_normalization_9, built=True> True\n",
      "<GlobalAveragePooling1D name=global_average_pooling1d_1, built=True> True\n",
      "<Dense name=dense_2, built=True> True\n",
      "<Dense name=dense_3, built=True> True\n",
      "Epoch 1/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0177 - mape: 28.4694 - mse: 0.0177 - rmse: 0.1332 - val_loss: 0.0126 - val_mape: 21.2390 - val_mse: 0.0126 - val_rmse: 0.1122\n",
      "Epoch 2/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0157 - mape: 26.5942 - mse: 0.0157 - rmse: 0.1251 - val_loss: 0.0124 - val_mape: 20.7511 - val_mse: 0.0124 - val_rmse: 0.1113\n",
      "Epoch 3/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0152 - mape: 26.2512 - mse: 0.0152 - rmse: 0.1233 - val_loss: 0.0121 - val_mape: 20.2297 - val_mse: 0.0121 - val_rmse: 0.1101\n",
      "Epoch 4/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0147 - mape: 25.3090 - mse: 0.0147 - rmse: 0.1212 - val_loss: 0.0120 - val_mape: 20.0575 - val_mse: 0.0120 - val_rmse: 0.1097\n",
      "Epoch 5/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0144 - mape: 24.9609 - mse: 0.0144 - rmse: 0.1201 - val_loss: 0.0119 - val_mape: 19.9535 - val_mse: 0.0119 - val_rmse: 0.1093\n",
      "Epoch 6/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0141 - mape: 24.5842 - mse: 0.0141 - rmse: 0.1187 - val_loss: 0.0118 - val_mape: 19.8163 - val_mse: 0.0119 - val_rmse: 0.1089\n",
      "Epoch 7/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0141 - mape: 24.6489 - mse: 0.0141 - rmse: 0.1189 - val_loss: 0.0118 - val_mape: 19.5737 - val_mse: 0.0118 - val_rmse: 0.1087\n",
      "Epoch 8/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0140 - mape: 24.5269 - mse: 0.0140 - rmse: 0.1184 - val_loss: 0.0118 - val_mape: 19.7488 - val_mse: 0.0118 - val_rmse: 0.1087\n",
      "Epoch 9/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0139 - mape: 24.1962 - mse: 0.0139 - rmse: 0.1178 - val_loss: 0.0118 - val_mape: 19.6192 - val_mse: 0.0118 - val_rmse: 0.1087\n",
      "Epoch 10/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0138 - mape: 24.0392 - mse: 0.0138 - rmse: 0.1175 - val_loss: 0.0117 - val_mape: 19.4414 - val_mse: 0.0117 - val_rmse: 0.1081\n",
      "Epoch 11/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0137 - mape: 24.0906 - mse: 0.0137 - rmse: 0.1169 - val_loss: 0.0117 - val_mape: 19.5499 - val_mse: 0.0117 - val_rmse: 0.1082\n",
      "Epoch 12/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0135 - mape: 23.7043 - mse: 0.0135 - rmse: 0.1161 - val_loss: 0.0118 - val_mape: 19.6007 - val_mse: 0.0118 - val_rmse: 0.1084\n",
      "Epoch 13/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0134 - mape: 23.6596 - mse: 0.0134 - rmse: 0.1155 - val_loss: 0.0117 - val_mape: 19.6048 - val_mse: 0.0117 - val_rmse: 0.1083\n",
      "Epoch 14/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0134 - mape: 23.5378 - mse: 0.0134 - rmse: 0.1156 - val_loss: 0.0117 - val_mape: 19.3987 - val_mse: 0.0117 - val_rmse: 0.1082\n",
      "Epoch 15/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0134 - mape: 23.6396 - mse: 0.0134 - rmse: 0.1158 - val_loss: 0.0117 - val_mape: 19.4777 - val_mse: 0.0117 - val_rmse: 0.1080\n",
      "Epoch 16/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0134 - mape: 23.5746 - mse: 0.0134 - rmse: 0.1156 - val_loss: 0.0115 - val_mape: 19.4745 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 17/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0131 - mape: 23.2671 - mse: 0.0131 - rmse: 0.1146 - val_loss: 0.0116 - val_mape: 19.6012 - val_mse: 0.0116 - val_rmse: 0.1076\n",
      "Epoch 18/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0132 - mape: 23.4553 - mse: 0.0132 - rmse: 0.1147 - val_loss: 0.0117 - val_mape: 19.3905 - val_mse: 0.0117 - val_rmse: 0.1082\n",
      "Epoch 19/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0133 - mape: 23.4726 - mse: 0.0133 - rmse: 0.1152 - val_loss: 0.0115 - val_mape: 19.2193 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 20/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0130 - mape: 23.3063 - mse: 0.0130 - rmse: 0.1141 - val_loss: 0.0115 - val_mape: 19.2061 - val_mse: 0.0116 - val_rmse: 0.1075\n",
      "Epoch 21/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0129 - mape: 23.0909 - mse: 0.0129 - rmse: 0.1134 - val_loss: 0.0115 - val_mape: 19.0800 - val_mse: 0.0115 - val_rmse: 0.1074\n",
      "Epoch 22/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0129 - mape: 22.9908 - mse: 0.0129 - rmse: 0.1134 - val_loss: 0.0115 - val_mape: 19.2666 - val_mse: 0.0115 - val_rmse: 0.1071\n",
      "Epoch 23/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0130 - mape: 23.1469 - mse: 0.0130 - rmse: 0.1139 - val_loss: 0.0114 - val_mape: 19.1572 - val_mse: 0.0114 - val_rmse: 0.1068\n",
      "Epoch 24/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0128 - mape: 23.2851 - mse: 0.0128 - rmse: 0.1130 - val_loss: 0.0115 - val_mape: 19.3911 - val_mse: 0.0115 - val_rmse: 0.1073\n",
      "Epoch 25/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0129 - mape: 22.9351 - mse: 0.0129 - rmse: 0.1134 - val_loss: 0.0115 - val_mape: 19.2155 - val_mse: 0.0115 - val_rmse: 0.1070\n",
      "Epoch 26/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0128 - mape: 23.0271 - mse: 0.0128 - rmse: 0.1132 - val_loss: 0.0114 - val_mape: 19.2827 - val_mse: 0.0114 - val_rmse: 0.1069\n",
      "Epoch 27/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0128 - mape: 23.2352 - mse: 0.0128 - rmse: 0.1131 - val_loss: 0.0114 - val_mape: 19.2071 - val_mse: 0.0114 - val_rmse: 0.1068\n",
      "Epoch 28/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0127 - mape: 22.8305 - mse: 0.0127 - rmse: 0.1128 - val_loss: 0.0114 - val_mape: 18.9781 - val_mse: 0.0114 - val_rmse: 0.1068\n",
      "Epoch 29/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0127 - mape: 22.9256 - mse: 0.0127 - rmse: 0.1126 - val_loss: 0.0113 - val_mape: 19.5116 - val_mse: 0.0113 - val_rmse: 0.1063\n",
      "Epoch 30/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0124 - mape: 22.6837 - mse: 0.0124 - rmse: 0.1115 - val_loss: 0.0113 - val_mape: 18.9330 - val_mse: 0.0113 - val_rmse: 0.1063\n",
      "Epoch 31/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0125 - mape: 22.7777 - mse: 0.0125 - rmse: 0.1118 - val_loss: 0.0114 - val_mape: 18.9065 - val_mse: 0.0115 - val_rmse: 0.1070\n",
      "Epoch 32/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0124 - mape: 22.6098 - mse: 0.0124 - rmse: 0.1113 - val_loss: 0.0114 - val_mape: 18.9939 - val_mse: 0.0114 - val_rmse: 0.1066\n",
      "Epoch 33/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0125 - mape: 22.4058 - mse: 0.0125 - rmse: 0.1116 - val_loss: 0.0115 - val_mape: 18.9503 - val_mse: 0.0115 - val_rmse: 0.1072\n",
      "Epoch 34/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0123 - mape: 22.5613 - mse: 0.0123 - rmse: 0.1107 - val_loss: 0.0113 - val_mape: 19.0601 - val_mse: 0.0113 - val_rmse: 0.1065\n",
      "Epoch 35/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0125 - mape: 22.6410 - mse: 0.0125 - rmse: 0.1117 - val_loss: 0.0113 - val_mape: 18.8433 - val_mse: 0.0113 - val_rmse: 0.1063\n",
      "Epoch 36/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0124 - mape: 22.7295 - mse: 0.0124 - rmse: 0.1114 - val_loss: 0.0113 - val_mape: 19.0901 - val_mse: 0.0113 - val_rmse: 0.1062\n",
      "Epoch 37/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0125 - mape: 22.7004 - mse: 0.0125 - rmse: 0.1117 - val_loss: 0.0116 - val_mape: 19.2491 - val_mse: 0.0116 - val_rmse: 0.1077\n",
      "Epoch 38/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0123 - mape: 22.5165 - mse: 0.0123 - rmse: 0.1109 - val_loss: 0.0113 - val_mape: 18.8403 - val_mse: 0.0114 - val_rmse: 0.1065\n",
      "Epoch 39/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0120 - mape: 22.3861 - mse: 0.0120 - rmse: 0.1096 - val_loss: 0.0113 - val_mape: 19.0216 - val_mse: 0.0113 - val_rmse: 0.1063\n",
      "Epoch 40/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0122 - mape: 22.5145 - mse: 0.0122 - rmse: 0.1104 - val_loss: 0.0112 - val_mape: 19.0839 - val_mse: 0.0112 - val_rmse: 0.1060\n",
      "Epoch 41/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0119 - mape: 22.4793 - mse: 0.0119 - rmse: 0.1092 - val_loss: 0.0113 - val_mape: 18.9268 - val_mse: 0.0113 - val_rmse: 0.1064\n",
      "Epoch 42/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0120 - mape: 22.2336 - mse: 0.0120 - rmse: 0.1094 - val_loss: 0.0114 - val_mape: 18.9977 - val_mse: 0.0114 - val_rmse: 0.1067\n",
      "Epoch 43/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0120 - mape: 22.1777 - mse: 0.0120 - rmse: 0.1095 - val_loss: 0.0113 - val_mape: 18.8764 - val_mse: 0.0113 - val_rmse: 0.1061\n",
      "Epoch 44/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0122 - mape: 22.4968 - mse: 0.0122 - rmse: 0.1103 - val_loss: 0.0114 - val_mape: 19.2263 - val_mse: 0.0114 - val_rmse: 0.1066\n",
      "Epoch 45/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0119 - mape: 22.0488 - mse: 0.0119 - rmse: 0.1093 - val_loss: 0.0112 - val_mape: 19.0967 - val_mse: 0.0112 - val_rmse: 0.1058\n",
      "Epoch 46/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - mape: 22.2050 - mse: 0.0119 - rmse: 0.1089 - val_loss: 0.0113 - val_mape: 19.0103 - val_mse: 0.0113 - val_rmse: 0.1063\n",
      "Epoch 47/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0119 - mape: 22.1940 - mse: 0.0119 - rmse: 0.1090 - val_loss: 0.0111 - val_mape: 19.1386 - val_mse: 0.0111 - val_rmse: 0.1056\n",
      "Epoch 48/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0120 - mape: 22.3230 - mse: 0.0120 - rmse: 0.1094 - val_loss: 0.0112 - val_mape: 19.1121 - val_mse: 0.0113 - val_rmse: 0.1061\n",
      "Epoch 49/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0117 - mape: 22.0113 - mse: 0.0117 - rmse: 0.1083 - val_loss: 0.0113 - val_mape: 19.3314 - val_mse: 0.0113 - val_rmse: 0.1061\n",
      "Epoch 50/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0118 - mape: 21.9682 - mse: 0.0118 - rmse: 0.1085 - val_loss: 0.0113 - val_mape: 19.0444 - val_mse: 0.0113 - val_rmse: 0.1061\n",
      "Epoch 51/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0117 - mape: 21.9914 - mse: 0.0117 - rmse: 0.1082 - val_loss: 0.0113 - val_mape: 18.6736 - val_mse: 0.0113 - val_rmse: 0.1061\n",
      "Epoch 52/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0116 - mape: 21.7986 - mse: 0.0116 - rmse: 0.1077 - val_loss: 0.0113 - val_mape: 19.1514 - val_mse: 0.0113 - val_rmse: 0.1061\n",
      "Epoch 53/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0118 - mape: 22.2144 - mse: 0.0118 - rmse: 0.1088 - val_loss: 0.0113 - val_mape: 19.0940 - val_mse: 0.0113 - val_rmse: 0.1062\n",
      "Epoch 54/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0117 - mape: 21.9292 - mse: 0.0117 - rmse: 0.1082 - val_loss: 0.0112 - val_mape: 19.1330 - val_mse: 0.0112 - val_rmse: 0.1059\n",
      "Epoch 55/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - mape: 21.8876 - mse: 0.0115 - rmse: 0.1074 - val_loss: 0.0113 - val_mape: 19.0065 - val_mse: 0.0113 - val_rmse: 0.1065\n",
      "Epoch 56/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0117 - mape: 22.0662 - mse: 0.0117 - rmse: 0.1082 - val_loss: 0.0113 - val_mape: 19.2952 - val_mse: 0.0113 - val_rmse: 0.1065\n",
      "Epoch 57/200\n",
      "\u001b[1m566/566\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0116 - mape: 22.1471 - mse: 0.0116 - rmse: 0.1078 - val_loss: 0.0114 - val_mape: 19.0073 - val_mse: 0.0114 - val_rmse: 0.1066\n",
      "[MODELO CONGELADO]: 86.95574998855591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n",
      "/home/workstation/Escritorio/PabloC/TransferLearning/funcionesComunes.py:247: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictionsDF = pd.concat([predictionsDF, tempDF], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Train): 44.291013837591024, 6.655149422634403, 0.809888818533487, 19.36814414249631\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Val): 34.559356891796966, 5.878720684961735, 0.8028565565437676, 18.69285234851038\n",
      "RESULTADOS DE MSE, RMSE, R2, MAPE (Test): 48.39309158638034, 6.956514327332356, 0.6578745170703524, 19.92465677968514\n"
     ]
    }
   ],
   "source": [
    "# Creamos un df que contiene los resultados de los modelos freeze de cada especie\n",
    "df_Freeze = pd.DataFrame(columns = [\"File\", \"Time\", \"TrainMSE\", \"TrainRMSE\", \"TrainR2\", \"TrainMAPE\", \n",
    "                                    \"ValidationMSE\", \"ValidationRMSE\", \"ValidationR2\", \"ValidationMAPE\",\n",
    "                                    \"TestMSE\", \"TestRMSE\", \"TestR2\", \"TestMAPE\"])\n",
    "\n",
    "for archivo in os.listdir(\"models/TCNNMerged_Option2\"):\n",
    "\n",
    "    # Leemos solo los archivos json para obtener los nombres\n",
    "    if os.path.splitext(archivo)[1] == \".json\":\n",
    "        \n",
    "        # Obtenemos el nombre de archivo\n",
    "        nombreArchivo = archivo.split(\"_best_models.json\")[0]\n",
    "\n",
    "        print(f\"PROCESANDO ARCHIVO: {nombreArchivo}\")\n",
    "\n",
    "        # Cargamos el modelo entrenado\n",
    "        df = pd.read_csv(f'RCPMerged/{nombreArchivo}_merged.csv')\n",
    "        df = df[~df[\"nametag\"].str.startswith(\"INDI005\")]\n",
    "\n",
    "        # Codificamos, normalización y split de datos\n",
    "        df = codification(df)\n",
    "        print(df.shape)\n",
    "\n",
    "        df, valorNormalizacion = individualNormalization(df)\n",
    "        print(f\"SE HA NORMALIZADO EL ARCHIVO: {archivo}\")\n",
    "\n",
    "        temp_df = df\n",
    "        train_data, val_data, test_data = split_population_individuals(temp_df, train_pct=0.80, val_pct_in_train=0.20, details=False)\n",
    "        train_data.shape, val_data.shape, test_data.shape\n",
    "\n",
    "        # Obtenemos X e y para los datasets de train, val y test \n",
    "        WINDOWS_SIZE = 3\n",
    "        X_train, y_train = df_to_X_y_ind_3(train_data, WINDOWS_SIZE)\n",
    "        X_val, y_val = df_to_X_y_ind_3(val_data, WINDOWS_SIZE)\n",
    "        X_test, y_test = df_to_X_y_ind_3(test_data, WINDOWS_SIZE)\n",
    "        print(X_train.shape, X_test.shape, X_val.shape)\n",
    "\n",
    "        # Cargamos el modelo global\n",
    "        if nombreArchivo == \"Juniperus spp. \":\n",
    "            nombreArchivo = \"Juniperus spp\"\n",
    "        modelLSTM = tf.keras.models.load_model(f'models/TCNNMerged_Option2_Trained/{nombreArchivo}.keras')\n",
    "        modelLSTMFreeze = modelLSTM\n",
    "\n",
    "        # Obtener el optimizador del modelo\n",
    "        optimizer = modelLSTMFreeze.optimizer\n",
    "\n",
    "        # Obtenemos el valor de batch size\n",
    "        with open(f'models/TCNNMerged_Option2/{archivo}') as f:\n",
    "            parameters = json.load(f)\n",
    "\n",
    "        batch_size_LSTM = parameters[0][\"batch_size\"]\n",
    "\n",
    "        # Indicamos el numero de layers a entrenar\n",
    "        NUM_TRAINABLE = 1\n",
    "\n",
    "        numTCNN_layers = sum(1 for layer in modelLSTMFreeze.layers if isinstance(layer, (Conv1D)))\n",
    "\n",
    "        numFreezeLayers = numTCNN_layers - NUM_TRAINABLE # Congelamos todas menos la útlima capa\n",
    "\n",
    "        numberLayersFreezed = 0\n",
    "\n",
    "        # Recorre las capas en orden inverso\n",
    "        for i, layer in enumerate(modelLSTMFreeze.layers):\n",
    "\n",
    "            if isinstance(layer, (Conv1D)):\n",
    "                numberLayersFreezed +=1\n",
    "        \n",
    "            if isinstance(layer, (Conv1D, BatchNormalization, MaxPooling1D)) and numFreezeLayers>=numberLayersFreezed:\n",
    "                \n",
    "                layer.trainable = False\n",
    "            \n",
    "            print(layer, layer.trainable)\n",
    "\n",
    "\n",
    "        # Compilamos el modelo con los nuevos datos\n",
    "        modelLSTMFreeze.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=[\n",
    "                                    MeanSquaredError(name='mse'),\n",
    "                                    RootMeanSquaredError(name='rmse'),\n",
    "                                    MeanAbsolutePercentageError(name='mape')\n",
    "                                ])\n",
    "\n",
    "        # Comienza a medir el tiempo de entrenamiento\n",
    "        start_time = time.time()\n",
    "\n",
    "        historyLSTMTransfer = modelLSTMFreeze.fit(X_train, y_train, epochs=200, batch_size=batch_size_LSTM,\n",
    "                                validation_data=(X_val, y_val), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
    "        \n",
    "        # Finaliza la medición del tiempo de entrenamiento\n",
    "        end_time = time.time()\n",
    "        print(f\"[MODELO CONGELADO]: {end_time-start_time}\")\n",
    "\n",
    "        # Realizar predicciones y calcular métricas para el conjunto de entrenamiento\n",
    "        predictions_train = predictionForIndividuals(X_train, y_train, modelLSTMFreeze, batch_size_LSTM)\n",
    "        predictions_train[\"PredictionsDenormalize\"] = predictions_train.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Predictions\"), axis=1)\n",
    "        predictions_train[\"ActualDenormalize\"] = predictions_train.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Actuals\"), axis=1)\n",
    "\n",
    "        train_mse = mean_squared_error(predictions_train[\"ActualDenormalize\"],predictions_train[\"PredictionsDenormalize\"])\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_mape = (np.sum(np.abs(predictions_train[\"PredictionsDenormalize\"] - predictions_train[\"ActualDenormalize\"])) / np.sum(np.abs(predictions_train[\"ActualDenormalize\"]))) * 100\n",
    "        train_r2 = r2_score(predictions_train[\"ActualDenormalize\"], predictions_train[\"PredictionsDenormalize\"])\n",
    "\n",
    "        # Realizar predicciones y calcular métricas para el conjunto de validación\n",
    "        predictions_val = predictionForIndividuals(X_val, y_val, modelLSTMFreeze, batch_size_LSTM)\n",
    "        predictions_val[\"PredictionsDenormalize\"] = predictions_val.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Predictions\"), axis=1)\n",
    "        predictions_val[\"ActualDenormalize\"] = predictions_val.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Actuals\"), axis=1)\n",
    "\n",
    "        val_mse = mean_squared_error(predictions_val[\"ActualDenormalize\"],predictions_val[\"PredictionsDenormalize\"])\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_mape = (np.sum(np.abs(predictions_val[\"PredictionsDenormalize\"] - predictions_val[\"ActualDenormalize\"])) / np.sum(np.abs(predictions_val[\"ActualDenormalize\"]))) * 100\n",
    "        val_r2 = r2_score(predictions_val[\"ActualDenormalize\"], predictions_val[\"PredictionsDenormalize\"])\n",
    "\n",
    "        # Realizar predicciones y calcular métricas para el conjunto de prueba\n",
    "        predictions_test = predictionForIndividuals(X_test, y_test, modelLSTMFreeze, batch_size_LSTM)\n",
    "        predictions_test[\"PredictionsDenormalize\"] = predictions_test.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Predictions\"), axis=1)\n",
    "        predictions_test[\"ActualDenormalize\"] = predictions_test.apply(lambda row: desnormalizacionBAI(row, valorNormalizacion, \"Actuals\"), axis=1)\n",
    "\n",
    "        test_mse = mean_squared_error(predictions_test[\"ActualDenormalize\"],predictions_test[\"PredictionsDenormalize\"])\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        test_mape = (np.sum(np.abs(predictions_test[\"PredictionsDenormalize\"] - predictions_test[\"ActualDenormalize\"])) / np.sum(np.abs(predictions_test[\"ActualDenormalize\"]))) * 100\n",
    "        test_r2 = r2_score(predictions_test[\"ActualDenormalize\"], predictions_test[\"PredictionsDenormalize\"])\n",
    "\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Train): {train_mse}, {train_rmse}, {train_r2}, {train_mape}\")\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Val): {val_mse}, {val_rmse}, {val_r2}, {val_mape}\")\n",
    "        print(f\"RESULTADOS DE MSE, RMSE, R2, MAPE (Test): {test_mse}, {test_rmse}, {test_r2}, {test_mape}\")\n",
    "\n",
    "        # Guardamos los datos calculados\n",
    "        df_Freeze.loc[len(df_Freeze)] = [nombreArchivo, end_time-start_time,train_mse, train_rmse, train_r2, train_mape, val_mse, val_rmse, val_r2, val_mape, test_mse, test_rmse, test_r2, test_mape]\n",
    "\n",
    "df_Freeze.to_csv(f'resultsTransferOption2/resultados_Option2_TCNN.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
